{"searchDocs":[{"title":"Authentication","type":0,"sectionRef":"#","url":"/v0.3/authentication","content":"Authentication After installation, user will be prompted to set the password for the default admin user on the first-time login. note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","keywords":"Harvester harvester Rancher rancher Authentication","version":"v0.3"},{"title":"Harvester Overview","type":0,"sectionRef":"#","url":"/v0.3/","content":"Harvester Overview Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing. Harvester Features​ Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port consoleVM live migration supportSupported VM backup and restoreDistributed block storageMultiple network interface controllers (NICs) in the VM connecting to the management network or VLANsVirtual Machine and cloud-init templatesRancher integration with multi-cluster management and the Harvester node driverPXE/iPXE boot supportVirtual IP and bond NIC supportMonitoring integration Harvester Architecture​ The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes.KubeVirt is a virtual machine management add-on for Kubernetes.Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements​ To get the Harvester server up and running, the following minimum hardware is required: Type\tRequirementsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory\t32 GB minimum; 64 GB or above preferred Disk Capacity\t140 GB minimum; 500 GB or above preferred Disk Performance\t5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd. Network Card\t1 Gbps Ethernet minimum; 10Gbps Ethernet recommended Network Switch\tTrunking of ports required for VLAN support Quick Start​ You can install Harvester via ISO installation or PXE Boot Installation. Instructions are provided in sections below. ISO Installation​ You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases. During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Note: This video shows a brief overview of the ISO installation process. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer.Choose the installation mode by either creating a new Harvester cluster or by joining an existing one.Choose the installation device to which the Harvester cluster will be formatted.Configure the hostname and select the network interface for the management network. By default, Harvester will create a bond NIC named harvester-mgmt, and the IP address can either be configured via DHCP or by static method.Optional: Configure the DNS servers; use commas as delimiters.Configure the Virtual IP with which you can use to access the cluster or join other nodes to the cluster.Configure the cluster token. This token will be used for adding other nodes to the cluster.Configure the login password of the host. The default ssh user is rancher.Optional: Configure the NTP Servers of the node if needed. Default is 0.suse.pool.ntp.org.Optional: If you need to use an HTTP proxy to access the outside world, enter the proxy URL address; otherwise, leave this blank.Optional: You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/&lt;username&gt;.keys.Optional: If you need to customize the host with cloud-init configuration, enter the HTTP URL.Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete.Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell).The default URL of the web interface is https://your-virtual-ip.Users will be prompted to set the password for the default admin user at first login. Other Installation Methods​ Harvester can be installed automatically also. Please refer to PXE Boot Install for detailed instructions for additional guidance. More iPXE usage examples are available at harvester/ipxe-examples.","keywords":"Harvester harvester Rancher rancher Harvester Intro","version":"v0.3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/v0.3/faq","content":"FAQ This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node? $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard? username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster? # You can either download the kubeconfig file from the Harvester # dashboard or access it via one of the Harvester management nodes. E.g., $ cat /etc/rancher/rke2/rke2.yaml How do I access the embedded Rancher dashboard? Please refer to the troubleshooting section. How to install the qemu-guest-agent of a running VM. # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/reference/cli.html#clean","keywords":"","version":"v0.3"},{"title":"ISO Installation","type":0,"sectionRef":"#","url":"/v0.3/install/iso-install","content":"ISO Installation To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option.Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one.Choose the installation device that Harvester will be formatted to.Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt, and the IP address can either be configured via DHCP or a static assigned a static one.(Optional) Configure the DNS servers. Use commas as a delimiter.Configure the Virtual IP which you can use to access the cluster or join the cluster to other nodes.Configure the cluster token. This token will be used for adding other nodes to the cluster.Configure the login password of the host. The default SSH user is rancher.(Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org.(Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank.(Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/&lt;username&gt;.keys.(Optional) If you need to customize the host with a cloud-init config, enter the HTTP URL here.After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete.Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell.The default URL of the web interface is https://your-virtual-ip.You will be prompted to set the password for the default admin user when logging in for the first time.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v0.3"},{"title":"Management Address","type":0,"sectionRef":"#","url":"/v0.3/install/management-address","content":"Management Address Harvester provides a fixed virtual IP (VIP) as the management address. Users can see the management address on the console dashboard after installation. Usages​ The management address has two usages. Allow users to access the Harvester UI via https protocol. Used by the other nodes to join the cluster. Configure VIP​ Users can specify the VIP during installation. It can either be configured via DHCP or assigned a static one. ::: In PXE boot, Harvester does not support setting the VIP via DHCP. It will be addressed in the next release. Issue: https://github.com/harvester/harvester/issues/1410 :::","keywords":"VIP","version":"v0.3"},{"title":"USB Installation","type":0,"sectionRef":"#","url":"/v0.3/install/usb-install","content":"USB Installation Create a bootable USB flash drive​ There are a couple of ways to create a USB installation flash drive. balenaEtcher​ balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive. dd command​ On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. caution Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb_device&gt; bs=64k Common issues​ When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens​ If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system. Graphics issue​ Firmwares of some graphic cards are not shipped in v0.3.0. You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Other issues​ Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot.For version v0.3.0, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","keywords":"","version":"v0.3"},{"title":"Harvester CSI Driver","type":0,"sectionRef":"#","url":"/v0.3/rancher/csi-driver","content":"Harvester CSI Driver The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying with Harvester RKE2 Node Driver​ When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected. Deploying with Harvester RKE1 Node Driver​ Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; ","keywords":"Harvester harvester Rancher Integration","version":"v0.3"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v0.3/monitoring/harvester-monitoring","content":"Monitoring Available as of v0.3.0 Dashboard Metrics​ Harvester v0.3.0 has provided a built-in monitoring integration using Prometheus. Monitoring is automatically installed during ISO installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. note Only admin users are able to view the dashboard metrics. VM Detail Metrics​ For each VM, users can view the VM metrics by clicking the VM details page.","keywords":"","version":"v0.3"},{"title":"Host Management","type":0,"sectionRef":"#","url":"/v0.3/host/","content":"Host Management Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are more than three nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance​ For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node​ Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you’re done, power back on and make the node schedulable again by uncordoning it. Multi-disk Management - Tech Preview​ Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config.Select the Disks tab and click Add Disks.Select either an additional raw block device or partition to add as an additional data volume. The Force Formatted option is required when adding an entire raw block device to form a single root disk partition using the ext4 filesystem.The Force Formatted option is optional when adding partitions where the filesystem type is ext4, XFS or cannot be found. It is required when adding partitions of any other filesystem type.","keywords":"","version":"v0.3"},{"title":"Harvester Node Driver","type":0,"sectionRef":"#","url":"/v0.3/rancher/node/node-driver","content":"Harvester Node Driver The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver, and the project repo is available at harvester/docker-machine-driver-harvester. Users can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.1 or above using the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. note Currently only Rancher v2.6.1 or above is compatible with Harvester v0.3.0. Enable Harvester Node Driver​ The Harvester node driver is not enabled by default from the Rancher UI. Click the Cluster Management tab to enable the Harvester node driver. Click the Drivers page, then click the Node Drivers tab Select the Harvester node driver, then click Activate to enable the Harvester node driver Now users can spin up Kubernetes clusters on top of the Harvester cluster and manage them there. RKE1 Kubernetes Cluster​ Click to learn how to create RKE1 Kubernetes Clusters. RKE2 Kubernetes Cluster​ Click to learn how to create RKE2 Kubernetes Clusters.","keywords":"Harvester harvester Rancher rancher Harvester Node Driver","version":"v0.3"},{"title":"Creating an RKE2 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v0.3/rancher/node/rke2-cluster","content":"Creating an RKE2 Kubernetes Cluster Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. note Harvester RKE2 node driver is in tech preview.VLAN network is required for Harvester node driver. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester&quot; or &quot;External Harvester&quot;.Click Create. Create RKE2 Kubernetes Cluster​ Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically.","keywords":"Harvester harvester Rancher rancher Rancher Integration RKE2","version":"v0.3"},{"title":"Creating an RKE1 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v0.3/rancher/node/rke1-cluster","content":"Creating an RKE1 Kubernetes Cluster Users can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. note Harvester RKE1 node driver is in tech preview.VLAN network is required for Harvester node driver. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential name.Select &quot;Imported Harvester&quot; or &quot;External Harvester&quot;.Click Create. Create Node Template​ You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials.Configure Instance Options: Configure the CPU, memory, and diskSelect an OS image that is compatible with the cloud-init config.Select a network that the node driver is able to connect to; currently, only VLAN is supported.Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu. Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Create RKE1 Kubernetes Cluster​ Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE1.Select Harvester node driver.Enter Cluster Name (required).Enter Name Prefix (required).Enter Template (required).Select etcd and Control Plane (required).Click Create.","keywords":"Harvester harvester Rancher rancher Rancher Integration RKE1","version":"v0.3"},{"title":"Harvester Network","type":0,"sectionRef":"#","url":"/v0.3/networking/harvester-network","content":"Harvester Network Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management networkVLAN Management Network​ Harvester uses flannel CNI as its default management network. It is a built-in network that can be used directly from the cluster. However, the management network IP is not persisted and will be changed after a VM reboot. Additionally, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. VLAN Network​ The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch.VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't.The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs.Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling Default VLAN Network​ Users can enable the VLAN network via Setting &gt; VLAN and selecting a common physical NIC for the nodes as the default VLAN config . It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (i.e., harvester-mgmt) for better network performance and isolation. note Modifying the default VLAN network setting will not update the existing configured host network.Harvester VLAN network supports bond interfaces, currently it can only be created automatically via PEX Boot Configuration. Users may also login to the node and create it manually. Optional: Users can customize each node's VLAN network via the HOST &gt; Network tab. Create a VLAN Network​ A new VLAN network can be created via the Advanced &gt; Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network. Create a VM with VLAN Network​ Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page.Specify the required parameters and click the Networks tab.Either configure the default network to be a VLAN network or select an additional network to add. note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card can be enabled by default via setting the cloud-init network data. E.g., version: 1 config: - type: physical name: enp1s0 # name is varies upon OS image subnets: - type: dhcp - type: physical name: enp2s0 subnets: - type: DHCP For more detailed configs you may refer to the cloud-init network configs. Configure DHCP servers on Networks​ By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually in a node or using a containerized method. Refer to this issue as an example.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v0.3"},{"title":"Harvester Cloud Provider","type":0,"sectionRef":"#","url":"/v0.3/rancher/cloud-provider","content":"Harvester Cloud Provider Available as of v0.3.0 Users can now provision both RKE1 and RKE2 clusters in Rancher 2.6.1, using the built-in Harvester Node Driver. Harvester can now provide load balancer support as well as cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2.How to configure a LoadBalancer service. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying to the RKE1 Cluster with Harvester Node Driver​ When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select the External cloud provider. Generate add-on configuration and add it to the RKE YAML file. # depend on kubectl to operate the Harvester cluster curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | sh -s &lt;serviceAccount name&gt; &lt;namespace&gt; Deploying to the RKE2 Cluster with Harvester Node Driver​ When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Load Balancer Support​ After deploying the Harvester cloud provider, users can now configure a Kubernetes service of the type LoadBalancer. Currently, users can only set the load balancer configuration using service annotations. Example You can specify the Harvester LoadBalancer service config through annotations. The cloudprovider.harvesterhci.io/healthcheck-port annotation is required. For example: cloudprovider.harvesterhci.io/ipam: dhcp - if the network of your Kubernetes nodes supports DHCP.cloudprovider.harvesterhci.io/healthcheck-port: 80 - specify the port of your service. IPAM​ Harvester's built-in load balancer supports both pool and dhcp modes. Users can specify the IPAM mode using the annotation key cloudprovider.harvesterhci.io/ipam. This value defaults to pool. pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. Refer to the guideline to configure an IP address pool. E.g, for a Namespace pool, a service will take an address based upon its namespace pool cidr/range-namespace. These would look like the following: $ kubectl get configmap -n kube-system kubevip -o yaml apiVersion: v1 kind: ConfigMap metadata: name: kubevip namespace: kube-system data: cidr-default: 192.168.0.200/29 cidr-development: 192.168.0.210/29 cidr-finance: 192.168.0.220/29 cidr-testing: 192.168.0.230/29 dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server of the Kubernetes nodes. Health Checks​ The Harvester load balancer supports TCP health checks. Supported annotations are shown below: Key\tValue\tRequired\tDescriptioncloudprovider.harvesterhci.io/healthcheck-port\tstring\ttrue\tSpecifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold\tstring\tfalse\tSpecifies the health check success threshold. The default value is 1. If the number of times the prober continuously detects an address successfully reaches the success threshold, then the backend server can start to forward traffic. cloudprovider.harvesterhci.io/healthcheck-failure-threshold\tstring\tfalse\tSpecifies the success and failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the failure threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds\tstring\tfalse\tSpecifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds\tstring\tfalse\tSpecifies the timeout of every health check. The default value is 3 seconds.","keywords":"Harvester harvester RKE rke RKE2 rke2 Harvester Cloud Provider","version":"v0.3"},{"title":"Harvester","type":0,"sectionRef":"#","url":"/v0.3/troubleshooting/harvester","content":"Harvester Generate a Support Bundle​ Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Access Embedded Rancher​ You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer. note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here. Access Embedded Longhorn​ You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn. note We only support to use the embedded Longhorn UI for debugging and validation purpose .","keywords":"","version":"v0.3"},{"title":"PXE Boot Installation","type":0,"sectionRef":"#","url":"/v0.3/install/pxe-boot-install","content":"PXE Boot Installation Starting from version 0.2.0, Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples. Prerequisite​ info Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers​ An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10, and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/. Preparing Boot Files​ Download the required files from the Harvester releases page. The ISO: harvester-&lt;version&gt;-amd64.isoThe kernel: harvester-&lt;version&gt;-vmlinuz-amd64The initrd: harvester-&lt;version&gt;-initrd-amd64The rootfs squashfs image: harvester-&lt;version&gt;-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts​ When performing an automatic installation, there are two modes: CREATE: we are installing a node to construct an initial Harvester cluster.JOIN: we are installing a node to join an existing Harvester cluster. CREATE Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token: token os: hostname: node1 ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password: rancher install: mode: create networks: harvester-mgmt: # (Mandatory) The management bond name. interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 harvester-vlan: # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces: # host to host, consider creating a bonding device. Users can then select - name: ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method: none bond_options: mode: balance-tlb miimon: 100 device: /dev/sda iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso vip: 10.100.0.100 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode: static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create. note If there are multiple network interfaces on the installing machine, the user can use ip=&lt;interface&gt;:dhcp to specify the booting interface (e.g., ip=eth1:dhcp). JOIN Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url: https://10.100.0.130:8443 token: token os: hostname: node2 ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers: - 1.1.1.1 - 8.8.8.8 password: rancher install: mode: join networks: harvester-mgmt: # (Mandatory) The management bond name. interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 10 harvester-vlan: # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces: # host to host, consider creating a bonding device. Users can then select - name: ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method: none bond_options: mode: balance-tlb miimon: 100 device: /dev/sda iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join. DHCP Server Configuration​ The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16; subnet 10.100.0.0 netmask 255.255.255.0 { option routers 10.100.0.10; option domain-name-servers 192.168.2.1; range 10.100.0.100 10.100.0.253; } group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } group { # join group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-join-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-join&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node2 { hardware ethernet 52:54:00:69:d5:92; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration​ For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support​ UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program​ Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration​ If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } elsif substring (option vendor-class-identifier, 0, 10) = &quot;HTTPClient&quot; { # UEFI HTTP Boot option vendor-class-identifier &quot;HTTPClient&quot;; filename &quot;http://10.100.0.10/harvester/ipxe.efi&quot;; } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi. The iPXE Script for UEFI Boot​ It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-&lt;version&gt;-vmlinuz initrd=harvester-&lt;version&gt;-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot The parameter initrd=harvester-&lt;version&gt;-initrd is required.","keywords":"Harvester harvester Rancher rancher Install Harvester Installing Harvester Harvester Installation PXE Boot Install","version":"v0.3"},{"title":"Installation","type":0,"sectionRef":"#","url":"/v0.3/troubleshooting/installation","content":"Installation The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS)​ Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancherPassword: rancher Meeting hardware requirements​ Check that your hardware meets the minimum requirements to complete installation. Receiving the message &quot;Loading images. This may take a few minutes...&quot;​ Because the system doesn't have a default route, your installer may become &quot;stuck&quot; in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev harvester-mgmt proto dhcp &lt;-- Does a default route exist? 10.10.0.0/24 dev harvester-mgmt proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. Collecting Information​ Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. Content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* Output of these commands: blkid dmesg ","keywords":"","version":"v0.3"},{"title":"Rancher Integration","type":0,"sectionRef":"#","url":"/v0.3/rancher/rancher-integration","content":"Rancher Integration Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support. Deploying Rancher​ Previously in Harvester v0.2.0, users had the option to enable the embedded Rancher server. This option has been removed from Harvester v0.3.0. To use Rancher with Harvester, please install the Rancher server separately from the Harvester. As an option, You can spin up a VM in the Harvester and install the Rancher v2.6.1 or above to try out the integration features. Quick Start Guide​ Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM)An on-premises VMA bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection.From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.2 note For more information about how to deploy the Rancher server, please refer to the Rancher documentation. Virtualization Management​ With Rancher's Virtualization Management, users can now import and manage Harvester clusters. By clicking on one of the clusters, users are able to view and manage the downstream Harvester resources such as VMs, images, volumes, etc. Additionally, Rancher's VM feature has leveraged existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please reference the virtualization management page. note Virtualization Management is in Tech Preview. Creating Kubernetes Clusters using the Harvester Node Driver​ Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. Starting with Rancher v2.6.1, the Harvester node driver has been added by default. Users can reference this doc for more details. note Harvester Node Driver is in Tech Preview.","keywords":"Harvester harvester Rancher rancher Rancher Integration","version":"v0.3"},{"title":"Harvester Configuration","type":0,"sectionRef":"#","url":"/v0.3/install/harvester-configuration","content":"Harvester Configuration Configuration Example​ Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url: https://someserver:8443 token: TOKEN_VALUE os: ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files: - encoding: &quot;&quot; content: test content owner: root path: /etc/test.txt permissions: '0755' hostname: myhost modules: - kvm - nvme sysctls: kernel.printk: &quot;4 4 1 7&quot; kernel.kptr_restrict: &quot;1&quot; dns_nameservers: - 8.8.8.8 - 1.1.1.1 ntp_servers: - 0.us.pool.ntp.org - 1.us.pool.ntp.org password: rancher environment: http_proxy: http://myserver https_proxy: http://myserver install: mode: create networks: harvester-mgmt: interfaces: - name: ens5 default_route: true method: dhcp force_efi: true device: /dev/vda silent: true iso_url: http://myserver/test.iso poweroff: true no_format: true debug: true tty: ttyS0 vip: 10.10.0.19 vip_hw_addr: 52:54:00:ec:0e:0b vip_mode: dhcp Configuration Reference​ Below is a reference of all configuration keys. caution Security Risks: The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. server_url​ Definition​ The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Example​ server_url: https://someserver:8443 install: mode: join token​ Definition​ The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example​ token: myclustersecret Or a node token token: &quot;K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4&quot; os.ssh_authorized_keys​ Definition​ A list of SSH authorized keys that should be added to the default user, rancher. SSH keys can be obtained from GitHub user accounts by using the formatgithub:${USERNAME}. This is done by downloading the keys from https://github.com/${USERNAME}.keys. Example​ os: ssh_authorized_keys: - &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D&quot; - &quot;github:ibuildthecloud&quot; os.write_files​ A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: &quot;&quot;: content data are written in plain text. In this case, the encoding field can be also omitted.b64, base64: content data are base64-encoded.gz, gzip: content data are gzip-compressed.gz+base64, gzip+base64, gz+b64, gzip+b64: content data are gzip-compressed first and then base64-encoded. Example os: write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner: root:root path: /etc/connman/main.conf permissions: '0644' - content: | # My new /etc/sysconfig/samba file SMDBOPTIONS=&quot;-D&quot; path: /etc/sysconfig/samba - content: !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path: /bin/arch permissions: '0555' - content: | 15 * * * * root ship_logs path: /etc/crontab os.hostname​ Definition​ Set the system hostname. If the system hostname is supplied via DHCP, then that value will be used here. If this value is not set and one is not supplied via DHCP, then a random hostname will be generated. Example​ os: hostname: myhostname os.modules​ Definition​ A list of kernel modules to be loaded on start. Example​ os: modules: - kvm - nvme os.sysctls​ Definition​ Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf. Values must be specified as strings. Example​ os: sysctls: kernel.printk: 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict: &quot;1&quot; # force the YAML parser to read as a string os.dns_nameservers​ Definition​ Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example​ os: dns_nameservers: - 8.8.8.8 - 1.1.1.1 os.ntp_servers​ Definition​ Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Example​ os: ntp_servers: - 0.us.pool.ntp.org - 1.us.pool.ntp.org os.password​ Definition​ The password for the default user, rancher. By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from/etc/shadow. You can also encrypt a password using openssl passwd -6. Example​ Encrypted: os: password: &quot;$6$kZYUnRaTxNdg4W8H$WSEJydGWsNpaRbbbRdTDLJ2hDLbkizxSFGW2RtexlqG6njEATaGQG9ssztjaKDCsaNUPBZ1E1YdsvSLMAi/IO/&quot; Or clear text: os: password: supersecure os.environment​ Definition​ Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example​ os: environment: http_proxy: http://myserver https_proxy: http://myserver install.mode​ Definition​ Harvester installation mode: create: Creating a new Harvester installation.join: Join an existing Harvester installation. Need to specify server_url. Example​ install: mode: create install.networks​ Definition​ Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method: Method to assign an IP to this network. The following are supported: static: Manually assign an IP and gateway.dhcp: Request an IP from the DHCP server.none: Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip: Static IP for this network. Required if static method is chosen.subnet_mask: Subnet mask for this network. Required if static method is chosen.gateway: Gateway for this network. Required if static method is chosen.interfaces: An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name: The name of the slave interface for the bonded network. default_route: Set the network as the default route or not.bond_options: Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlbmiimon: 100 note A network called harvester-mgmt is mandatory to establish a valid management network. note Harvester uses the systemd net naming scheme. Please make sure the interface name is present on the target machine before installation. Example​ install: mode: create networks: harvester-mgmt: # The management bond name. This is mandatory. interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 harvester-vlan: # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces: - name: ens6 method: none bond_options: mode: balance-tlb miimon: 100 bond0: interfaces: - name: ens8 method: static ip: 10.10.18.2 subnet_mask: 255.255.255.0 gateway: 192.168.11.1 install.force_efi​ Force EFI installation even when EFI is not detected. Default: false. install.device​ The device to install the OS. install.silent​ Reserved. install.iso_url​ ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff​ Shutdown the machine after installation instead of rebooting install.no_format​ Do not partition and format, assume layout exists already. install.debug​ Run the installation with additional logging and debugging enabled for the installed system. install.tty​ Definition​ The tty device used for the console. Example​ install: tty: ttyS0,115200n8 install.vip​ install.vip_mode​ install.vip_hw_addr​ Definition​ install.vip: The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://&lt;VIP&gt;.install.vip_mode dhcp: Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided.static: Harvester uses a static VIP. install.vip_hw_addr: The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp. Example​ Configure a static VIP. install: vip: 192.168.0.100 vip_mode: static Configure a DHCP VIP. install: vip: 10.10.0.19 vip_mode: dhcp vip_hw_addr: 52:54:00:ec:0e:0b ","keywords":"Harvester harvester Rancher rancher Harvester Configuration","version":"v0.3"},{"title":"Virtualization Management","type":0,"sectionRef":"#","url":"/v0.3/rancher/virtualization-management","content":"Virtualization Management For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6.x. First, you will need to install Rancher v2.6.1 or above. For testing purposes, you can spin up a Rancher server using the following docker run command: $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.2 note For a production environment setup, please refer to the official Rancher docs. Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server.Specify the Cluster Name and click Create. You will then see the registration commands; copy the appropriate command and ssh to one of the Harvester management nodes to run this command accordingly.Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly.From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy​ In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication, users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions: Define user authorization outside the scope of any particular cluster. Cluster and Project Roles: Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings.A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example​ The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users &amp; Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project.A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab.Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save.Open an incognito browser and log in as project-owner.After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned.Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed.Create a VM with one of the images that you have uploaded.Log in with another user, e.g., project-readonly, and this user will only have the read permission of this project. note A known issue was found that allows the read-only user to be able to manage API actions. Delete Imported Harvester Cluster​ Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management &gt; Harvester Clusters. Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. caution Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","keywords":"Harvester Rancher","version":"v0.3"},{"title":"Upgrading Harvester","type":0,"sectionRef":"#","url":"/v0.3/upgrade","content":"Upgrading Harvester note Upgrade is not supported from previous versions to v0.3.0 version. A manual upgrade process starting with v0.3.0 is being investigated. Harvester will inform the community once this process is in place.One-click upgrade will be supported starting with the v1.0.0 release.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v0.3"},{"title":"Harvester Terraform Provider","type":0,"sectionRef":"#","url":"/v0.3/terraform/terraform-provider","content":"Harvester Terraform Provider Requirements​ Terraform &gt;= 0.13.xGo 1.16 to build the provider plugin Install the Provider​ Option 1: Download and install the provider from the Terraform registry.​ To install this provider, copy and paste this code into your Terraform configuration. Then, run terraform init. Terraform 0.13+ terraform { required_providers { harvester = { source = &quot;harvester/harvester&quot; version = &quot;0.2.8&quot; } } } provider &quot;harvester&quot; { # Configuration options } For more details, please refer to the Harvester provider documentation. Option 2: Build and install the provider manually.​ Building the provider:​ Clone the repository using the following command: git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider; this will build the provider and put the provider binary in ./bin. Use the following command: cd terraform-provider-harvester make Installing the provider:​ The expected location for the Harvester provider for the target platform within one of the local search directories is as follows: registry.terraform.io/harvester/harvester/0.2.8/linux_amd64/terraform-provider-harvester_v0.2.8 The default location for locally-installed providers will be one of the following, depending on the operating system under which you are running Terraform: Windows: %APPDATA%\\terraform.d\\pluginsAll other systems: ~/.terraform.d/plugins Place the provider into the plugin directory as in the following example: version=0.2.8 arch=linux_amd64 terraform_harvester_provider_bin=./bin/terraform-provider-harvester terraform_harvester_provider_dir=&quot;${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/&quot; mkdir -p &quot;${terraform_harvester_provider_dir}&quot; cp ${terraform_harvester_provider_bin} &quot;${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}&quot;} Using the provider​ After placing the provider into your plugins directory, run terraform init to initialize it. More information about provider-specific configuration options can be found on the docs directory","keywords":"","version":"v0.3"},{"title":"Access to the Virtual Machine","type":0,"sectionRef":"#","url":"/v0.3/vm/access-to-the-vm","content":"Access to the Virtual Machine Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI​ VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu minimal cloud image, the VM can be accessed with the serial console. Access with the SSH Client​ Enter the IP address of the host in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@&lt;ip-address-or-hostname&gt; ","keywords":"Harvester harvester Rancher rancher Access to the VM","version":"v0.3"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/v0.3/troubleshooting/os","content":"Operating System Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit. The following sections contain information and tips to help users troubleshoot OS-related issues. How to log into a Harvester node​ Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~&gt; sudo blkid # Or become root rancher@node1:~&gt; sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only?​ The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: caution Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0, we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat &gt; /oem/91_hack.yaml &lt;&lt;'EOF' name: &quot;Rootfs Layout Settings for debugrw&quot; stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline &amp;&amp; grep -q rd.cos.debugrw /proc/cmdline' name: &quot;Layout configuration for debugrw&quot; environment_file: /run/cos/cos-layout.env environment: RW_PATHS: &quot; &quot; EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters​ note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry &quot;Harvester ea6e7f5-dirty&quot; --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry​ To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg &lt;...&gt; menuentry &quot;Harvester ea6e7f5-dirty (debug)&quot; --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug. We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug How to debug a system crash or hang​ Collect crash log​ If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps​ For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/&lt;time&gt; directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","keywords":"","version":"v0.3"},{"title":"Hot-Plug Volumes","type":0,"sectionRef":"#","url":"/v0.3/vm/hotplug-volume","content":"Hot-Plug Volumes Harvester supports adding hot-plug volumes to a running VM. Adding Hot-Plug Volumes to a Running VM​ The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page.Find the VM that you want to add a volume to and select ⋮ &gt; Add Volume.Enter the Name and select the Volume.Click Apply.","keywords":"Harvester Hot-plug Volume","version":"v0.3"},{"title":"Upload Images","type":0,"sectionRef":"#","url":"/v0.3/upload-image","content":"Upload Images Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL​ To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File​ Currently, qcow2, raw, and ISO images are supported. note Please do not refresh the page until the file upload is finished.This feature is temporarily unusable on the single cluster UI and will be fixed via #1415. Create Images via Volumes​ On the Volumes page, click Export Image. Enter image name to create image.","keywords":"Harvester harvester Rancher rancher Import Images","version":"v0.3"},{"title":"VM Backup & Restore","type":0,"sectionRef":"#","url":"/v0.3/vm/backup-restore","content":"VM Backup &amp; Restore Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. note A backup target must be set up. For more information, see Configure Backup Target. If the backup target has not been set, you’ll be prompted with a message to do so. Configure Backup Target​ A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings &gt; backup-target. Parameter\tType\tDescriptionType\tstring\tChoose S3 or NFS Endpoint\tstring\tA hostname or an IP address. It can be left empty for AWS S3. BucketName\tstring\tName of the bucket BucketRegion\tstring\tRegion of the bucket AccessKeyID\tstring\tA user-id that uniquely identifies your account SecretAccessKey\tstring\tThe password to your account Certificate\tstring\tPaste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle\tbool\tUse VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup​ Once the backup target is set, go to the Virtual Machines page.Click Take Backup of the VM actions to create a new VM backup.Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced &gt; Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup​ To restore a new VM from a backup, follow these steps: Go to the Backups page.Specify the new VM name and click Create.A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an Existing VM using a backup​ You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page.Click Create. The restore process can be viewed from the Virtual Machines page. Restore a new VM on another Harvester cluster​ Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata &amp; content backup feature. prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster​ Check the existing image name (normally starts with image-) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: &quot;&quot; pvcNamespace: &quot;&quot; sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster​ Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster.Go to the Backups page.Select the synced VM backup metadata and choose to restore a new VM with a specified VM name.A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","keywords":"Harvester harvester Rancher rancher VM Backup & Restore","version":"v0.3"},{"title":"Live Migration","type":0,"sectionRef":"#","url":"/v0.3/vm/live-migration","content":"Live Migration Live migration means moving a virtual machine to a different host without downtime. note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. Starting a Migration​ Go to the Virtual Machines page.Find the virtual machine that you want to migrate and select ⋮ &gt; Migrate.Choose the node to which you want to migrate the virtual machine. Click Apply. Aborting a Migration​ Go to the Virtual Machines page.Find the virtual machine in migrating status that you want to abort. Select ⋮ &gt; Abort Migration. Migration Timeouts​ Completion Timeout​ The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout​ Live migration will also be aborted when copying memory doesn't make any progress in 150s.","keywords":"Harvester harvester Rancher rancher Live Migration","version":"v0.3"},{"title":"Air Gapped Environment","type":0,"sectionRef":"#","url":"/v1.0/airgap","content":"Air Gapped Environment This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy​ In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation​ You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings​ You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI.Find the http-proxy setting, click ⋮ &gt; Edit settingEnter the value(s) for http-proxy, https-proxy and no-proxy. note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,.svc,.cluster.local","keywords":"Harvester offline Air-gap HTTP proxy","version":"v1.0"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/v1.0/authentication","content":"Authentication After installation, user will be prompted to set the password for the default admin user on the first-time login. note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","keywords":"Harvester harvester Rancher rancher Authentication","version":"v1.0"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v0.3/vm/create-vm","content":"Create a Virtual Machine How to Create a VM​ Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances.The VM Name is a required field.The VM Template is optional. You can select ISO, raw, and Windows image templates as default options.Configure the CPU and Memory of the VM.Select SSH keys or upload new keys.Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using VLAN networks. You may configure these on Advanced &gt; Networks.Advanced options such as hostname and cloud-init data are optional. You may configure these in the Advanced Options section. Cloud Configuration Examples​ Password configuration for the default user: #cloud-config password: password chpasswd: { expire: False } ssh_pwauth: True Network-data configuration using DHCP: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init configuration for the VM. Networks​ Management Network​ A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network. Secondary Network​ It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks.","keywords":"Harvester harvester Rancher rancher Create a VM","version":"v0.3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/v1.0/faq","content":"FAQ This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node?​ $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard?​ username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster?​ Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM?​ # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/reference/cli.html#clean How can I reset the administrator password?​ In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher --no-headers | head -1 | awk '{ print $1 }') -c rancher -- reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt; ","keywords":"","version":"v1.0"},{"title":"ISO Installation","type":0,"sectionRef":"#","url":"/v1.0/install/iso-install","content":"ISO Installation To get the Harvester ISO image, download it from the Github releases page. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR. Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt, and the IP address can be configured via DHCP or a statically assigned one (Note: The Node IP can not change at the lifecycle of a Harvester cluster, in case the DHCP is used, the user must make sure the DHCP server always offers the same IP for the same Node. Due to a changed Node IP the related Node can not join the cluster, or even break the cluster). (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP, VIP must be different than any Node IP). Configure the cluster token. This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher. Recommended configuring the NTP server to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org. (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/&lt;username&gt;.keys. (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip. You will be prompted to set the password for the default admin user when logging in for the first time.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.0"},{"title":"Harvester Overview","type":0,"sectionRef":"#","url":"/v1.0/","content":"Harvester Overview Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing. Harvester Features​ Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port consoleVM live migration supportSupported VM backup and restoreDistributed block storageMultiple network interface controllers (NICs) in the VM connecting to the management network or VLANsVirtual Machine and cloud-init templatesRancher integration with multi-cluster management and the Harvester node driverPXE/iPXE boot supportVirtual IP and bond NIC supportMonitoring integration Harvester Architecture​ The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes.KubeVirt is a virtual machine management add-on for Kubernetes.Elemental for SLE-Micro 5.2 (based on openSUSE Leap 15.3 before v1.0.3) is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements​ To get the Harvester server up and running, the following minimum hardware is required: Type\tRequirementsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory\t32 GB minimum; 64 GB or above preferred Disk Capacity\t140 GB minimum for testing; 500 GB or above preferred for production Disk Performance\t5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd Network Card\t1 Gbps Ethernet minimum for testing; 10Gbps Ethernet recommended for production Network Switch\tTrunking of ports required for VLAN support Quick Start​ You can install Harvester via the ISO installation or the PXE boot installation. Instructions are provided in the sections below. ISO Installation​ You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases. During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer. Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device on which the Harvester cluster will be installed Note: By default, Harvester uses GPT partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select MBR. Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt, and the IP address can be configured via DHCP or a statically assigned one (Note: The Node IP can not change at the lifecycle of a Harvester cluster, in case the DHCP is used, the user must make sure the DHCP server always offers the same IP for the same Node. Due to a changed Node IP the related Node can not join the cluster, or even break the cluster). Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP which you can use to access the cluster or join other nodes to the cluster (Note: If your IP address is configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP, VIP must be different than any Node IP). Configure the cluster token. This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher. Recommended configuring the NTP server to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org. (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote server URL. Your GitHub public keys can be used with https://github.com/&lt;username&gt;.keys. (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip. Users will be prompted to set the password for the default admin user at first login. PXE/iPXE Installation​ Harvester can also be installed automatically. Please refer to PXE Boot Install for detailed instructions and additional guidance. note More iPXE usage examples are available at harvester/ipxe-examples.","keywords":"Harvester harvester Rancher rancher Harvester Intro","version":"v1.0"},{"title":"Host Management","type":0,"sectionRef":"#","url":"/v1.0/host/","content":"Host Management Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance​ For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node​ Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you’re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node​ Deleting a node is done in two phases: Delete the node from Harvester Go to the Hosts pageOn the node you want to modify, click ⋮ &gt; Delete Uninstall RKE2 from the node Login to the node as rootRun rke2-uninstall.sh to delete the whole RKE2 service. caution You will lose all data of the control plane node after deleting the RKE2 service. note There's a known issue about node hard delete. Once resolved, the last step can be skipped. Multi-disk Management - Tech Preview​ Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config.Select the Disks tab and click Add Disks.Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. note In order for Harvester to identify the disks, each disk needs to have a unique WWN. Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above.","keywords":"","version":"v1.0"},{"title":"Management Address","type":0,"sectionRef":"#","url":"/v1.0/install/management-address","content":"Management Address Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address​ To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath='{.metadata.annotations}' Example of output: {&quot;kube-vip.io/hwaddr&quot;:&quot;02:00:00:09:7f:3f&quot;,&quot;kube-vip.io/requestedIP&quot;:&quot;10.84.102.31&quot;} Usages​ The management address has two usages. Allows the access to the Harvester API/UI via HTTPS protocol.Is the address the other nodes use to join the cluster.","keywords":"VIP","version":"v1.0"},{"title":"USB Installation","type":0,"sectionRef":"#","url":"/v1.0/install/usb-install","content":"USB Installation Create a bootable USB flash drive​ There are a couple of ways to create a USB installation flash drive. balenaEtcher​ balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive. dd command​ On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. caution Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb_device&gt; bs=64k Common issues​ When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens​ If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system. Graphics issue​ Firmwares of some graphic cards are not shipped in v0.3.0. You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Other issues​ Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot.For version v0.3.0 or above, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","keywords":"","version":"v1.0"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.0/monitoring/harvester-monitoring","content":"Monitoring Available as of v0.3.0 Dashboard Metrics​ Harvester has provided a built-in monitoring integration using Prometheus. Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. note Only admin users are able to view the cluster dashboard metrics. Additionally, Grafana is provided by rancher-monitoring, so the default admin password is: prom-operator Reference: values.yaml VM Detail Metrics​ For VMs, you can view VM metrics by clicking on the VM details page &gt; VM Metrics. note The current Memory Usage is calculated based on (1 - free/total) * 100%, not (used/total) * 100%. For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100%, roughly 40%. How to Configure Monitoring Settings​ Available as of v1.0.1 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: PrometheusPrometheus Node Exporter(UI configurable as of v1.0.2) From WebUI​ In the Advanced Settings page, you can view and change the resources settings as follows: Navigate to settings page, find harvester-monitoring. Click Show harvester-monitoring to view the current values. Click ⋮ &gt; Edit Setting to set a new value. Click Save and the Monitoring resource will be restarted within a few seconds. Please be aware that the reboot can take some time to reload previous data. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. caution When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM(out of memory). In that case, you should increase the value of limits.memory. From CLI​ To update those values, you can also use the CLI command with: $kubectl edit managedchart rancher-monitoring -n fleet-local. For Harvester version &gt;= v1.0.1, the related path and default value are: # Prometheus configs spec.values.prometheus.prometheusSpec.resources.limits.cpu: 1000m spec.values.prometheus.prometheusSpec.resources.limits.memory: 2500Mi spec.values.prometheus.prometheusSpec.resources.requests.cpu: 750m spec.values.prometheus.prometheusSpec.resources.requests.memory: 1750Mi --- # node exporter configs spec.values.prometheus-node-exporter.resources.limits.cpu: 200m spec.values.prometheus-node-exporter.resources.limits.memory: 180Mi spec.values.prometheus-node-exporter.resources.requests.cpu: 100m spec.values.prometheus-node-exporter.resources.requests.memory: 30Mi For versions &lt;= v1.0.0, the related path and default value are not specified in the managedchart rancher-monitoring, you need to add them accordingly. Troubleshooting​ For Monitoring support and troubleshooting, please refer to the troubleshooting page .","keywords":"","version":"v1.0"},{"title":"Multiple NICs with Non VLAN-aware Switch","type":0,"sectionRef":"#","url":"/v1.0/networking/best-practice/multiple-nics-non-vlan-aware-switch","content":"Multiple NICs with Non VLAN-aware Switch In this best practice guide for &quot;non VLAN-aware&quot; switch, also known as &quot;dummy&quot; switch, we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture​ Hardware: Three Harvester servers with dual ports network card.One or more &quot;non VLAN-aware&quot; switch(es). Network Specification: The host and the VM networks are in the same subnet. Cabling: The Harvester servers are connected to the switch in a port from 1 to 6. The following diagram illustrates the cabling used for this guide: External Switch Configuration​ Typically, a &quot;non VLAN-aware&quot; switch cannot be configured. Create a VLAN Network in Harvester​ You can create a new VLAN network on the Advanced &gt; Networks page, and click the Create button. Specify the name and a VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured). Connect a VM to the subnet of the Harvester hosts​ The &quot;non VLAN-aware&quot; switch will only send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. If you need a VM to connect to the subnet of the Harvester hosts, you have to create a VLAN Network in Harvester with VLAN ID 1. Please refer to this page for additional information on Harvester Networking. note If you create a VLAN Network different from 1, the connection between VMs in different nodes will fail.","keywords":"Harvester harvester Rancher rancher Network network VLAN vlan","version":"v1.0"},{"title":"Overview","type":0,"sectionRef":"#","url":"/v1.0/networking/best-practice/overview","content":"Overview In a real production environment, we generally recommend that you have multiple NICs in your machine, one for node access and one for VM networking. If your machine has multiple NICs, please refer to multiple NICs for best practices. Otherwise, please refer to Single NIC best practice. note If you configure a bond interface with multiple NICs, please refer to the single NIC scenario, unless the Harvester node has multiple bond interfaces. Best Practice​ Multiple NICs with VLAN-aware switchMultiple NICs with non VLAN-aware switchSingle NIC with VLAN-aware switchSingle NIC with non VLAN-aware switch","keywords":"Harvester harvester Rancher rancher Network network VLAN vlan","version":"v1.0"},{"title":"Requirements","type":0,"sectionRef":"#","url":"/v1.0/install/requirements","content":"Requirements As an HCI solution on bare metal servers, Harvester has some minimum requirements as outlined below. Hardware Requirements​ To get the Harvester server up and running the following minimum hardware is required: Type\tRequirementsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory\t32 GB minimum, 64 GB or above preferred Disk Capacity\t140 GB minimum for testing, 500 GB or above preferred for production Disk Performance\t5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for etcd Network Card\t1 Gbps Ethernet minimum for testing, 10Gbps Ethernet recommended for production Network Switch\tTrunking of ports required for VLAN support We recommend server-class hardware for best results. Laptops and nested virtualization are not officially supported. Networking​ Harvester Hosts Inbound Rules​ Protocol\tPort\tSource\tDescriptionTCP\t2379\tHarvester management nodes\tEtcd client port TCP\t2381\tHarvester management nodes\tEtcd health checks TCP\t2380\tHarvester management nodes\tEtcd peer port TCP\t10010\tHarvester management and compute nodes\tContainerd TCP\t6443\tHarvester management nodes\tKubernetes API TCP\t9345\tHarvester management nodes\tKubernetes API TCP\t10252\tHarvester management nodes\tKube-controller-manager health checks TCP\t10257\tHarvester management nodes\tKube-controller-manager secure port TCP\t10251\tHarvester management nodes\tKube-scheduler health checks TCP\t10259\tHarvester management nodes\tKube-scheduler secure port TCP\t10250\tHarvester management and compute nodes\tKubelet TCP\t10256\tHarvester management and compute nodes\tKube-proxy health checks TCP\t10258\tHarvester management nodes\tCloud-controller-manager TCP\t9091\tHarvester management and compute nodes\tCanal calico-node felix TCP\t9099\tHarvester management and compute nodes\tCanal CNI health checks UDP\t8472\tHarvester management and compute nodes\tCanal CNI with VxLAN TCP\t2112\tHarvester management nodes\tKube-vip TCP\t6444\tHarvester management and compute nodes\tRKE2 agent TCP\t6060\tHarvester management and compute nodes\tNode-disk-manager TCP\t10246/10247/10248/10249\tHarvester management and compute nodes\tNginx worker process TCP\t8181\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t8444\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t10245\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t80\tHarvester management and compute nodes\tNginx TCP\t9796\tHarvester management and compute nodes\tNode-exporter TCP\t30000-32767\tHarvester management and compute nodes\tNodePort port range TCP\t22\tHarvester management and compute nodes\tsshd UDP\t68\tHarvester management and compute nodes\tWicked TCP\t3260\tHarvester management and compute nodes\tiscsid Typically, all outbound traffic will be allowed. Integrating Harvester with Rancher​ If you want to integrate Harvester with Rancher, you need to make sure, that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. The VMs of Kubernetes clusters, that are provisioned from Rancher into Harvester, also need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise the cluster won't be manageable by Rancher. For more information see also Rancher Architecture. Guest clusters​ As for the port requirements for the guest clusters deployed inside Harvester virtual machines, refer to the following links. K3s: https://rancher.com/docs/k3s/latest/en/installation/installation-requirements/#networkingRKE: https://rancher.com/docs/rke/latest/en/os/#portsRKE2: https://docs.rke2.io/install/requirements#networking","keywords":"Installation Requirements","version":"v1.0"},{"title":"Multiple NICs with VLAN-aware Switch","type":0,"sectionRef":"#","url":"/v1.0/networking/best-practice/multiple-nics-vlan-aware-switch","content":"Multiple NICs with VLAN-aware Switch In this best practice guide on how to configure &quot;VLAN-aware&quot;, we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture​ Hardware: Three Harvester servers with daul ports network card.One or more VLAN-aware switch(es). We will use &quot;Cisco like&quot; configuration as example. Network Specification: Assume that the subnet of the Harvester hosts is in VLAN 100.Assume that the VMs are in the VLAN 101-200. Cabling: The Harvester servers are connected to the switch in a port from 1 to 6. The following diagram illustrates the cabling used for this guide: External Switch Configuration​ For the external switch configuration, we'll use a &quot;Cisco-like&quot; configuration as an example. You can apply the following configurations to your switch: For harvester-mgmt ports: switch# config terminal switch(config)# interface ethernet1/&lt;Port Number&gt; switch(config-if)# switchport switch(config-if)# switchport mode access switch(config-if)# switchport access 100 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config note In this case, you need to avoid using harvester-mgmt as the VLAN Network interface. This setting will only allow the traffic in the same subnet of harvester-mgmt and disallow other VLAN traffic. For VLAN network ports: switch# config terminal switch(config)# interface ethernet1/&lt;Port Number&gt; switch(config-if)# switchport switch(config-if)# switchport mode trunk switch(config-if)# switchport trunk allowed vlan 100-200 switch(config-if)# switchport trunk native vlan 1 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config note We use the VLAN Trunk setup to set up the network ports for the VLAN Network. In this case, you can simply set VLAN 100 for the VMs in the Harvester VLAN network to connect to the same subnet of harvester-mgmt. Create a VLAN Network in Harvester​ You can create a new VLAN network in the Advanced &gt; Networks page, and click the Create button. Specify the name and a VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured). Connect a VM to the subnet of the Harvester hosts​ Once you finished the configuration in the previous section, the external switch will send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. Therefore, if you need VMs to connect to the VLAN ID 1, you need to create a VLAN ID 1 Network in Harvester also. note We strongly recommend against using VLAN 1 in this scenario. Connect a VM to specific VLAN network​ You need to create a VLAN network with a specific VLAN ID and associate the VM with that VLAN network. Please refer to this page for additional information on Harvester Networking.","keywords":"Harvester harvester Rancher rancher Network network VLAN vlan","version":"v1.0"},{"title":"Single NIC with Non VLAN-aware Switch","type":0,"sectionRef":"#","url":"/v1.0/networking/best-practice/single-nic-non-vlan-aware-switch","content":"Single NIC with Non VLAN-aware Switch In this best practice guide for &quot;non VLAN-aware&quot; switch, also known as &quot;dummy&quot; switch, we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture​ Hardware: Three Harvester servers with only one single port network card.One or more &quot;non VLAN-aware&quot; switch(es). Network Specification: The host and the VM networks are in the same subnet. Cabling: The Harvester servers are connected to the switch in a port from 1 to 3. The following diagram illustrates the cabling used for this guide: External Switch Configuration​ Typically, a &quot;non VLAN-aware&quot; switch cannot be configured. Create a VLAN Network in Harvester​ You can create a new VLAN network in the Advanced &gt; Networks page, and click the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured). Connect a VM to the subnet of the Harvester hosts​ The &quot;non VLAN-aware&quot; switch will only send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. If you need a VM to connect to the subnet of the Harvester hosts, you have to create a VLAN Network in Harvester with VLAN ID 1. Please refer to this page for additional information on Harvester Networking. note If you create a VLAN Network different from 1, the connection between VMs in different nodes will fail.","keywords":"Harvester harvester Rancher rancher Network network VLAN vlan","version":"v1.0"},{"title":"Single NIC with VLAN-aware Switch","type":0,"sectionRef":"#","url":"/v1.0/networking/best-practice/single-nic-vlan-aware-switch","content":"Single NIC with VLAN-aware Switch In this best practice guide on how to configure &quot;VLAN-aware&quot;, we will introduce Harvester VLAN network and external switch configuration for common scenario. Architecture​ Hardware: Three Harvester servers with only one single port network card.One or more VLAN-aware switch(es). We will use &quot;Cisco like&quot; configuration as example. Network Specification: Assume that the subnet of the Harvester hosts is in VLAN 100.Assume that the VMs are in the VLAN 101-200. Cabling: The Harvester servers are connected to the switch in a port from 1 to 3. The following diagram illustrates the cabling used for this guide: External Switch Configuration​ For the external switch configuration, we'll use a &quot;Cisco like&quot; configuration as example. You can apply the following configurations to your switch: switch# config terminal switch(config)# interface ethernet1/&lt;Port Number&gt; switch(config-if)# switchport switch(config-if)# switchport mode trunk switch(config-if)# switchport trunk allowed vlan 100-200 switch(config-if)# switchport trunk native vlan 100 switch(config-if)# no shutdown switch(config-if)# end switch# copy running-config startup-config Create a VLAN Network in Harvester​ You can create a new VLAN network in the Advanced &gt; Networks page, and click the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same VLAN ID in different namespaces if you have Rancher multi-tenancy configured). Connect a VM to the subnet of the Harvester hosts​ Once you finished the configuration in the previous section, the external switch will send out untagged network traffic to the subnet of the Harvester hosts. In Harvester, the untagged traffic is received in VLAN 1. Therefore, if you need VMs to connect to the VLAN ID 100, you need to create a VLAN ID 1 Network in Harvester. The external switch will remove the VLAN 100 tag from the packet for egress and harvester-br0 will add the VLAN 1 tag to the packet and treat it as VLAN 1 as shown in the following diagram: caution Do not create a VLAN Network with VLAN 100 and associate any VM to it. The connectivity will not always be ensured and depends on the external switch behavior to add/remove VLAN tag from packets. Connect a VM to specific VLAN network​ You need to create a VLAN Network with specific VLAN ID and associate the VM to that VLAN network. Please refer to this page for additional information on Harvester Networking.","keywords":"Harvester harvester Rancher rancher Network network VLAN vlan","version":"v1.0"},{"title":"Creating an K3s Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.0/rancher/node/k3s-cluster","content":"Creating an K3s Kubernetes Cluster You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ using the built-in Harvester node driver. note Harvester K3s node driver is in tech preview.VLAN network is required for Harvester node driver. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester&quot; or &quot;External Harvester&quot;.Click Create. Create K3s Kubernetes Cluster​ You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. Add Node Affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules. Using Harvester K3s Node Driver in Air Gapped Environment​ K3s provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.0"},{"title":"PXE Boot Installation","type":0,"sectionRef":"#","url":"/v1.0/install/pxe-boot-install","content":"PXE Boot Installation Starting from version 0.2.0, Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples. Prerequisite​ info Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers​ An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10, and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/. Preparing Boot Files​ Download the required files from the Harvester releases page. The ISO: harvester-&lt;version&gt;-amd64.isoThe kernel: harvester-&lt;version&gt;-vmlinuz-amd64The initrd: harvester-&lt;version&gt;-initrd-amd64The rootfs squashfs image: harvester-&lt;version&gt;-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts​ When performing an automatic installation, there are two modes: CREATE: we are installing a node to construct an initial Harvester cluster.JOIN: we are installing a node to join an existing Harvester cluster. CREATE Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token: token os: hostname: node1 ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password: rancher ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install: mode: create networks: harvester-mgmt: # (Mandatory) The management bond name. interfaces: - name: ens5 method: dhcp bond_options: mode: balance-tlb miimon: 100 harvester-vlan: # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces: # host to host, consider creating a bonding device. Users can then select - name: ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method: none bond_options: mode: balance-tlb miimon: 100 device: /dev/sda iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso vip: 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode: static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create. note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp). See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter. JOIN Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url: https://10.100.0.99:443 # Should be the VIP set up in &quot;CREATE&quot; config token: token os: hostname: node2 ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers: - 1.1.1.1 - 8.8.8.8 password: rancher install: mode: join networks: harvester-mgmt: # (Mandatory) The management bond name. interfaces: - name: ens5 method: dhcp bond_options: mode: balance-tlb miimon: 10 harvester-vlan: # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces: # host to host, consider creating a bonding device. Users can then select - name: ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method: none bond_options: mode: balance-tlb miimon: 100 device: /dev/sda iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join. DHCP Server Configuration​ The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16; subnet 10.100.0.0 netmask 255.255.255.0 { option routers 10.100.0.10; option domain-name-servers 192.168.2.1; range 10.100.0.100 10.100.0.253; } group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } group { # join group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-join-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-join&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node2 { hardware ethernet 52:54:00:69:d5:92; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration​ For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support​ UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program​ Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration​ If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } elsif substring (option vendor-class-identifier, 0, 10) = &quot;HTTPClient&quot; { # UEFI HTTP Boot option vendor-class-identifier &quot;HTTPClient&quot;; filename &quot;http://10.100.0.10/harvester/ipxe.efi&quot;; } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi. The iPXE Script for UEFI Boot​ It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-&lt;version&gt;-vmlinuz initrd=harvester-&lt;version&gt;-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot The parameter initrd=harvester-&lt;version&gt;-initrd is required. Useful Kernel Parameters​ Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7). ip=dhcp​ If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces. rd.net.dhcp.retry=&lt;cnt&gt;​ Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=&lt;cnt&gt;to retry DHCP request for &lt;cnt&gt; times.","keywords":"Harvester harvester Rancher rancher Install Harvester Installing Harvester Harvester Installation PXE Boot Install","version":"v1.0"},{"title":"Harvester Network","type":0,"sectionRef":"#","url":"/v1.0/networking/harvester-network","content":"Harvester Network Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management NetworkVLAN Network Management Network​ Harvester uses canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. VLAN Network​ The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch.VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't.The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs.Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling Default VLAN Network​ You can enable VLAN network via Settings &gt; vlan. Select enabled and you will be able to select one network interface from the nodes as the default VLAN NIC config. For better network performances and isolation, we recommend to choose different network interfaces for the VLAN and the one used for the management network (i.e., harvester-mgmt). note When selecting the network interface, the value in parentheses represents the distribution percentage of the network interface on all hosts. If a network interface with a value less than 100% is selected, the network interface needs to be manually specified on the host where the VLAN network configuration fails.Modifying the default VLAN network setting will not update the existing configured host network.Harvester VLAN network supports bond interfaces. Currently it can only be created automatically via PXE Boot Configuration. You may also login to the node and create it manually. You can also customize each node's VLAN network via the Hosts &gt; Network tab. Create a VLAN Network​ A new VLAN network can be created via the Advanced &gt; Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network (You can specify the same vlan ID on different namespaces of Rancher multi-tenancy support). Configure a route in order to allow the hosts to connect to the VLAN network using IPv4 addresses. The CIDR and gateway of the VLAN network are mandatory parameters for the route configuration. You can configure the route by choosing one of the following options: auto(DHCP) mode: the Harvester network controller will get the CIDR and gateway values from the DHCP server using the DHCP protocol. Optionally, you can specify the DHCP server address.manual mode: You need to specify the CIDR and gateway values manually. Create a VM with VLAN Network​ Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page.Specify the required parameters and click the Networks tab.Either configure the default network to be a VLAN network or select an additional network to add. note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You need to be careful to configure virtual machines with multiple NICs to avoid connectivity issues. You can refer to the knowledge base for more details.You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. You can choose to add one or multiple network interface cards. The additional network interface cards can be enabled by default via the cloud-init network data setting. e.g., version: 1 config: - type: physical name: enp1s0 # name is varies upon OS image subnets: - type: dhcp - type: physical name: enp2s0 subnets: - type: static address: 10.0.0.100/24 # IP address is varies upon your environment Harvester is fully compatible with the cloud-init network configs. You can refer to the documentation for more details. note If you add additional NICs after the VM has started, you will need to manually configure IPs for the additional NICs. Configure DHCP servers on Networks​ By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually on a node or using a containerized method, e.g, like #947.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v1.0"},{"title":"Harvester Configuration","type":0,"sectionRef":"#","url":"/v1.0/install/harvester-configuration","content":"Harvester Configuration Configuration Example​ Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url: https://cluster-VIP:443 token: TOKEN_VALUE os: ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files: - encoding: &quot;&quot; content: test content owner: root path: /etc/test.txt permissions: '0755' hostname: myhost modules: - kvm - nvme sysctls: kernel.printk: &quot;4 4 1 7&quot; kernel.kptr_restrict: &quot;1&quot; dns_nameservers: - 8.8.8.8 - 1.1.1.1 ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password: rancher environment: http_proxy: http://myserver https_proxy: http://myserver labels: foo: bar mylabel: myvalue install: mode: create networks: harvester-mgmt: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7C&quot; method: dhcp force_efi: true device: /dev/vda silent: true iso_url: http://myserver/test.iso poweroff: true no_format: true debug: true tty: ttyS0 vip: 10.10.0.19 vip_hw_addr: 52:54:00:ec:0e:0b vip_mode: dhcp force_mbr: false system_settings: auto-disk-provision-paths: &quot;&quot; Configuration Reference​ Below is a reference of all configuration keys. caution Security Risks: The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. note Configuration Priority: When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname. server_url​ Definition​ server_url is the URL of the Harvester cluster, which is used for the new node to join the cluster. This configuration is mandatory when the installation is in JOIN mode. The default format of server_url is https://cluster-VIP:443. note To ensure a high availability (HA) Harvester cluster, please use either the Harvester cluster VIP or a domain name in server_url. Example​ server_url: https://cluster-VIP:443 install: mode: join token​ Definition​ The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example​ token: myclustersecret Or a node token token: &quot;K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4&quot; os.ssh_authorized_keys​ Definition​ A list of SSH authorized keys that should be added to the default user, rancher. SSH keys can be obtained from GitHub user accounts by using the formatgithub:${USERNAME}. This is done by downloading the keys from https://github.com/${USERNAME}.keys. Example​ os: ssh_authorized_keys: - &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D&quot; - &quot;github:ibuildthecloud&quot; os.write_files​ A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: &quot;&quot;: content data are written in plain text. In this case, the encoding field can be also omitted.b64, base64: content data are base64-encoded.gz, gzip: content data are gzip-compressed.gz+base64, gzip+base64, gz+b64, gzip+b64: content data are gzip-compressed first and then base64-encoded. Example os: write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner: root:root path: /etc/connman/main.conf permissions: '0644' - content: | # My new /etc/sysconfig/samba file SMDBOPTIONS=&quot;-D&quot; path: /etc/sysconfig/samba - content: !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path: /bin/arch permissions: '0555' - content: | 15 * * * * root ship_logs path: /etc/crontab os.hostname​ Definition​ Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value. Example​ os: hostname: myhostname os.modules​ Definition​ A list of kernel modules to be loaded on start. Example​ os: modules: - kvm - nvme os.sysctls​ Definition​ Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf. Values must be specified as strings. Example​ os: sysctls: kernel.printk: 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict: &quot;1&quot; # force the YAML parser to read as a string os.dns_nameservers​ Definition​ Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example​ os: dns_nameservers: - 8.8.8.8 - 1.1.1.1 os.ntp_servers​ Definition​ Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines. Example​ os: ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org os.password​ Definition​ The password for the default user, rancher. By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from/etc/shadow. You can also encrypt a password using openssl passwd -6. Example​ Encrypted: os: password: &quot;$6$kZYUnRaTxNdg4W8H$WSEJydGWsNpaRbbbRdTDLJ2hDLbkizxSFGW2RtexlqG6njEATaGQG9ssztjaKDCsaNUPBZ1E1YdsvSLMAi/IO/&quot; Or clear text: os: password: supersecure os.environment​ Definition​ Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example​ os: environment: http_proxy: http://myserver https_proxy: http://myserver note This example sets the HTTP(S) proxy for foundational OS components. To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy. os.labels​ Definition​ Labels to be added to this Node. install.mode​ Definition​ Harvester installation mode: create: Creating a new Harvester installation.join: Join an existing Harvester installation. Need to specify server_url. Example​ install: mode: create install.networks​ Definition​ Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method: Method to assign an IP to this network. The following are supported: static: Manually assign an IP and gateway.dhcp: Request an IP from the DHCP server.none: Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip: Static IP for this network. Required if static method is chosen.subnet_mask: Subnet mask for this network. Required if static method is chosen.gateway: Gateway for this network. Required if static method is chosen.interfaces: An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name: The name of the slave interface for the bonded network.interfaces.hwAddr: The hardware MAC address of the interface. bond_options: Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlbmiimon: 100 mtu: The MTU for the interface. note A network called harvester-mgmt is mandatory to establish a valid management network. note Harvester uses the systemd net naming scheme. Please make sure the interface name is present on the target machine before installation. Example​ install: mode: create networks: harvester-mgmt: # The management bond name. This is mandatory. interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7D&quot; # The hwAddr is optional method: dhcp bond_options: mode: balance-tlb miimon: 100 mtu: 1492 harvester-vlan: # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces: - name: ens6 hwAddr: &quot;B8:CA:3A:6A:64:7E&quot; # The hwAddr is optional method: none bond_options: mode: balance-tlb miimon: 100 bond0: interfaces: - name: ens8 hwAddr: &quot;B8:CA:3A:6A:64:7F&quot; # The hwAddr is optional method: static ip: 10.10.18.2 subnet_mask: 255.255.255.0 gateway: 192.168.11.1 mtu: 9000 install.force_efi​ Force EFI installation even when EFI is not detected. Default: false. install.device​ The device to install the OS. install.silent​ Reserved. install.iso_url​ ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff​ Shutdown the machine after installation instead of rebooting install.no_format​ Do not partition and format, assume layout exists already. install.debug​ Run the installation with additional logging and debugging enabled for the installed system. install.tty​ Definition​ The tty device used for the console. Example​ install: tty: ttyS0,115200n8 install.vip​ install.vip_mode​ install.vip_hw_addr​ Definition​ install.vip: The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://&lt;VIP&gt;.install.vip_mode dhcp: Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided.static: Harvester uses a static VIP. install.vip_hw_addr: The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp. See Management Address for more information. Example​ Configure a static VIP. install: vip: 192.168.0.100 vip_mode: static Configure a DHCP VIP. install: vip: 10.10.0.19 vip_mode: dhcp vip_hw_addr: 52:54:00:ec:0e:0b install.force_mbr​ Definition​ By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. note Harvester creates an additional partition for storing VM data ifinstall.data_disk is configured to use the same storage device as the one set for install.device. When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example​ install: force_mbr: true install.data_disk​ Available as of v1.0.1 Definition​ Sets the default storage device to store the VM data. Default: Same storage device as the one set for install.device Example​ install: data_disk: /dev/sdb system_settings​ Definition​ You can overwrite the default Harvester system settings by configuring system_settings. See the Settings page for additional information and the list of all the options. note Overwriting system settings only works when Harvester is installed in &quot;create&quot; mode. If you install Harvester in &quot;join&quot; mode, this setting is ignored. Installing in &quot;join&quot; mode will adopt the system settings from the existing Harvester system. Example​ The example below overwrites http-proxy and ui-source settings. The values must be a string. system_settings: http-proxy: '{&quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc&quot;}' ui-source: auto cluster_networks​ Available as of v1.0.1 Definition​ You can setup the default network in Harvester by configuring cluster_networks. Network configuration reference: vlan: Setup for VLAN network. The following fields are supported: enable: enable VLAN network settings or not. Default value: false.description: Additional information for ClusterNetworks. Default value: &quot;&quot;.config: ClusterNetworks configuration to be used. Valid configuration fields are: defaultPhysicalNIC (string, required): assign a physical NIC to be external entry of VLAN network. note To configure the cluster_networks, Harvester needs to be installed in &quot;create&quot; mode. If you install Harvester in &quot;join&quot; mode, this setting is ignored. Installing in &quot;join&quot; mode will apply the cluster_networks configuration from the existing Harvester system. Example​ The following example sets the default physical NIC name of the VLAN network: cluster_networks: vlan: enable: true description: &quot;some description about this cluster network&quot; config: defaultPhysicalNIC: ens3 ","keywords":"Harvester harvester Rancher rancher Harvester Configuration","version":"v1.0"},{"title":"Harvester CSI Driver","type":0,"sectionRef":"#","url":"/v1.0/rancher/csi-driver","content":"Harvester CSI Driver The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. note Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only(ROX) and read-write(RWX) support. Deploying with Harvester RKE1 Node Driver​ Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; Deploying with Harvester RKE2 Node Driver​ When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected. Install CSI Driver Manually in the RKE2 Cluster​ If you prefer to deploy the Harvester CSI driver without enabling the Harvester cloud provider, you can choose either Default - RKE2 Embedded or External in the Cloud Provider field. If you are using Rancher v2.6, select None instead. Prerequisites​ Ensure that you have the following prerequisites in place: You have kubectl and jq installed on your system.You have the kubeconfig file for your bare-metal Harvester cluster. export KUBECONFIG=/path/to/your/harvester-kubeconfig Perform the following steps to deploy the Harvester CSI Driver manually: Deploy Harvester CSI Driver​ Generate cloud-config. You can generate the kubeconfig file using the generate_addon_csi.sh script. It is available on the harvester/harvester-csi-driver repo. You can follow the steps below to get the cloud-config and cloud-init data: The &lt;serviceaccount name&gt; usually corresponds to your guest cluster name (the value of &quot;Cluster Name&quot; in the figure below), and &lt;namespace&gt; should match the namespace (the value of &quot;Namespace&quot;) of the guest cluster. # ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; RKE2 ########## cloud-config ############ apiVersion: v1 clusters: - cluster: &lt;token&gt; server: https://&lt;YOUR HOST HARVESTER VIP&gt;:6443 name: default contexts: - context: cluster: default namespace: default user: rke2-guest-01-default-default name: rke2-guest-01-default-default current-context: rke2-guest-01-default-default kind: Config preferences: {} users: - name: rke2-guest-01-default-default user: token: &lt;token&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: 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 owner: root:root path: /var/lib/rancher/rke2/etc/config-files/cloud-provider-config permissions: '0644' Copy and paste the output below cloud-init user data to Machine Pools &gt;Show Advanced &gt; User Data. Set up cloud-provider-config. The cloud-provider-config should be created after you apply the above cloud-init user data. You can check again on the path /var/lib/rancher/rke2/etc/config-files/cloud-provider-config. note If you want to change the cloud-provider-config path, you should update the cloud-init user data. Install Harvester CSI Driver. Install the Harvester CSI Driver chart from the Rancher marketplace. (Note: by default, you do not need to change the cloud-config path). note If you prefer not to install the Harvester CSI driver using Rancher (Apps &gt; Charts), you can use Helm instead. The Harvester CSI driver is packaged as a Helm chart. For more information, see https://charts.harvesterhci.io. By following the above steps, you should be able to see those CSI driver pods are up and running, and you can verify it by provisioning a new PVC using the default storageClass harvester.. Deploying with Harvester K3s Node Driver​ You can follow the Deploy Harvester CSI Driver steps described in the RKE2 section for Prerequisites The only difference is that you need to change the script command as follows: # ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; k3s ","keywords":"Harvester harvester Rancher Integration","version":"v1.0"},{"title":"Harvester Node Driver","type":0,"sectionRef":"#","url":"/v1.0/rancher/node/node-driver","content":"Harvester Node Driver The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver, and the project repo is available at harvester/docker-machine-driver-harvester. You can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.3+ with the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. While you can upload and view .ISO images in the Harvester UI, the same capability is not available in the Rancher UI. For more information on this, see the Rancher docs. note Harvester v1.0.0 is compatible with Rancher v2.6.3+ only. Harvester Node Driver​ The Harvester node driver is enabled by default from Rancher v2.6.3. You can go to Cluster Management &gt; Drivers &gt; Node Drivers page to manage the Harvester node driver manually. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. RKE1 Kubernetes Cluster​ Click to learn how to create RKE1 Kubernetes Clusters. RKE2 Kubernetes Cluster​ Click to learn how to create RKE2 Kubernetes Clusters. K3s Kubernetes Cluster​ Click to learn how to create k3s Kubernetes Clusters. Topology Spread Constraints​ Available as of v1.0.3 In your guest Kubernetes cluster, you can use topology spread constraints to control how workloads are spread across the Harvester VMs among failure-domains such as regions and zones. This can help to achieve high availability as well as efficient resource utilization of your cluster resources. The minimum RKE2 versions required to support the sync topology label feature are as follows: Supported RKE2 Version&gt;= v1.24.3+rke2r1 &gt;= v1.23.9+rke2r1 &gt;= v1.22.12+rke2r1 In addition, the cloud provider version installed via the Apps of RKE/K3s must be &gt;= v0.1.4 Sync Topology Labels to the Guest Cluster Node​ During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone topology labels are supported. note Label synchronization will only take effect during guest node initialization. To avoid node drifts to another region or zone, it is recommended to add the node affinity rules during the cluster provisioning, so that the VMs can be scheduled to the same zone even after rebuilding. Configuring topology labels on the Harvester nodes through Hosts &gt; Edit Config &gt; Labels. e.g., add the topology labels as follows: topology.kubernetes.io/region: us-east-1 topology.kubernetes.io/zone: us-east-1a Creating a guest Kubernetes cluster using the Harvester node driver and it is recommended to add the node affinity rules, this will help to avoid node drifting to other zones after VM rebuilding. After the cluster is successfully deployed, confirm that guest Kubernetes node labels are successfully synchronized from the Harvester VM node. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints.","keywords":"Harvester harvester Rancher rancher Harvester Node Driver","version":"v1.0"},{"title":"Creating an RKE1 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.0/rancher/node/rke1-cluster","content":"Creating an RKE1 Kubernetes Cluster You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ with the built-in Harvester node driver. note VLAN network is required for Harvester node driver. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials, you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster.In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential name.Select &quot;Imported Harvester&quot; or &quot;External Harvester&quot;.Click Create. Create Node Template​ You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials.Configure Instance Options: Configure the CPU, memory, and diskSelect an OS image that is compatible with the cloud-init config.Select a network that the node driver is able to connect to; currently, only VLAN is supported.Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu. (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs:Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Add Node Affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration &gt; Node Templates: Check the Advanced Options tab and click Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Create RKE1 Kubernetes Cluster​ Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE1.Select Harvester node driver.Enter Cluster Name (required).Enter Name Prefix (required).Enter Template (required).Select etcd and Control Plane (required).On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver.Click Create. Using Harvester RKE1 Node Driver in Air Gapped Environment​ RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent and docker installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=&quot;http://192.168.0.1:3128&quot; HTTPS_PROXY=&quot;http://192.168.0.1:3128&quot; append: true ","keywords":"","version":"v1.0"},{"title":"Virtualization Management","type":0,"sectionRef":"#","url":"/v1.0/rancher/virtualization-management","content":"Virtualization Management For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6 and above. As a prerequisite, Harvester v1.0.0 integration requires Rancher server v2.6.3 or above. In production, use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix Metal caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM)An on-premises VMA bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection.From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.6 Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create. You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy​ In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication, users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions: Define user authorization outside the scope of any particular cluster. Cluster and Project Roles: Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings.A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example​ The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users &amp; Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project.A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab.Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save.Open an incognito browser and log in as project-owner.After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned.Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed.Create a VM with one of the images that you have uploaded.Log in with another user, e.g., project-readonly, and this user will only have the read permission of this project. Delete Imported Harvester Cluster​ Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management &gt; Harvester Clusters. Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. caution Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","keywords":"Harvester Rancher","version":"v1.0"},{"title":"Creating an RKE2 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.0/rancher/node/rke2-cluster","content":"Creating an RKE2 Kubernetes Cluster Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. note Harvester RKE2 node driver is in tech preview.VLAN network is required for Harvester node driver. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester&quot; or &quot;External Harvester&quot;.Click Create. Create RKE2 Kubernetes Cluster​ Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration.Currently only imported Harvester clusters are supported automatically. Add Node Affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Using Harvester RKE2 Node Driver in Air Gapped Environment​ RKE2 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine. However, it may not be feasible to install qemu-guest-agent in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.0"},{"title":"Harvester Cloud Provider","type":0,"sectionRef":"#","url":"/v1.0/rancher/cloud-provider","content":"Harvester Cloud Provider RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2.How to use the Harvester load balancer. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying to the RKE1 Cluster with Harvester Node Driver​ When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. note You should specify the Cluster name. The default value kubernetes will be set if no Cluster name is entered. The Cluster name is used to distinguish the ownership of the Harvester load balancers. Install Harvester csi driver from the Rancher marketplace if needed. Deploying to the RKE2 Cluster with Harvester Node Driver [Experimental]​ When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Deploying to the K3s Cluster with Harvester Node Driver [Experimental]​ Choose the Kubernetes version to be k3s and click the Edit as YAML button to config the K3s cluster YAML (For existing cluster, you can also click the Edit YAML button to update it): Edit K3s cluster YAML. Set disable-cloud-provider: true to disable default k3s cloud provider.Add cloud-provider=external to use harvester cloud provider. Generate addon configuration and put it in K3s VMs /etc/kubernetes/cloud-config. Deploy external cloud provider​ Deploying external cloud provider is similar for both RKE2 and K3s based clusters. Once the in-tree cloud provider has been disabled by following the above steps, you can deploy the external cloud provider via: A sample additional manifest is as follows: apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.1.12 helmVersion: v3 The cloud provider needs a kubeconfig file to work, a limited scoped one can be generated using the generate_addon.sh script available in the harvester/cloud-provider-harvester repo. NOTE: The script needs access to the harvester cluster kubeconfig to work. In addition the namespace needs to be the namespace in which the workload cluster will be created. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh &lt;serviceaccount name&gt; &lt;namespace&gt; The output will look as follows: (⎈ |local:default)➜ cloud-provider-harvester git:(master) ✗ ./deploy/generate_addon.sh harvester-cloud-provider default Creating target directory to hold files in ./tmp/kube...done Creating a service account in default namespace: harvester-cloud-provider W0506 16:44:15.429068 3008674 helpers.go:598] --dry-run is deprecated and can be replaced with --dry-run=client. serviceaccount/harvester-cloud-provider configured Creating a role in default namespace: harvester-cloud-provider role.rbac.authorization.k8s.io/harvester-cloud-provider unchanged Creating a rolebinding in default namespace: harvester-cloud-provider W0506 16:44:23.798293 3008738 helpers.go:598] --dry-run is deprecated and can be replaced with --dry-run=client. rolebinding.rbac.authorization.k8s.io/harvester-cloud-provider configured Getting secret of service account harvester-cloud-provider on default Secret name: harvester-cloud-provider-token-5zkk9 Extracting ca.crt from secret...done Getting user token from secret...done Setting current context to: local Cluster name: local Endpoint: https://HARVESTER_ENDPOINT/k8s/clusters/local Preparing k8s-harvester-cloud-provider-default-conf Setting a cluster entry in kubeconfig...Cluster &quot;local&quot; set. Setting token credentials entry in kubeconfig...User &quot;harvester-cloud-provider-default-local&quot; set. Setting a context entry in kubeconfig...Context &quot;harvester-cloud-provider-default-local&quot; created. Setting the current-context in the kubeconfig file...Switched to context &quot;harvester-cloud-provider-default-local&quot;. ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: CACERT server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: TOKEN This cloud-config file can now be injected via the user-data available in the advanced options for the nodepool. With these settings in place a K3s / RKE2 cluster should provision successfully while using the external cloud provider. Upgrade Cloud Provider​ Upgrade RKE2​ The cloud provider can be upgraded by upgrading the RKE2 version. You can upgrade the RKE2 cluster via the Rancher UI as follows: Click ☰ &gt; Cluster Management.Find the guest cluster that you want to upgrade and select ⋮ &gt; Edit Config.Select Kubernetes Version.Click Save. Upgrade RKE/K3s​ RKE/K3s upgrade cloud provider via the Rancher UI, as follows: Click ☰ &gt; RKE/K3s Cluster &gt; Apps &gt; Installed Apps.Find the cloud provider chart and select ⋮ &gt; Edit/Upgrade.Select Version.Click Next &gt; Update. Load Balancer Support​ After deploying the Harvester Cloud provider, you can use the Kubernetes LoadBalancer service to expose a microservice inside the guest cluster to the external world. When you create a Kubernetes LoadBalancer service, a Harvester load balancer is assigned to the service and you can edit it through the Add-on Config in the Rancher UI. IPAM​ Harvester's built-in load balancer supports both pool and dhcp modes. You can select the mode in the Rancher UI. Harvester adds the annotation cloudprovider.harvesterhci.io/ipam to the service behind. pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server. note It is not allowed to modify the IPAM mode. You need to create a new service if you want to modify the IPAM mode. Health Checks​ The Harvester load balancer supports TCP health checks. You can specify the parameters in the Rancher UI if you enable the Health Check option. Alternatively, you can specify the parameters by adding annotations to the service manually. The following annotations are supported: Annotation Key\tValue Type\tRequired\tDescriptioncloudprovider.harvesterhci.io/healthcheck-port\tstring\ttrue\tSpecifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold\tstring\tfalse\tSpecifies the health check success threshold. The default value is 1. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-failure-threshold\tstring\tfalse\tSpecifies the health check failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds\tstring\tfalse\tSpecifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds\tstring\tfalse\tSpecifies the timeout of every health check. The default value is 3 seconds.","keywords":"Harvester harvester RKE rke RKE2 rke2 Harvester Cloud Provider","version":"v1.0"},{"title":"Harvester Terraform Provider","type":0,"sectionRef":"#","url":"/v1.0/terraform/terraform-provider","content":"Harvester Terraform Provider Requirements​ Terraform &gt;= 0.13.xGo 1.18 to build the provider plugin Install The Provider​ copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = &quot;harvester/harvester&quot; version = &quot;&lt;replace to the latest release version&gt;&quot; } } } provider &quot;harvester&quot; { # Configuration options } Using the provider​ More details about the provider-specific configurations can be found in the docs. Github Repo: https://github.com/harvester/terraform-provider-harvester","keywords":"","version":"v1.0"},{"title":"Rancher Integration","type":0,"sectionRef":"#","url":"/v1.0/rancher/rancher-integration","content":"Rancher Integration Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Rancher &amp; Harvester Support Matrix​ For the support matrix, please see Harvester &amp; Rancher Support Matrix. note Harvester v1.0.0 is compatible with Rancher v2.6.3 or above only. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support. Deploying Rancher Server​ To use Rancher with Harvester, please install the Rancher and Harvester in two separated servers. If you want to try out the integration features, you can create a VM in Harvester and install Rancher v2.6.3 or above (the latest stable version is recommended). Use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix Metal caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM)An on-premises VMA bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection.From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.6 Virtualization Management​ With Rancher's Virtualization Management feature, you can now import and manage Harvester clusters. By clicking on one of the clusters, you are able to view and manage the imported Harvester cluster resources like Hosts, VMs, images, volumes, etc. Additionally, the Virtualization Management leverages existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please check the virtualization management page. Creating Kubernetes Clusters using the Harvester Node Driver​ Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage guest Kubernetes clusters. Starting with Rancher v2.6.1, the Harvester node driver has been added by default. Users can reference the node-driver page for more details.","keywords":"Harvester harvester Rancher rancher Rancher Integration","version":"v1.0"},{"title":"Harvester","type":0,"sectionRef":"#","url":"/v1.0/troubleshooting/harvester","content":"Harvester Generate a support bundle​ Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Access Embedded Rancher​ You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer. note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here. Access Embedded Longhorn​ You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn. note We only support to use the embedded Longhorn UI for debugging and validation purpose . I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers​ If you changedSSL/TLS enabled protocols and ciphers settingsand you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl: # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{&quot;protocols&quot;:&quot;TLS99&quot;,&quot;ciphers&quot;:&quot;WRONG_CIPHER&quot;}' # &lt;- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly.","keywords":"","version":"v1.0"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/v1.0/troubleshooting/os","content":"Operating System Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit. The following sections contain information and tips to help users troubleshoot OS-related issues. How to log into a Harvester node​ Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~&gt; sudo blkid # Or become root rancher@node1:~&gt; sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only?​ The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: caution Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0, we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat &gt; /oem/91_hack.yaml &lt;&lt;'EOF' name: &quot;Rootfs Layout Settings for debugrw&quot; stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline &amp;&amp; grep -q rd.cos.debugrw /proc/cmdline' name: &quot;Layout configuration for debugrw&quot; environment_file: /run/cos/cos-layout.env environment: RW_PATHS: &quot; &quot; EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters​ note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry &quot;Harvester ea6e7f5-dirty&quot; --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry​ To change the default entry, first check the --id attribute of a menu entry. Grub menu entries are located in the following files: /run/initramfs/cos-state/grub2/grub.cfg: Contains the default, fallback, and recovery entries/run/initramfs/cos-state/grubmenu: Contains the debug entry In the following example, the id of the entry is debug. # cat \\ /run/initramfs/cos-state/grub2/grub.cfg \\ /run/initramfs/cos-state/grubmenu &lt;...&gt; menuentry &quot;${display_name} (debug)&quot; --id debug { search --no-floppy --set=root --label COS_STATE set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd ${extra_cmdline} ${extra_passive_cmdline} ${crash_kernel_params} initrd (loop0)$initramfs } You can configure the default entry by running the following commands: # mount -o remount,rw /run/initramfs/cos-state # grub2-editenv /run/initramfs/cos-state/grub_oem_env set saved_entry=debug If necessary, you can undo the change by running the command grub2-editenv /run/initramfs/cos-state/grub_oem_env unset saved_entry. How to debug a system crash or hang​ Collect crash log​ If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps​ For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/&lt;time&gt; directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","keywords":"","version":"v1.0"},{"title":"Upgrade from v1.0.0 to v1.0.1","type":0,"sectionRef":"#","url":"/v1.0/upgrade/previous-releases/v1-0-0-to-v1-0-1","content":"Upgrade from v1.0.0 to v1.0.1 This document describes how to upgrade from Harvester v1.0.0 to v1.0.1. Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: caution Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -&gt; Virtual Machines -&gt; Select VMs -&gt; Actions -&gt; Stop).Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 25 GB of free space (df -h /usr/local/). caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Create a version​ Log in to one of your server nodes. Become root and create a version: rancher@node1:~&gt; sudo -i node1:~ # kubectl create -f https://releases.rancher.com/harvester/v1.0.1/version.yaml version.harvesterhci.io/1.0.1 created note By default, the ISO image is downloaded from the Harvester release server. To speed up the upgrade and make the upgrade progress smoother, the user can also download the ISO file to a local HTTP server first and substitute the isoURL value in the version.yaml manifest. e.g., # Download the ISO from release server first, assume it's store in http://10.10.0.1/harvester.iso $ sudo -i $ curl -fL https://releases.rancher.com/harvester/v1.0.1/version.yaml -o version.yaml $ vim version.yaml apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.1 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso releaseDate: '20220408' Start the upgrade​ Make sure to read the Warning paragraph at the top of this document first. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Known issues​ Fail to download upgrade image​ Description Downloading the upgrade image can't complete. Workaround We can delete the current upgrade and start over. # log in to one of the server nodes $ sudo -i # list current upgrade, the name changes between deployments $ kubectl get upgrades.harvesterhci.io -n harvester-system NAMESPACE NAME AGE harvester-system hvst-upgrade-77cks 119m $ kubectl delete upgrades.harvesterhci.io hvst-upgrade-77cks -n harvester-system We recommend mirroring the ISO file to a local webserver, please check the notes in the previous section. Stuck in Upgrading System Service​ Description The upgrade is stuck at Upgrading System service. Similar logs are found in rancher pods: [ERROR] available chart version (100.0.2+up0.3.8) for fleet is less than the min version (100.0.3+up0.3.9-rc1) [ERROR] Failed to find system chart fleet will try again in 5 seconds: no chart name found Workaround Delete rancher cluster repositories and restart rancher pods. # login to a server node and become root first kubectl delete clusterrepos.catalog.cattle.io rancher-charts kubectl delete clusterrepos.catalog.cattle.io rancher-rke2-charts kubectl delete clusterrepos.catalog.cattle.io rancher-partner-charts kubectl delete settings.management.cattle.io chart-default-branch kubectl rollout restart deployment rancher -n cattle-system Related issues [BUG] Rancher upgrade fail: Failed to find system chart &quot;fleet&quot; VMs fail to migrate​ Description A node keeps waiting in Pre-draining state.There are VMs on that node (checking for virt-launcher-xxx pods) and they can't be live-migrated out of the node. Workaround Shutdown the VMs, you can do this by: Using the GUI.Using the virtctl command. Related issues [BUG] Upgrade: VMs fail to live-migrate to other hosts in some cases fleet-local/local: another operation (install/upgrade/rollback) is in progress​ Description You see bundles have fleet-local/local: another operation (install/upgrade/rollback) is in progress status in the output: kubectl get bundles -A Related issues [BUG] Upgrade: rancher-monitoring charts can't be upgraded Single node upgrade might fail if node name is too long (&gt;24 characters)​ Related issues https://github.com/harvester/harvester/issues/2114","keywords":"","version":"v1.0"},{"title":"Upgrade from v1.0.1 to v1.0.2","type":0,"sectionRef":"#","url":"/v1.0/upgrade/previous-releases/v1-0-1-to-v1-0-2","content":"Upgrade from v1.0.1 to v1.0.2 General information​ The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ Please check Known issues here.","keywords":"","version":"v1.0"},{"title":"Upgrading Harvester","type":0,"sectionRef":"#","url":"/v1.0/upgrade/automatic","content":"Upgrading Harvester Upgrade support matrix​ The following table shows the upgrade path of all supported versions. Upgrade from version\tSupported new version(s)v1.0.2\tv1.0.3 v1.0.1\tv1.0.2 v1.0.0\tv1.0.1 Start an upgrade​ Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: caution Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -&gt; Virtual Machines -&gt; Select VMs -&gt; Actions -&gt; Stop).Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 25 GB of free space (df -h /usr/local/). caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Prepare an air-gapped upgrade​ caution Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages. Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso. Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml. Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~&gt; sudo -i rancher@node1:~&gt; kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v1.0"},{"title":"Installation","type":0,"sectionRef":"#","url":"/v1.0/troubleshooting/installation","content":"Installation The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS)​ Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancherPassword: rancher Meeting hardware requirements​ Check that your hardware meets the minimum requirements to complete installation. Receiving the message &quot;Loading images. This may take a few minutes...&quot;​ Because the system doesn't have a default route, your installer may become &quot;stuck&quot; in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev harvester-mgmt proto dhcp &lt;-- Does a default route exist? 10.10.0.0/24 dev harvester-mgmt proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. Modifying cluster token on agent nodes​ When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg=&quot;Bootstrapping Rancher (master-head/v1.21.5+rke2r1)&quot; msg=&quot;failed to bootstrap system, will retry: generating plan: insecure cacerts download from https://192.168.122.115:443/cacerts: Get \\&quot;https://192.168.122.115:443/cacerts\\&quot;: EOF&quot; To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml. For example, if the cluster token setup in the server node is ThisIsTheCorrectOne, you will update the token value as follow: token: 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/99_custom.yaml: name: Harvester Configuration stages: ... initramfs: - commands: - rm -f /etc/sysconfig/network/ifroute-harvester-mgmt files: - path: /etc/rancher/rancherd/config.yaml permissions: 384 owner: 0 group: 0 content: | role: cluster-init token: 'ThisIsTheCorrectOne' # &lt;- Update this value kubernetesVersion: v1.21.5+rke2r1 labels: - harvesterhci.io/managed=true encoding: &quot;&quot; ownerstring: &quot;&quot; note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml. For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting troubleshooting information​ Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file. Before v1.0.2 Please help capture the content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* And output of these commands: blkid dmesg ","keywords":"","version":"v1.0"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.0/troubleshooting/monitoring","content":"Monitoring The following sections contain tips to troubleshoot Harvester Monitoring. Monitoring is unusable​ When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons. Monitoring is unusable due to Pod being stuck in Terminating status​ Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0/3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1/1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod &quot;prometheus-rancher-monitoring-prometheus-0&quot; force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ... Expand PV/Volume Size​ Harvester integrates Longhorn as the default storage provider. Harvester Monitoring uses Persistent Volume (PV) to store running data. When a cluster has been running for a certain time, the Persistent Volume may need to expand its size. Based on the Longhorn Volume expansion guide, Harvester illustrates how to expand the volume size. View Volume​ From Embedded Longhorn WebUI​ Access the embedded Longhorn WebUI according to this document. The default view. Click Volume to list all existing volumes. From CLI​ You can also use kubectl to get all Volumes. # kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 5Gi RWO harvester-longhorn 43h cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 50Gi RWO harvester-longhorn 43h cattle-monitoring-system rancher-monitoring-grafana Bound pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 2Gi RWO harvester-longhorn 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 attached degraded 2147483648 harv31 43h Scale Down a Deployment​ To detach the Volume, you need to scale down the deployment that uses the Volume. The example below is against the PVC claimed by rancher-monitoring-grafana. Find the deployment in the namespace cattle-monitoring-system. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // target deployment rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h Scale down the deployment rancher-monitoring-grafana to 0. # kubectl scale --replicas=0 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment and the volume. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 0/0 0 0 43h // scaled down rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 detached unknown 2147483648 43h // volume is detached Expand Volume​ In the Longhorn WebUI, the related volume becomes Detached. Click the icon in the Operation column, and select Expand Volume. Input a new size, and Longhorn will expand the volume to this size. Scale Up a Deployment​ After the Volume is expanded to target size, you need to scale up the aforementioned deployment to its original replicas. For the above example of rancher-monitoring-grafana, the original replicas is 1. # kubectl scale --replicas=1 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment again. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // scaled up rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h The Volume is attached to the new POD. To now, the Volume is expanded to the new size and the POD is using it smoothly.","keywords":"","version":"v1.0"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/v1.0/upgrade/troubleshooting","content":"Troubleshooting Overview​ Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes. You can click the version in the support matrix table to see if there are any known issues.Dive into the upgrade design proposal. The following section briefly describes phases within an upgrade and possible diagnostic methods. Diagnose the upgrade flow​ A Harvester upgrade process contains several phases. Phase 1: Provision upgrade repository VM.​ The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s Phase 2: Preload container images​ The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase. Phase 3: Upgrade system services​ In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ... Phase 4: Upgrade nodes​ The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node.post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-&lt;hostname&gt;). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... caution Please do not start over an upgrade if the upgrade fails at this phase. Phase 5: Clean-up​ The Harvester controller deletes the upgrade repository VM and all files that are no longer needed. Common operations​ Start over an upgrade​ Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again.","keywords":"","version":"v1.0"},{"title":"Access to the Virtual Machine","type":0,"sectionRef":"#","url":"/v1.0/vm/access-to-the-vm","content":"Access to the Virtual Machine Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI​ VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu-minimal-cloud image, the VM can only be accessed with the serial console. SSH Access​ Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection, which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection, which allows keys or basic auth to be updated dynamically at runtime. Static SSH Key Injection via cloud-init​ You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place. Example of SSH key cloud-init configuration:​ #cloud-config ssh_authorized_keys: - &gt;- ssh-rsa #replace with your public key Dynamic SSH Key Injection via Qemu guest agent​ Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the qemuGuestAgent propagation method. note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click ⋮ button.Click the Edit Config button and go to the Access Credentials tab.Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE).Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI. Access with the SSH Client​ Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@&lt;ip-address-or-hostname&gt; ","keywords":"Harvester harvester Rancher rancher Access to the VM","version":"v1.0"},{"title":"VM Backup & Restore","type":0,"sectionRef":"#","url":"/v1.0/vm/backup-restore","content":"VM Backup &amp; Restore Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. note A backup target must be set up. For more information, see Configure Backup Target. If the backup target has not been set, you’ll be prompted with a message to do so. Configure Backup Target​ A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings &gt; backup-target. Parameter\tType\tDescriptionType\tstring\tChoose S3 or NFS Endpoint\tstring\tA hostname or an IP address. It can be left empty for AWS S3. BucketName\tstring\tName of the bucket BucketRegion\tstring\tRegion of the bucket AccessKeyID\tstring\tA user-id that uniquely identifies your account SecretAccessKey\tstring\tThe password to your account Certificate\tstring\tPaste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle\tbool\tUse VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup​ Once the backup target is set, go to the Virtual Machines page.Click Take Backup of the VM actions to create a new VM backup.Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced &gt; Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup​ To restore a new VM from a backup, follow these steps: Go to the Backups page.Specify the new VM name and click Create.A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an Existing VM using a backup​ You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page.Click Create. The restore process can be viewed from the Virtual Machines page. Restore a new VM on another Harvester cluster​ Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata &amp; content backup feature. prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster​ Check the existing image name (normally starts with image-) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: &quot;&quot; pvcNamespace: &quot;&quot; sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster​ Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster.Go to the Backups page.Select the synced VM backup metadata and choose to restore a new VM with a specified VM name.A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","keywords":"Harvester harvester Rancher rancher VM Backup & Restore","version":"v1.0"},{"title":"Settings","type":0,"sectionRef":"#","url":"/v1.0/settings/","content":"Settings This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca​ This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example​ -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- caution Changing this setting might cause a short downtime for single-node clusters. backup-target​ This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation. Default: none Example​ { &quot;type&quot;: &quot;s3&quot;, &quot;endpoint&quot;: &quot;https://s3.endpoint.svc&quot;, &quot;accessKeyId&quot;: &quot;test-access-key-id&quot;, &quot;secretAccessKey&quot;: &quot;test-access-key&quot;, &quot;bucketName&quot;: &quot;test-bup&quot;, &quot;bucketRegion&quot;: &quot;us‑east‑2&quot;, &quot;cert&quot;: &quot;&quot;, &quot;virtualHostedStyle&quot;: false } cluster-registration-url​ This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example​ https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml http-proxy​ This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: &quot;httpProxy&quot;: &quot;http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Proxy URL for HTTPS requests: &quot;httpsProxy&quot;: &quot;https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Comma-separated list of hostnames and/or CIDRs: &quot;noProxy&quot;: &quot;&lt;hostname | CIDR&gt;&quot; caution If you configure httpProxy and httpsProxy, you must also put Harvester node's CIDR into noProxy, otherwise the Harvester cluster will be broken. If you also configure cluster-registration-url, you usually need to add the host of cluster-registration-url to noProxy as well, otherwise you cannot access the Harvester cluster from Rancher. Example​ { &quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc,172.16.0.0/16&quot; } note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,.svc,.cluster.local caution Changing this setting might cause a short downtime for single-node clusters. log-level​ This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panicfatalerrorwarn, warninginfodebugtrace Example​ debug overcommit-config​ This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { &quot;cpu&quot;:1600, &quot;memory&quot;:150, &quot;storage&quot;:200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100mCPU for it from Kubernetes scheduler. Example​ { &quot;cpu&quot;: 1000, &quot;memory&quot;: 200, &quot;storage&quot;: 300 } release-download-url​ Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester Example of the version.yaml​ apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL} server-version​ This setting displays the version of Harvester server. Example​ v1.0.0-abcdef-head ssl-certificates​ This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example​ { &quot;ca&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;publicCertificate&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;privateKey&quot;: &quot;-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----&quot; } caution Changing this setting might cause a short downtime on single-node clusters. ssl-parameters​ This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols: Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers: Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list isECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305. Default: none note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example​ The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list toECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305. { &quot;protocols&quot;: &quot;TLSv1.2 TLSv1.3&quot;, &quot;ciphers&quot;: &quot;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305&quot; } ui-index​ This setting allows you to configure HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example​ https://your.static.dashboard-ui/index.html ui-source​ This setting allows you to configure how to load the UI source. The following values can be set: auto: The default. Auto-detect whether to use bundled UI or not.external: Use external UI source.bundled: Use the bundled UI source. Example​ external upgrade-checker-enabled​ This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example​ false upgrade-checker-url​ This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example​ https://your.upgrade.checker-url/v99/checkupgrade auto-disk-provision-paths [Experimental]​ This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. caution This setting is applied to every Node in the cluster.All the data in these storage devices will be destroyed. Use at your own risk. Default: none Example​ The following example will add disks matching the glob pattern /dev/sd* or /dev/hd*: /dev/sd*,/dev/hd* vm-force-reset-policy​ This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready, it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {&quot;enable&quot;:true, &quot;period&quot;:300} Example​ { &quot;enable&quot;: &quot;true&quot;, &quot;period&quot;: 300 } ","keywords":"","version":"v1.0"},{"title":"Upload Images","type":0,"sectionRef":"#","url":"/v1.0/upload-image","content":"Upload Images Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL​ To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File​ Currently, qcow2, raw, and ISO images are supported. note Please do not refresh the page until the file upload is finished. Create Images via Volumes​ On the Volumes page, click Export Image. Enter image name to create image. Image labels​ You can add labels to the image, which will help identify the OS type more accurately. Additionally, you can also add any custom labels when needed. If you create an image from a URL, the UI will automatically recognize the OS type and image category based on the image name. However, if you created the image by uploading a local file, you will need to manually select the corresponding labels.","keywords":"Harvester harvester Rancher rancher Import Images","version":"v1.0"},{"title":"Edit a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.0/vm/edit-vm","content":"Edit a Virtual Machine How to Edit a VM​ After creating a virtual machine, you can edit your virtual machine by clicking the ⋮ button and selecting the Edit Configurations button. note In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect. Basics​ On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS. Networks​ You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server. For more details about the network implementation, please refer to the Networking page. Volumes​ You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, then click Edit Image Volume to edit the size of the expanded volume. After waiting for the resize to complete and restarting the VM, your disk will automatically finish expanding. Access Credentials​ Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has QEMU guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Edit a VM","version":"v1.0"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.0/vm/create-vm","content":"Create a Virtual Machine How to Create a VM​ You can create one or more virtual machines from the Virtual Machines page. note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances.Select the namespace of your VMs, only the harvester-public namespace is visible to all users.The VM Name is a required field.(Optional) VM template is optional, you can choose iso-image, raw-image or windows-iso-image template to speed up your VM instance creation.Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision).Select SSH keys or upload new keys.Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured.You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced &gt; Networks first. Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable. Volumes​ You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type\tdescriptiondisk\tA disk disk will expose the volume as an ordinary disk to the VM. cd-rom\tA cd-rom disk will expose the volume as a cd-rom drive to the VM. It is read-only by default. Container Disk Container disks are ephemeral storage devices that can be assigned to any number of VMs. This makes them an ideal tool for users who want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Note: Container disks are not a good solution for any workload that requires persistent root disks across VM restarts. Networks​ You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type\tdescriptionbridge\tConnect using a Linux bridge masquerade\tConnect using iptables rules to NAT the traffic Management Network​ A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes. Secondary Network​ It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks. In bridge VLAN, virtual machines are connected to the host network through a linux bridge. The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Advanced Options​ Run Strategy​ Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true. RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false. Cloud Configuration​ Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Cloud-init​ Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password: password chpasswd: { expire: False } ssh_pwauth: True Example of network-data configuration using DHCP: network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp You can also use the Advanced &gt; Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM. Installing the QEMU guest agent​ The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Create a VM","version":"v1.0"},{"title":"Upgrade from v1.0.2 to v1.0.3","type":0,"sectionRef":"#","url":"/v1.0/upgrade/v1-0-2-to-v1-0-3","content":"Upgrade from v1.0.2 to v1.0.3 General information​ The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. Fail to download the upgrade image​ Description Downloading the upgrade image can't be complete or fails with an error. Related issues [BUG] failed to create upgrade image Workaround Delete the current upgrade and start over. Please see &quot;Start over an upgrade&quot;. 2. An upgrade is stuck, a node is in &quot;Pre-drained&quot; state (case 1)​ Description Users might see a node is stuck at the Pre-drained state for a while (&gt; 30 minutes). This might be caused by instance-manager-r-* pod on node harvester-z7j2g can’t be drained. To verify the above case: Check rancher server logs: kubectl logs deployment/rancher -n cattle-system Example output: error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Verify the pod longhorn-system/instance-manager-r-10dd59c4 is on the stuck node: kubectl get pod instance-manager-r-10dd59c4 -n longhorn-system -o=jsonpath='{.spec.nodeName}' Example output: harvester-z7j2g Check degraded volumes: kubectl get volumes -n longhorn-system Example output: NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-08c34593-8225-4be6-9899-10a978df6ea1 attached healthy True 10485760 harvester-279l2 3d13h pvc-526600f5-bde2-4244-bb8e-7910385cbaeb attached healthy True 21474836480 harvester-x9jqw 3d1h pvc-7b3fc2c3-30eb-48b8-8a98-11913f8314c2 attached healthy True 10737418240 harvester-x9jqw 3d pvc-8065ed6c-a077-472c-920e-5fe9eacff96e attached healthy True 21474836480 harvester-x9jqw 3d pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 attached degraded True 10737418240 harvester-x9jqw 2d23h pvc-9a6539b8-44e5-430e-9b24-ea8290cb13b7 attached healthy True 53687091200 harvester-x9jqw 3d13h Here we can see volume pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 is degraded. note The user needs to check all degraded volumes one by one. Check degraded volume’s replica state: kubectl get replicas -n longhorn-system --selector longhornvolume=pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 -o json | jq '.items[] | {replica: .metadata.name, healthyAt: .spec.healthyAt, nodeID: .spec.nodeID, state: .status.currentState}' Example output: { &quot;replica&quot;: &quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246&quot;, &quot;healthyAt&quot;: &quot;2022-07-25T07:33:16Z&quot;, &quot;nodeID&quot;: &quot;harvester-z7j2g&quot;, &quot;state&quot;: &quot;running&quot; } { &quot;replica&quot;: &quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-22974d0f&quot;, &quot;healthyAt&quot;: &quot;&quot;, &quot;nodeID&quot;: &quot;harvester-279l2&quot;, &quot;state&quot;: &quot;running&quot; } { &quot;replica&quot;: &quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5&quot;, &quot;healthyAt&quot;: &quot;&quot;, &quot;nodeID&quot;: &quot;harvester-x9jqw&quot;, &quot;state&quot;: &quot;stopped&quot; } Here the only healthy replica is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246 and it’s on node harvester-z7j2g. So we can confirm the instance-manager-r-* pod resides on node harvester-z7j2g and avoids the drain. Related issues [BUG] Upgrade: longhorn-system can't be evicted Workaround We need to start the “Stopped” replica, from the previous example, the stopped replica’s name is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5. Check the Longhorn manager log, we should see a replica waiting for the backing image. First, we need to get the manager's name: kubectl get pods -n longhorn-system --selector app=longhorn-manager --field-selector spec.nodeName=harvester-x9jqw Example output: NAME READY STATUS RESTARTS AGE longhorn-manager-zmfbw 1/1 Running 0 3d10h Get pod log: kubectl logs longhorn-manager-zmfbw -n longhorn-system | grep pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 Example output: (...) time=&quot;2022-07-28T04:35:34Z&quot; level=debug msg=&quot;Prepare to create instance pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5&quot; time=&quot;2022-07-28T04:35:34Z&quot; level=debug msg=&quot;Replica pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 is waiting for backing image harvester-system-harvester-iso-n7bxh downloading file to node harvester-x9jqw disk 3830342d-c13d-4e55-ac74-99cad529e9d4, the current state is in-progress&quot; controller=longhorn-replica dataPath= node=harvester-x9jqw nodeID=harvester-x9jqw ownerID=harvester-x9jqw replica=pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 time=&quot;2022-07-28T04:35:34Z&quot; level=info msg=&quot;Event(v1.ObjectReference{Kind:\\&quot;Replica\\&quot;, Namespace:\\&quot;longhorn-system\\&quot;, Name:\\&quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\\&quot;, UID:\\&quot;c511630f-2fe2-4cf9-97a4-21bce73782b1\\&quot;, APIVersion:\\&quot;longhorn.io/v1beta1\\&quot;, ResourceVersion:\\&quot;632926\\&quot;, FieldPath:\\&quot;\\&quot;}): type: 'Normal' reason: 'Start' Starts pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5&quot; Here we can determine the replica is waiting for the backing image harvester-system-harvester-iso-n7bxh.Get the disk file map from the backing image: kubectl describe backingimage harvester-system-harvester-iso-n7bxh -n longhorn-system Example output: (...) Disk File Status Map: 3830342d-c13d-4e55-ac74-99cad529e9d4: Last State Transition Time: 2022-07-25T08:30:34Z Message: Progress: 29 State: in-progress 3aa804e1-229d-4141-8816-1f6a7c6c3096: Last State Transition Time: 2022-07-25T08:33:20Z Message: Progress: 100 State: ready 92726efa-bfb3-478e-8553-3206ad34ce70: Last State Transition Time: 2022-07-28T04:31:49Z Message: Progress: 100 State: ready The disk file with UUID 3830342d-c13d-4e55-ac74-99cad529e9d4 has the state in-progress.Next, we need to find backing-image-manager that contains this disk file: kubectl get pod -n longhorn-system --selector=longhorn.io/disk-uuid=3830342d-c13d-4e55-ac74-99cad529e9d4 Example output: NAME READY STATUS RESTARTS AGE backing-image-manager-c00e-3830 1/1 Running 0 3d1h Restart the backing-image-manager by deleting its pod: kubectl delete pod -n longhorn-system backing-image-manager-c00e-3830 3. An upgrade is stuck, a node is in &quot;Pre-drained&quot; state (case 2)​ Description Users might see a node is stuck at the Pre-drained state for a while (&gt; 30 minutes). Here are some steps to verify this issue has happened: Visit the Longhorn GUI: https://{{VIP}}/k8s/clusters/local/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/volume (replace VIP with an appropriate value) and check degraded volumes. The degraded volume might contain a healthy replica only (with blue background) and the healthy replica resides on the &quot;Pre-drained&quot; node: Hover the mouse to the red scheduled icon, the reason is toomanysnapshots: Related issues [BUG] Upgrade is stuck in &quot;Pre-drained&quot; state (Volume has too many system snapshots) Workaround In the &quot;Snapshots and Backup&quot; panel, toggle the &quot;Show System Hidden&quot; switch and delete the latest system snapshot (which is just before the &quot;Volume Head&quot;). The volume will continue rebuilding to resume the upgrade.","keywords":"","version":"v1.0"},{"title":"Live Migration","type":0,"sectionRef":"#","url":"/v1.0/vm/live-migration","content":"Live Migration Live migration means moving a virtual machine to a different host without downtime. note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. Starting a Migration​ Go to the Virtual Machines page.Find the virtual machine that you want to migrate and select ⋮ &gt; Migrate.Choose the node to which you want to migrate the virtual machine. Click Apply. Aborting a Migration​ Go to the Virtual Machines page.Find the virtual machine in migrating status that you want to abort. Select ⋮ &gt; Abort Migration. Migration Timeouts​ Completion Timeout​ The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout​ Live migration will also be aborted when copying memory doesn't make any progress in 150s.","keywords":"Harvester harvester Rancher rancher Live Migration","version":"v1.0"},{"title":"Resource Overcommit","type":0,"sectionRef":"#","url":"/v1.0/vm/resource-overcommit","content":"Resource Overcommit Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config, this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600%RAM allocation ratio: 150%Storage allocation ratio: 200% note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated. Configure the global setting overcommit-config​ Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced &gt; Settings page.Find the overcommit-config setting.Configure the desired CPU, Memory, and Storage ratio. Configure overcommit for a single virtual machine​ If you need to configure individual virtual machines without involving global configuration, consider adjusting the spec.template.spec.domain.resources.&lt;memory|cpu&gt; value on the target VirtualMachine resource individually. Note that by modifying these values, you are taking over control of virtual machine resource management from Harvester. Reserve more memory for the system overhead​ By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ... Why my virtual machines are scheduled unevenly?​ The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling.","keywords":"Harvester Overcommit Overprovision ballooning","version":"v1.0"},{"title":"Harvester Overview","type":0,"sectionRef":"#","url":"/v1.1/","content":"Harvester Overview Harvester is a modern, open, interoperable, hyperconverged infrastructure (HCI) solution built on Kubernetes. It is an open-source alternative designed for operators seeking a cloud-native HCI solution. Harvester runs on bare metal servers and provides integrated virtualization and distributed storage capabilities. In addition to traditional virtual machines (VMs), Harvester supports containerized environments automatically through integration with Rancher. It offers a solution that unifies legacy virtualized infrastructure while enabling the adoption of containers from core to edge locations. Harvester Architecture​ The Harvester architecture consists of cutting-edge open-source technologies: Linux OS. Elemental for SLE-Micro 5.3 is at the core of Harvester and is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Built on top of Kubernetes. Kubernetes has become the predominant infrastructure language across all form factors, and Harvester is an HCI solution with Kubernetes under the hood.Virtualization management with Kubevirt. Kubevirt provides virtualization management using KVM on top of Kubernetes.Storage management with Longhorn. Longhorn provides distributed block storage and tiering.Observability with Grafana and Prometheus. Grafana and Prometheus provide robust monitoring and logging. Harvester Features​ Harvester is an enterprise-ready, easy-to-use infrastructure platform that leverages local, direct attached storage instead of complex external SANs. It utilizes Kubernetes API as a unified automation language across container and VM workloads. Some key features of Harvester include: Easy to get started. Since Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image or automatically install it using iPXE scripts.VM lifecycle management. Easily create, edit, clone, and delete VMs, including SSH-Key injection, cloud-init, and graphic and serial port console.VM live migration. Move a VM to a different host or node with zero downtime.VM backup, snapshot, and restore. Back up your VMs from NFS, S3 servers, or NAS devices. Use your backup to restore a failed VM or create a new VM on a different cluster.Storage management. Harvester supports distributed block storage and tiering. Volumes represent storage; you can easily create, edit, clone, or export a volume.Network management. Supports using a virtual IP (VIP) and multiple Network Interface Cards (NICs). If your VMs need to connect to the external network, create a VLAN or untagged network.Integration with Rancher. Access Harvester directly within Rancher through Rancher’s Virtualization Management page and manage your VM workloads alongside your Kubernetes clusters. Harvester Dashboard​ Harvester provides a powerful and easy-to-use web-based dashboard for visualizing and managing your infrastructure. Once you install Harvester, you can access the IP address for the Harvester Dashboard from the node's terminal.","keywords":"Harvester harvester Rancher rancher Harvester Intro","version":"v1.1"},{"title":"Hot-Plug Volumes","type":0,"sectionRef":"#","url":"/v1.0/vm/hotplug-volume","content":"Hot-Plug Volumes Harvester supports adding hot-plug volumes to a running VM. Adding Hot-Plug Volumes to a Running VM​ The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page.Find the VM that you want to add a volume to and select ⋮ &gt; Add Volume.Enter the Name and select the Volume.Click Apply.","keywords":"Harvester Hot-plug Volume","version":"v1.0"},{"title":"Addons","type":0,"sectionRef":"#","url":"/v1.1/advanced/addons","content":"Addons Available as of v1.1.0 Beginning with v1.1.0, Harvester will make optional functionality available as Addons. One of the key reasons for the same is to ensure that Harvester installation footprint can be kept low while allowing users to enable/disable functionality based on their use case or requirements. Some level of customization is allowed for each addon, which depends on the underlying addon. v1.1.0 currently ships with two Addons: pcidevices-controllervm-import-controller","keywords":"","version":"v1.1"},{"title":"Create a Windows Virtual Machine","type":0,"sectionRef":"#","url":"/v1.0/vm/create-windows-vm","content":"Create a Windows Virtual Machine Create one or more virtual machines from the Virtual Machines page. note For creating Linux virtual machines, please refer to this page. How to Create a Windows VM​ Header Section​ Create a single VM instance or multiple VM instances.Set the VM name.(Optional) Provide a description for the VM.(Optional) Select the VM template windows-iso-image-base-template. This template will add a volume with the virtio drivers for Windows. Basics Tab​ Configure the number of CPU cores assigned to the VM.Configure the amount of Memory assigned to the VM.(Optional) Select existing SSH keys or upload new ones. note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. caution The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab​ The first volume is an Image Volume with the following values: Name: The value cdrom-disk is set by default. You can keep it or change it.Image: Select the Windows image to be installed. See Upload Images for the full description on how to create new images.Type: Select cd-rom.Size: The value 20 is set by default. You can change it if your image has a bigger size.Bus: The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name: The value rootdisk is set by default. You can keep it or change it.Size: The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value.Bus: The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI. The third volume is a Container with the following values: Name: The value virtio-container-disk is set by default. You can keep it or change it.Docker Image: The value registry.suse.com/suse/vmdp/vmdp:2.5.3 is set by default. It's recommended you don't change it.Bus: The value SATA is set by default. It's recommended you don't change it. You can add additional disks using the buttons Add Volume, Add Existing Volume, Add VM Image, or Add Container. Networks Tab​ The Management Network is added by default with the following values: Name: The value default is set by default. You can keep it or change it.Network: The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks.Model: The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown.Type: The value masquerade is set by default. You can keep it or change it to the other available option, bridge. You can add additional networks by clicking Add Network. caution Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration. Node Scheduling Tab​ Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab​ OS Type: The value Windows is set by default. It's recommended you don't change it.Machine Type: The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value.(Optional) Hostname: Set the VM hostname.(Optional) Cloud Config: Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs. Footer Section​ Start virtual machine on creation: This option is checked by default. You can uncheck it if you don't want the VM to start once it's created. Once all the settings are in place, click on Create to create the VM. note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML. And if you want to cancel all changes made, click Cancel. Installation of Windows​ Select the VM you just created, and click Start to boot up the VM.(If you checked Start virtual machine on creation the VM will start automatically once it's created) Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template, the instruction is as follows: Click on Load driver, and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside.Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows, and click Next to load the driver.Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version\tSupported\tDriver pathWindows 7\tNo\tN/A Windows Server 2008\tNo\tN/A Windows Server 2008r2\tNo\tN/A Windows 8 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly. Known Issues​ Windows ISO unable to boot when using EFI mode​ When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO. VM crashes when reserved memory not enough​ There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. We will add a default 256MiB of reserved memory to the Windows template to prevent this problem in the future release. BSoD (Blue Screen of Death) at first boot time of Windows​ There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name: Harvester Configuration stages: initramfs: - commands: # ... files: - path: /etc/modprobe.d/kvm.conf permissions: 384 owner: 0 group: 0 content: | options kvm ignore_msrs=1 encoding: &quot;&quot; ownerstring: &quot;&quot; # ... note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","keywords":"Harvester harvester Rancher rancher Windows windows Virtual Machine virtual machine Create a Windows VM","version":"v1.0"},{"title":"PCI Devices (Experimental)","type":0,"sectionRef":"#","url":"/v1.1/advanced/pcidevices","content":"PCI Devices (Experimental) Available as of v1.1.0 A PCIDevice in Harvester represents a host device with a PCI address. The devices can be passed through the hypervisor to a VM by creating a PCIDeviceClaim resource, or by using the UI to enable passthrough. Passing a device through the hypervisor means that the VM can directly access the device, and effectively owns the device. A VM can even install its own drivers for that device. This is accomplished by using the pcidevices-controller addon. To use the PCI devices feature, users need to enable the pcidevices-controller addon first. Enabling Passthrough on a PCI Device​ Now go to the Advanced -&gt; PCI Devices page: Search for your device by vendor name (e.g. NVIDIA, Intel, etc.) or device name. Select the devices you want to enable for passthrough: Then click Enable Passthrough and read the warning message. If you still want to enable these devices, click Enable and wait for all devices to be Enabled. caution Please do not use host-owned PCI devices (e.g., management and VLAN NICs). Incorrect device allocation may cause damage to your cluster, including node failure. Attaching PCI Devices to a VM​ After enabling these PCI devices, you can navigate to the Virtual Machines page and select Edit Config to pass these devices. Select PCI Devices and use the Available PCI Devices drop-down. Select the devices you want to attach from the list displayed and then click Save. Using a passed-through PCI Device inside the VM​ Boot the VM up, and run lspci inside the VM, the attached PCI devices will show up, although the PCI address in the VM won't necessarily match the PCI address in the host. Installing drivers for your PCI device inside the VM​ This is just like installing drivers in the host. The PCI passthrough feature will bind the host device to the vfio-pci driver, which gives VMs the ability to use their own drivers. Here is a screenshot of NVIDIA drivers being installed in a VM. It includes a CUDA example that proves that the device drivers work. Known Issues​ The 1.1.0 version of PCI passthrough matches VMs to devices using vendorId:deviceId. This means that if there is more than one device with the same vendorId:deviceId pair, then KubeVirt will choose which device to allocate to a VM in a way that is essentially random. This will be addressed in 1.1.2 with the new deviceplugin implementation.","keywords":"","version":"v1.1"},{"title":"StorageClass","type":0,"sectionRef":"#","url":"/v1.1/advanced/storageclass","content":"StorageClass A StorageClass allows administrators to describe the classes of storage they offer. Different Longhorn StorageClasses might map to replica policies, or to node schedule policies, or disk schedule policies determined by the cluster administrators. This concept is sometimes called profiles in other storage systems. Creating a StorageClass​ You can create one or more StorageClasses from the Advanced &gt; StorageClasses page. note After a StorageClass is created, nothing can be changed except Description. Header Section​ Name: name of the StorageClassDescription (optional): description of the StorageClass Parameters Tab​ Number of Replicas​ The number of replicas created for each volume in Longhorn. Defaults to 3. Stale Replica Timeout​ Determines when Longhorn would clean up an error replica after the replica's status is ERROR. The unit is minute. Defaults to 30 minutes in Harvester. Node Selector (Optional)​ Select the node tags to be matched in the volume scheduling stage. You can add node tags by going to Host &gt; Edit Config. Disk Selector (Optional)​ Select the disk tags to be matched in the volume scheduling stage. You can add disk tags by going to Host &gt; Edit Config. Migratable​ Whether Live Migration is supported. Defaults to Yes. Customize Tab​ Reclaim Policy​ Volumes dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class. The Delete mode is used by default. Delete: Deletes volumes and the underlying devices when the volume claim is deleted.Retain: Retains the volume for manual cleanup. Allow Volume Expansion​ Volumes can be configured to be expandable. This feature is Enabled by default, which allows users to resize the volume by editing the corresponding PVC object. note You can only use the volume expansion feature to grow a Volume, not to shrink it. Volume Binding Mode​ The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. The Immediate mode is used by default. Immediate: Binds and provisions a persistent volume once the PersistentVolumeClaim is created.WaitForFirstConsumer: Binds and provisions a persistent volume once a VM using the PersistentVolumeClaim is created. Appendix - Use Case​ HDD Scenario​ With the introduction of StorageClass, users can now use HDDs for tiered or archived cold storage. caution HDD is not recommended for guest RKE2 clusters or VMs with good performance disk requirements. Recommended Practice​ First, add your HDD on the Host page and specify the disk tags as needed, such asHDD or ColdStorage. For more information on how to add extra disks and disk tags, see Multi-disk Management for details. Then, create a new StorageClass for the HDD (use the above disk tags). For hard drives with large capacity but slow performance, the number of replicas can be reduced to improve performance. You can now create a volume using the above StorageClass with HDDs mostly for cold storage or archiving purpose.","keywords":"","version":"v1.1"},{"title":"Air Gapped Environment","type":0,"sectionRef":"#","url":"/v1.1/airgap","content":"Air Gapped Environment This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy​ In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation​ You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings​ You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI.Find the http-proxy setting, click ⋮ &gt; Edit settingEnter the value(s) for http-proxy, https-proxy and no-proxy. note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. When the nodes in the cluster do not use a proxy to communicate with each other, the CIDR needs to be added to http-proxy.noProxy after the first node is installed successfully. Please refer to fail to deploy a multi-node cluster. Guest Cluster Images​ All necessary images to install and run Harvester are conveniently packaged into the ISO, eliminating the need to pre-load images on bare-metal nodes. A Harvester cluster manages them independently and effectively behind the scenes. However, it's essential to understand a guest K8s cluster (e.g., RKE2 cluster) created by the Harvester node driver is a distinct entity from a Harvester cluster. A guest cluster operates within VMs and requires pulling images either from the internet or a private registry. If the Cloud Provider option is configured to Harvester in a guest K8s cluster, it deploys the Harvester cloud provider and Container Storage Interface (CSI) driver. As a result, we recommend monitoring each RKE2 release in your air gapped environment and pulling the required images into your private registry. Please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section on the Harvester support matrix page for the best Harvester cloud provider and CSI driver capability support.","keywords":"Harvester offline Air-gap HTTP proxy","version":"v1.1"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/v1.1/authentication","content":"Authentication After installation, user will be prompted to set the password for the default admin user on the first-time login. note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","keywords":"Harvester harvester Rancher rancher Authentication","version":"v1.1"},{"title":"VM Import","type":0,"sectionRef":"#","url":"/v1.1/advanced/vmimport","content":"VM Import Available as of v1.1.0 Beginning with v1.1.0, users can import their virtual machines from VMWare and OpenStack into Harvester. This is accomplished using the vm-import-controller addon. To use the VM Import feature, users need to enable the vm-import-controller addon. By default, vm-import-controller leverages ephemeral storage, which is mounted from /var/lib/kubelet. During the migration, a large VM's node could run out of space on this mount, resulting in subsequent scheduling failures. To avoid this, users are advised to enable PVC-backed storage and customize the amount of storage needed. According to the best practice, the PVC size should be twice the size of the largest VM being migrated. This is essential as the PVC is used as scratch space to download the VM, and convert the disks into raw image files. vm-import-controller​ Currently, the following source providers are supported: VMWareOpenStack API​ The vm-import-controller introduces two CRDs. Sources​ Sources allow users to define valid source clusters. For example: apiVersion: migration.harvesterhci.io/v1beta1 kind: VmwareSource metadata: name: vcsim namespace: default spec: endpoint: &quot;https://vscim/sdk&quot; dc: &quot;DCO&quot; credentials: name: vsphere-credentials namespace: default The secret contains the credentials for the vCenter endpoint: apiVersion: v1 kind: Secret metadata: name: vsphere-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; As part of the reconciliation process, the controller will log into vCenter and verify whether the dc specified in the source spec is valid. Once this check is passed, the source is marked as ready and can be used for VM migrations. $ kubectl get vmwaresource.migration NAME STATUS vcsim clusterReady For OpenStack-based source clusters, an example definition is as follows: apiVersion: migration.harvesterhci.io/v1beta1 kind: OpenstackSource metadata: name: devstack namespace: default spec: endpoint: &quot;https://devstack/identity&quot; region: &quot;RegionOne&quot; credentials: name: devstack-credentials namespace: default The secret contains the credentials for the OpenStack endpoint: apiVersion: v1 kind: Secret metadata: name: devstack-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; &quot;project_name&quot;: &quot;admin&quot; &quot;domain_name&quot;: &quot;default&quot; &quot;ca_cert&quot;: &quot;pem-encoded-ca-cert&quot; The OpenStack source reconciliation process attempts to list VMs in the project and marks the source as ready. $ kubectl get opestacksource.migration NAME STATUS devstack clusterReady VirtualMachineImport​ The VirtualMachineImport CRD provides a way for users to define a source VM and map to the actual source cluster to perform VM export/import. A sample VirtualMachineImport looks like this: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: alpine-export-test namespace: default spec: virtualMachineName: &quot;alpine-export-test&quot; networkMapping: - sourceNetwork: &quot;dvSwitch 1&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;dvSwitch 2&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: vcsim namespace: default kind: VmwareSource apiVersion: migration.harvesterhci.io/v1beta1 This will trigger the controller to export the VM named &quot;alpine-export-test&quot; on the VMWare source cluster to be exported, processed and recreated into the harvester cluster This can take a while based on the size of the virtual machine, but users should see VirtualMachineImages created for each disk in the defined virtual machine. The list of items in networkMapping will define how the source network interfaces are mapped to the Harvester Networks. If a match is not found, each unmatched network interface is attached to the default managementNetwork. Once the virtual machine has been imported successfully, the object will reflect the status: $ kubectl get virtualmachineimport.migration NAME STATUS alpine-export-test virtualMachineRunning openstack-cirros-test virtualMachineRunning Similarly, users can define a VirtualMachineImport for an OpenStack source as well: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: openstack-demo namespace: default spec: virtualMachineName: &quot;openstack-demo&quot; #Name or UUID for instance networkMapping: - sourceNetwork: &quot;shared&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;public&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: devstack namespace: default kind: OpenstackSource apiVersion: migration.harvesterhci.io/v1beta1 note OpenStack allows users to have multiple instances with the same name. In such a scenario, users are advised to use the Instance ID. The reconciliation logic tries to perform a name-to-ID lookup when a name is used.","keywords":"","version":"v1.1"},{"title":"Management Address","type":0,"sectionRef":"#","url":"/v1.1/install/management-address","content":"Management Address Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address​ To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath='{.metadata.annotations}' Example of output: {&quot;kube-vip.io/hwaddr&quot;:&quot;02:00:00:09:7f:3f&quot;,&quot;kube-vip.io/requestedIP&quot;:&quot;10.84.102.31&quot;} Usages​ The management address has two usages. Allows the access to the Harvester API/UI via HTTPS protocol.Is the address the other nodes use to join the cluster.","keywords":"VIP","version":"v1.1"},{"title":"Settings","type":0,"sectionRef":"#","url":"/v1.1/advanced/index","content":"Settings This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca​ This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example​ -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- caution Changing this setting might cause a short downtime for single-node clusters. auto-disk-provision-paths [Experimental]​ This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. caution This setting is applied to every Node in the cluster.All the data in these storage devices will be destroyed. Use at your own risk. Default: none Example​ The following example will add disks matching the glob pattern /dev/sd* or /dev/hd*: /dev/sd*,/dev/hd* backup-target​ This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation. Default: none Example​ { &quot;type&quot;: &quot;s3&quot;, &quot;endpoint&quot;: &quot;https://s3.endpoint.svc&quot;, &quot;accessKeyId&quot;: &quot;test-access-key-id&quot;, &quot;secretAccessKey&quot;: &quot;test-access-key&quot;, &quot;bucketName&quot;: &quot;test-bup&quot;, &quot;bucketRegion&quot;: &quot;us‑east‑2&quot;, &quot;cert&quot;: &quot;&quot;, &quot;virtualHostedStyle&quot;: false } cluster-registration-url​ This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example​ https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml note When you configure this setting, a new pod called cattle-cluster-agent-* is created in the namespace cattle-system for registration purposes. This pod uses the container image rancher/rancher-agent:related-version, which is not packed into the Harvester ISO and is instead determined by Rancher. The related-version is usually the same as the Rancher version. For example, when you register Harvester to Rancher v2.7.9, the image is rancher/rancher-agent:v2.7.9. For more information, see Find the required assets for your Rancher version in the Rancher documentation. Depending on your Harvester settings, the image is downloaded from either of the following locations: Harvester containerd-registry: You can configure a private registry for the Harvester cluster. Docker Hub (docker.io): This is the default option when you do not configure a private registry in Rancher. Alternatively, you can obtain a copy of the image and manually upload it to all Harvester nodes. containerd-registry​ This setting allows you to configure a private registry for the Harvester cluster. The value will be set in /etc/rancher/rke2/registries.yaml of each node. You can read RKE2 - Containerd Registry Configuration for more information. note If you set a username and password for a private registry, the system will automatically remove it to protect the credential after the system saves it in registries.yaml. Example​ { &quot;Mirrors&quot;: { &quot;docker.io&quot;: { &quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;], &quot;Rewrites&quot;: null } }, &quot;Configs&quot;: { &quot;myregistry.local:5000&quot;: { &quot;Auth&quot;: { &quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot; }, &quot;TLS&quot;: { &quot;InsecureSkipVerify&quot;: false } } } } http-proxy​ This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: &quot;httpProxy&quot;: &quot;http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Proxy URL for HTTPS requests: &quot;httpsProxy&quot;: &quot;https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Comma-separated list of hostnames and/or CIDRs: &quot;noProxy&quot;: &quot;&lt;hostname | CIDR&gt;&quot; caution If you configure httpProxy and httpsProxy, you must also put Harvester node's CIDR into noProxy, otherwise the Harvester cluster will be broken. If you also configure cluster-registration-url, you usually need to add the host of cluster-registration-url to noProxy as well, otherwise you cannot access the Harvester cluster from Rancher. Example​ { &quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc,172.16.0.0/16&quot; } note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. caution Changing this setting might cause a short downtime for single-node clusters. log-level​ This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panicfatalerrorwarn, warninginfodebugtrace Example​ debug overcommit-config​ This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { &quot;cpu&quot;:1600, &quot;memory&quot;:150, &quot;storage&quot;:200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100mCPU for it from Kubernetes scheduler. Example​ { &quot;cpu&quot;: 1000, &quot;memory&quot;: 200, &quot;storage&quot;: 300 } release-download-url​ Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester Example of the version.yaml​ apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL} server-version​ This setting displays the version of Harvester server. Example​ v1.0.0-abcdef-head ssl-certificates​ This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example​ { &quot;ca&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;publicCertificate&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;privateKey&quot;: &quot;-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----&quot; } caution Changing this setting might cause a short downtime on single-node clusters. ssl-parameters​ This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols: Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers: Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list isECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305. Default: none note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example​ The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list toECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305. { &quot;protocols&quot;: &quot;TLSv1.2 TLSv1.3&quot;, &quot;ciphers&quot;: &quot;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305&quot; } storage-network​ By default, Longhorn uses the default management network in the Harvester cluster that is limited to a single interface and shared with other workloads cluster-wide. This setting allows you to configure a segregated storage network when network isolation is preferred. For details, please refer to the Harvester Storage Network caution Any change to storage-network requires shutting down all VMs before applying this setting. IP Range should be IPv4 CIDR format and 4 times the number of your cluster nodes. Default: &quot;&quot; Example​ { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } ui-index​ This setting allows you to configure HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example​ https://your.static.dashboard-ui/index.html ui-source​ This setting allows you to configure how to load the UI source. The following values can be set: auto: The default. Auto-detect whether to use bundled UI or not.external: Use external UI source.bundled: Use the bundled UI source. Example​ external upgrade-checker-enabled​ This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example​ false upgrade-checker-url​ This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example​ https://your.upgrade.checker-url/v99/checkupgrade vip-pools​ This setting allows you to configure the global or namespace IP address pools of the VIP by CIDR or IP range. Default: {} note Configuring multi-CIDR or IP range from UI is only available from Harvester v1.1.1. Example​ { &quot;default&quot;: &quot;172.16.0.0/24,172.16.1.0/24&quot;, &quot;demo&quot;: &quot;172.16.2.50-172.16.2.100,172.16.2.150-172.16.3.200&quot; } vm-force-reset-policy​ This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready, it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {&quot;enable&quot;:true, &quot;period&quot;:300} note When a host is unavailable or is powered off, the VM only reboots and does not migrate. Example​ { &quot;enable&quot;: &quot;true&quot;, &quot;period&quot;: 300 } ","keywords":"","version":"v1.1"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/v1.1/faq","content":"FAQ This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node?​ $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard?​ username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster?​ Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM?​ # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/reference/cli.html#clean How can I reset the administrator password?​ In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher --no-headers | head -1 | awk '{ print $1 }') -c rancher -- reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt; I added an additional disk with partitions. Why is it not getting detected?​ As of Harvester v1.0.2, we no longer support adding additional partitioned disks, so be sure to delete all partitions first (e.g., using fdisk). Why are there some Harvester pods that become ErrImagePull/ImagePullBackOff?​ This is likely because your Harvester cluster is an air-gapped setup, and some pre-loaded container images are missing. Kubernetes has a mechanism that does garbage collection against bloated image stores. When the partition which stores container images is over 85% full, kubelet tries to prune the images based on the last time they were used, starting with the oldest, until the occupancy is lower than 80%. These numbers (85% and 80%) are default High/Low thresholds that come with Kubernetes. To recover from this state, do one of the following depending on the cluster's configuration: Pull the missing images from sources outside of the cluster (if it's an air-gapped environment, you might need to set up an HTTP proxy beforehand).Manually import the images from the Harvester ISO image. note Take v1.1.2 as an example, download the Harvester ISO image from the official URL. Then extract the image list from the ISO image to decide which image tarball we're going to import. For instance, we want to import the missing container image rancher/harvester-upgrade $ curl -sfL https://releases.rancher.com/harvester/v1.1.2/harvester-v1.1.2-amd64.iso -o harvester.iso $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images-lists images-lists $ grep -R &quot;rancher/harvester-upgrade&quot; images-lists/ images-lists/harvester-images-v1.1.2.txt:docker.io/rancher/harvester-upgrade:v1.1.2 Find out the location of the image tarball, and extract it from the ISO image. Decompress the extracted zstd image tarball. $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images/harvester-images-v1.1.2.tar.zst harvester.tar.zst $ zstd -d --rm harvester.tar.zst Upload the image tarball to the Harvester nodes that need recover. Finally, execute the following command to import the container images on each of them. $ ctr -n k8s.io images import harvester.tar $ rm harvester.tar Find the missing images on that node from the other nodes, then export the images from the node where the images still exist and import them on the missing node. To prevent this from happening, we recommend cleaning up unused container images from the previous version after each successful Harvester upgrade if the image store disk space is stressed. We provided a harv-purge-images script that makes cleaning up disk space easy, especially for container image storage. The script has to be executed on each Harvester node. For example, if the cluster was originally in v1.1.1, and now it gets upgraded to v1.1.2, you can do the following to discard the container images that are only used in v1.1.1 but no longer needed in v1.1.2: # on each node $ ./harv-purge-images.sh v1.1.1 v1.1.2 caution The script only downloads the image lists and compares the two to calculate the difference between the two versions. It does not communicate with the cluster and, as a result, does not know what version the cluster was upgraded from.We published image lists for each version released since v1.1.0. For clusters older than v1.1.0, you have to clean up the old images manually.","keywords":"","version":"v1.1"},{"title":"Host Management","type":0,"sectionRef":"#","url":"/v1.1/host/","content":"Host Management Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance​ For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node​ Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you’re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node​ Deleting a node is done in two phases: Delete the node from Harvester Go to the Hosts pageOn the node you want to modify, click ⋮ &gt; Delete Uninstall RKE2 from the node Login to the node as rootRun rke2-uninstall.sh to delete the whole RKE2 service. caution You will lose all data of the control plane node after deleting the RKE2 service. note There's a known issue about node hard delete. Once resolved, the last step can be skipped. Multi-disk Management​ Add Additional Disks​ Users can view and add multiple disks as additional data volumes from the edit host page. Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config. Select the Storage tab and click Add Disk. caution As of Harvester v1.0.2, we no longer support adding partitions as additional disks. If you want to add it as an additional disk, be sure to delete all partitions first (e.g., using fdisk). Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. Last, you can click ⋮ &gt; Edit Config again to check the newly added disk. Meanwhile, you can also add the &quot;Host/Disk&quot; tag (details are described in the next section). note In order for Harvester to identify the disks, each disk needs to have a unique WWN. Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above. Storage Tags​ The storage tag feature enables only certain nodes or disks to be used for storing Longhorn volume data. For example, performance-sensitive data can use only the high-performance disks which can be tagged as fast, ssd or nvme, or only the high-performance nodes tagged as baremetal. This feature supports both disks and nodes. Setup​ The tags can be set up through the Harvester UI on the host page: Click Hosts -&gt; Edit Config -&gt; StorageClick Add Host/Disk Tags to start typing and hit enter to add new tags.Click Save to update tags.On the StorageClasses page, create a new storage class and select those defined tags on the Node Selector and Disk Selector fields. All the existing scheduled volumes on the node or disk won’t be affected by the new tags. note When multiple tags are specified for a volume, the disk and the nodes (that the disk belongs to) must have all the specified tags to become usable. Remove disks​ Before removing a disk, you must first evict Longhorn replicas on the disk. note The replica data would be rebuilt to another disk automatically to keep the high availability. Identify the disk to remove (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select the node name and go to the Storage tab.Find the disk you want to remove. Let's assume we want to remove /dev/sdb, and the disk's mount point is /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04. Evict replicas (Longhorn dashboard)​ Please follow this session to enable the embedded Longhorn dashboard.Visit the Longhorn dashboard and go to the Node page.Expand the node containing the disk. Confirm the mount point /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04 is in the disks list. Select Edit node and disks. Scroll to the disk you want to remove. Set Scheduling to Disable.Set Eviction Requested to True.Select Save. Do not select the delete icon. The disk will be disabled. Please wait until the disk replica count becomes 0 to proceed with removing the disk. Remove the disk (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select ⋮ &gt; Edit Config.Go to the Storage tab and select x to remove the disk. Select Save to remove the disk. Ksmtuned Mode​ Available as of v1.1.0 Ksmtuned is a KSM automation tool deployed as a DaemonSet to run Ksmtuned on each node. It will start or stop the KSM by watching the available memory percentage ratio (i.e. Threshold Coefficient). By default, you need to manually enable Ksmtuned on each node UI. You will be able to see the KSM statistics from the node UI after 1-2 minutes.(check KSM for more details). Quick Run​ Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config.Select the Ksmtuned tab and select Run in Run Strategy.(Optional) You can modify Threshold Coefficient as needed. Click Save to update.Wait for about 1-2 minutes and you can check its Statistics by clicking Your Node &gt; Ksmtuned tab. Parameters​ Run Strategy: Stop: Stop Ksmtuned and KSM. VMs can still use shared memory pages.Run: Run Ksmtuned.Prune: Stop Ksmtuned and prune KSM memory pages. Threshold Coefficient: configures the available memory percentage ratio. If the available memory is less than the threshold, KSM will be started; otherwise, KSM will be stopped. Merge Across Nodes: specifies if pages from different NUMA nodes can be merged. Mode: Standard: The default mode. The control node ksmd uses about 20% of a single CPU. It uses the following parameters: Boost: 0 Decay: 0 Maximum Pages: 100 Minimum Pages: 100 Sleep Time: 20 High-performance: Node ksmd uses 20% to 100% of a single CPU and has higher scanning and merging efficiency. It uses the following parameters: Boost: 200 Decay: 50 Maximum Pages: 10000 Minimum Pages: 100 Sleep Time: 20 Customized: You can customize the configuration to reach the performance that you want. Ksmtuned uses the following parameters to control KSM efficiency: Parameters\tDescriptionBoost\tThe number of scanned pages is incremented each time if the available memory is less than the Threshold Coefficient. Decay\tThe number of scanned pages is decremented each time if the available memory is greater than the Threshold Coefficient. Maximum Pages\tMaximum number of pages per scan. Minimum Pages\tThe minimum number of pages per scan, also the configuration for the first run. Sleep Time (ms)\tThe interval between two scans, which is calculated with the formula (Sleep Time * 16 * 1024* 1024 / Total Memory). Minimum: 10ms. For example, assume you have a 512GiB memory node that uses the following parameters: Boost: 300 Decay: 100 Maximum Pages: 5000 Minimum Pages: 1000 Sleep Time: 50 When Ksmtuned starts, initialize pages_to_scan in KSM to 1000 (Minimum Pages) and set sleep_millisecs to 10 (50 * 16 * 1024 * 1024 / 536870912 KiB &lt; 10). KSM starts when the available memory falls below the Threshold Coefficient. If it detects that it is running, pages_to_scan increments by 300 (Boost) every minute until it reaches 5000 (Maximum Pages). KSM will stop when the available memory is above the Threshold Coefficient. If it detects that it is stopped, pages_to_scan decrements by 100 (Decay) every minute until it reaches 1000 (Minimum Pages).","keywords":"","version":"v1.1"},{"title":"Storage Network","type":0,"sectionRef":"#","url":"/v1.1/advanced/storagenetwork","content":"Storage Network Harvester uses Longhorn as its built-in storage system to provide block device volumes for VMs and Pods. If the user wishes to isolate Longhorn replication traffic from the Kubernetes cluster network (i.e. the management network) or other cluster-wide workloads. Users can allocate a dedicated storage network for Longhorn replication traffic to get better network bandwidth and performance. For more information, please see Longhorn Storage Network note Configuring Longhorn settings directly is not recommended, as this can lead to untested situations. Prerequisites​ There are some prerequisites before configuring the Harvester Storage Network setting. Well-configured Cluster Network and VLAN Config. Users have to ensure the Cluster Network is configured and VLAN Config will cover all nodes and ensure the network connectivity is working and expected in all nodes. All VMs should be stopped. We recommend checking the VM status with the following command and should get an empty result.kubectl get -A vmi All pods that are attached to Longhorn Volumes should be stopped. Users could skip this step with the Harvester Storage Network setting. Harvester will stop Longhorn-related pods automatically. All ongoing image uploads or downloads should be either completed or deleted. caution If the Harvester cluster was upgraded from v1.0.3, please check if Whereabouts CNI is installed properly before you move on to the next step. We will always recommend following this guide to check. Issue 3168 describes that the Harvester cluster will not always install Whereabouts CNI properly. Verify the ippools.whereabouts.cni.cncf.io CRD exists with the following command. kubectl get crd ippools.whereabouts.cni.cncf.io If the Harvester cluster doesn't have ippools.whereabouts.cni.cncf.io, please add these two CRDs before configuring storage-network setting. kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_ippools.yaml kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml Configuration Example​ VLAN ID Please check with your network switch setting, and provide a dedicated VLAN ID for Storage Network. Well-configured Cluster Network and VLAN Config Please refer Networking page for more details and configure Cluster Network and VLAN Config but not Networks. IP range for Storage Network IP range should not conflict or overlap with Kubernetes cluster networks(10.42.0.0/16, 10.43.0.0/16, 10.52.0.0/16 and 10.53.0.0/16 are reserved).IP range should be in IPv4 CIDR format and Longhorn pods use Storage Network as follows: instance-manger-e and instance-manager-r pods: These require 2 IPs per node. During an upgrade, two versions of these pods will exist (old and new), and the old version will be deleted once the upgrade is successful.backing-image-ds pods: These are employed to process on-the-fly uploads and downloads of backing image data sources. These pods will be removed once the image uploads or downloads are completed.backing-image-manager pods: 1 IP per disk, similar to the instance manager pods. Two versions of these will coexist during an upgrade, and the old ones will be removed after the upgrade is completed.The required number of IPs is calculated using a simple formula: Required Number of IPs = Number of Nodes * 4 + Number of Disks * 2 + Number of Images to Download/Upload For example, if your cluster has five nodes, each node has two disks, and ten images will be uploaded simultaneously, the IP range should be greater than or equal to /26 (5 * 4 + 5 * 2 * 2 + 10 = 50). We will take the following configuration as an example to explain the details of the Storage Network VLAN ID for Storage Network: 100Cluster Network: storageIP range: 192.168.0.0/24 Configuration Process​ Harvester will create Multus NetworkAttachmentDefinition from the configuration, stop pods related to Longhorn Volume, update Longhorn setting, and restart previous pods. Before Applying Harvester Storage Network Setting​ Here we have two cases. Expect that VM VLAN traffic and Longhorn Storage Network use the same group of physical interfaces.Expect that VM VLAN traffic and Longhorn Storage Network use different physical interfaces. Longhorn will send replication traffic through the specific interfaces shown as the red line in the figure. Same Physical Interfaces​ Take eth2 and eth3 as an example for VM VLAN traffic and Longhorn Storage Network simultaneously. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth2 and eth3 and remember the ClusterNetwork name for the further step. Different Physical Interfaces​ eth2 and eth3 are for VM VLAN Traffic. eth4 and eth5 are for Longhorn Storage Network. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth4 and eth5 for Storage Network and remember the ClusterNetwork name for the further step. Harvester Storage Network Setting​ Harvester Storage Network setting will need range, clusterNetwork, vlan field to construct Multus NetworkAttachmentDefinition for Storage Network usage. You could apply this setting via Web UI or CLI. Web UI​ Harvester Storage Network setting could be easily modified on the Settings &gt; storage-network page. CLI​ Users could use this command to edit Harvester Storage Network setting. kubectl edit settings.harvesterhci.io storage-network The value format is JSON string or empty string as shown in below. { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } The full configuration will be like this example. apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: name: storage-network value: '{&quot;vlan&quot;:100,&quot;clusterNetwork&quot;:&quot;storage&quot;,&quot;range&quot;:&quot;192.168.0.0/24&quot;}' caution Because of the design, Harvester will treat extra and insignificant characters in JSON string as a different configuration. After Applying Harvester Storage Network Setting​ After applying Harvester's Storage Network setting, Harvester will stop all pods that are related to Longhorn volumes. Currently, Harvester has some pods listed below that will be stopped during setting. PrometheusGrafanaAlertmanagerVM Import Controller Harvester will also create a new NetworkAttachmentDefinition and update the Longhorn Storage Network setting. Once the Longhorn setting is updated, Longhorn will restart all instance-manager-r, instance-manager-e, and backing-image-manager pods to apply the new network configuration, and Harvester will restart the pods. note Harvester will not start VM automatically. Users should check whether the configuration is completed or not in the next section and start VM manually on demand. Verify Configuration is Completed​ Step 1​ Check if Harvester Storage Network setting's status is True and the type is configured. kubectl get settings.harvesterhci.io storage-network -o yaml Completed Setting Example: apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: annotations: storage-network.settings.harvesterhci.io/hash: da39a3ee5e6b4b0d3255bfef95601890afd80709 storage-network.settings.harvesterhci.io/net-attach-def: &quot;&quot; storage-network.settings.harvesterhci.io/old-net-attach-def: &quot;&quot; creationTimestamp: &quot;2022-10-13T06:36:39Z&quot; generation: 51 name: storage-network resourceVersion: &quot;154638&quot; uid: 2233ad63-ee52-45f6-a79c-147e48fc88db status: conditions: - lastUpdateTime: &quot;2022-10-13T13:05:17Z&quot; reason: Completed status: &quot;True&quot; type: configured Step 2​ Verify the readiness of all Longhorn instance-manager-e, instance-manager-r, and backing-image-manager pods, and confirm that their networks are correctly configured. Execute the following command to inspect a pod's details: kubectl -n longhorn-system describe pod &lt;pod-name&gt; If you encounter an event resembling the following one, the Storage Network might have run out of its available IPs: Events: Type Reason Age From Message ---- ------ ---- ---- ------- .... Warning FailedCreatePodSandBox 2m58s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox &quot;04e9bc160c4f1da612e2bb52dadc86702817ac557e641a3b07b7c4a340c9fc48&quot;: plugin type=&quot;multus&quot; name=&quot;multus-cni-network&quot; failed (add): [longhorn-system/ba cking-image-ds-default-image-lxq7r/7d6995ee-60a6-4f67-b9ea-246a73a4df54:storagenetwork-sdfg8]: error adding container to network &quot;storagenetwork-sdfg8&quot;: erro r at storage engine: Could not allocate IP in range: ip: 172.16.0.1 / - 172.16.0.6 / range: net.IPNet{IP:net.IP{0xac, 0x10, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0xff, 0xf8}} .... Please reconfigure the Storage Network with a sufficient IP range. note If the Storage Network has run out of IPs, you might encounter the same error when you upload/download images. Please delete the related images and reconfigure the Storage Network with a sufficient IP range. Step 3​ Check the k8s.v1.cni.cncf.io/network-status annotations and ensure that an interface named lhnet1 exists, with an IP address within the designated IP range. Users could use the following command to show all Longhorn Instance Manager to verify. kubectl get pods -n longhorn-system -l longhorn.io/component=instance-manager -o yaml Correct Network Example: apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/containerID: 2518b0696f6635896645b5546417447843e14208525d3c19d7ec6d7296cc13cd cni.projectcalico.org/podIP: 10.52.2.122/32 cni.projectcalico.org/podIPs: 10.52.2.122/32 k8s.v1.cni.cncf.io/network-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] k8s.v1.cni.cncf.io/networks: '[{&quot;namespace&quot;: &quot;harvester-system&quot;, &quot;name&quot;: &quot;storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;}]' k8s.v1.cni.cncf.io/networks-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] kubernetes.io/psp: global-unrestricted-psp longhorn.io/last-applied-tolerations: '[{&quot;key&quot;:&quot;kubevirt.io/drain&quot;,&quot;operator&quot;:&quot;Exists&quot;,&quot;effect&quot;:&quot;NoSchedule&quot;}]' Omitted... Start VM Manually​ After verifying the configuration, users could start VM manually on demand.","keywords":"","version":"v1.1"},{"title":"PXE Boot Installation","type":0,"sectionRef":"#","url":"/v1.1/install/pxe-boot-install","content":"PXE Boot Installation Starting from version 0.2.0, Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples. Prerequisite​ info Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers​ An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10, and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/. Preparing Boot Files​ Download the required files from the Harvester releases page. The ISO: harvester-&lt;version&gt;-amd64.isoThe kernel: harvester-&lt;version&gt;-vmlinuz-amd64The initrd: harvester-&lt;version&gt;-initrd-amd64The rootfs squashfs image: harvester-&lt;version&gt;-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts​ When performing an automatic installation, there are two modes: CREATE: we are installing a node to construct an initial Harvester cluster.JOIN: we are installing a node to join an existing Harvester cluster. CREATE Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml scheme_version: 1 token: token # Replace with a desired token os: hostname: node1 # Set a hostname. This can be omitted if DHCP server offers hostnames ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install: mode: create management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console vip: 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use vip_mode: static # Or dhcp, check configuration file for more information # vip_hw_addr: 52:54:00:ec:0e:0b # Leave empty when vip_mode is static For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create. note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp). See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter. JOIN Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml scheme_version: 1 server_url: https://10.100.0.99:443 # Should be the VIP set up in &quot;CREATE&quot; config token: token os: hostname: node2 ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password dns_nameservers: - 1.1.1.1 - 8.8.8.8 install: mode: join management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join. DHCP Server Configuration​ note In the PXE installation scenario, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. In the ISO installation scenario, when the management network interface is in DHCP mode, you are also required to add the routers option (option routers) when configuring the DHCP server. For example: Harvester Host:~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see ISC DHCPv4 Option Configuration. The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16; subnet 10.100.0.0 netmask 255.255.255.0 { option routers 10.100.0.10; option domain-name-servers 192.168.2.1; range 10.100.0.100 10.100.0.253; } group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } group { # join group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-join-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-join&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node2 { hardware ethernet 52:54:00:69:d5:92; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration​ For more information about Harvester configuration, please refer to the Harvester configuration page. By default, the first node will be the management node of the cluster. When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config. In this case, at least three different zones are required. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support​ UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program​ Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration​ If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } elsif substring (option vendor-class-identifier, 0, 10) = &quot;HTTPClient&quot; { # UEFI HTTP Boot option vendor-class-identifier &quot;HTTPClient&quot;; filename &quot;http://10.100.0.10/harvester/ipxe.efi&quot;; } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi. The iPXE Script for UEFI Boot​ It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-&lt;version&gt;-vmlinuz initrd=harvester-&lt;version&gt;-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot The parameter initrd=harvester-&lt;version&gt;-initrd is required. Useful Kernel Parameters​ Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7). ip=dhcp​ If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces. rd.net.dhcp.retry=&lt;cnt&gt;​ Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=&lt;cnt&gt;to retry DHCP request for &lt;cnt&gt; times.","keywords":"Harvester harvester Rancher rancher Install Harvester Installing Harvester Harvester Installation PXE Boot Install","version":"v1.1"},{"title":"ISO Installation","type":0,"sectionRef":"#","url":"/v1.1/install/iso-install","content":"ISO Installation Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image. To get the ISO image, download 💿 harvester-v1.x.x-amd64.iso from the Harvester releases page. During the installation, you can either choose to create a new Harvester cluster or join the node to an existing Harvester cluster. The following video shows a quick overview of an ISO installation. Installation Steps​ Mount the Harvester ISO file and boot the server by selecting the Harvester Installer option. Use the arrow keys to choose an installation mode. By default, the first node will be the management node of the cluster. Create a new Harvester cluster: creates an entirely new Harvester cluster. Join an existing Harvester cluster: joins an existing Harvester cluster. You need the VIP and cluster token of the cluster you want to join. info When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config by providing a URL of Harvester configuration on the customize the host step. In this case, at least three different zones are required. Choose the installation disk you want to install the Harvester cluster on. By default, Harvester uses GUID Partition Table (GPT) partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select Master boot record (MBR). Choose the data disk you want to store VM data on. We recommend choosing a separate disk to store VM data. Configure the HostName of the node. Configure network interface(s) for the management network. By default, Harvester creates a bonded NIC named mgmt-bo, and the IP address can be configured via DHCP or statically assigned. note It is not possible to change the node IP throughout the lifecycle of a Harvester cluster. If using DHCP, you must ensure the DHCP server always offers the same IP for the same node. If the node IP is changed, the related node cannot join the cluster and might even break the cluster. In addition, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. For example: Linux~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see DHCP Server Configuration. (Optional) Configure the DNS Servers. Use commas as a delimiter to add more DNS servers. Leave it blank to use the default DNS server. Configure the virtual IP (VIP) by selecting a VIP Mode. This VIP is used to access the cluster or for other nodes to join the cluster. note If using DHCP to configure the IP address, you need to configure a static MAC-to-IP address mapping on your DHCP server to have a persistent virtual IP (VIP), and the VIP must be unique. Configure the Cluster token. This token is used for adding other nodes to the cluster. Configure and confirm a Password to access the node. The default SSH user is rancher. Configure NTP servers to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org. Use commas as a delimiter to add more NTP servers. (Optional) If you need to use an HTTP proxy to access the outside world, enter the Proxy address. Otherwise, leave this blank. (Optional) You can choose to import SSH keys by providing HTTP URL. For example, your GitHub public keys https://github.com/&lt;username&gt;.keys can be used. (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Review and confirm your installation options. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, your node restarts. After the restart, the Harvester console displays the management URL and status. The default URL of the web interface is https://your-virtual-ip. You can use F12 to switch from the Harvester console to the Shell and type exit to go back to the Harvester console. You will be prompted to set the password for the default admin user when logging in for the first time. Known Issue​ Installer may crash when using an older graphics card/monitor​ In some cases, if you are using an older graphics card/monitor, you may encounter a panic: invalid dimensions error during ISO installation. This is a known issue we are working on, and will be fixed in future releases. Here is a workaround for this issue: Boot up with the ISO, and press E to edit the first menu entry: Append vga=792 to the line started with $linux: Press Ctrl+X or F10 to boot up. Fail to join nodes using FQDN to a cluster which has custom SSL certificate configured​ You may encounter that newly joined nodes stay in the Not Ready state indefinitely. This is likely the outcome if you already have a set of custom SSL certificates configured on the to-be-joined Harvester cluster and provide an FQDN instead of a VIP address for the management address during the Harvester installation. You can check the SSL certificates on the Harvester dashboard's setting page or using the command line tool kubectl get settings.harvesterhci.io ssl-certificates to see if there is any custom SSL certificate configured (by default, it is empty). The second thing to look at is the joining nodes. Try to get access to the nodes via consoles or SSH sessions and then check the log of rancherd: $ journalctl -u rancherd.service Oct 03 08:58:52 node-0 systemd[1]: Starting Rancher Bootstrap... Oct 03 08:58:52 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:52Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/50-defaults.yaml]&quot; Oct 03 08:58:52 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:52Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/91-harvester-bootstrap-repo.yaml]&quot; Oct 03 08:58:52 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:52Z&quot; level=info msg=&quot;Loading config file [/etc/rancher/rancherd/config.yaml]&quot; Oct 03 08:58:52 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:52Z&quot; level=info msg=&quot;Bootstrapping Rancher (v2.6.11/v1.24.11+rke2r1)&quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;Writing plan file to /var/lib/rancher/rancherd/plan/plan.json&quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;Applying plan with checksum &quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231003-085853-applied.plan/_0&quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;Running command: /usr/bin/env [sh /var/lib/rancher/rancherd/install.sh]&quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent configuration directory /etc/rancher/agent&quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent var directory /var/lib/rancher/agent&quot; Oct 03 08:58:53 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:53Z&quot; level=info msg=&quot;[stderr]: [WARN] /usr/local is read-only or a mount point; installing to /opt/rancher-system-agent&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Determined CA is necessary to connect to Rancher&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded CA certificate&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Value from https://harvester.192.168.48.240.sslip.io:443/cacerts is an x509 certificate&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully tested Rancher connection&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Downloading rancher-system-agent binary from https://harvester.192.168.48.240.sslip.io:443/assets/rancher-system-agent-amd64&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded the rancher-system-agent binary.&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Downloading rancher-system-agent-uninstall.sh script from https://harvester.192.168.48.240.sslip.io:443/assets/system-agent-uninstall.sh&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded the rancher-system-agent-uninstall.sh script.&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Generating Cattle ID&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded Rancher connection information&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] systemd: Creating service file&quot; Oct 03 08:58:54 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:54Z&quot; level=info msg=&quot;[stdout]: [INFO] Creating environment file /etc/systemd/system/rancher-system-agent.env&quot; Oct 03 08:58:55 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:55Z&quot; level=info msg=&quot;[stdout]: [INFO] Enabling rancher-system-agent.service&quot; Oct 03 08:58:55 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:55Z&quot; level=info msg=&quot;[stderr]: Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.&quot; Oct 03 08:58:55 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:55Z&quot; level=info msg=&quot;[stdout]: [INFO] Starting/restarting rancher-system-agent.service&quot; Oct 03 08:58:55 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:55Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231003-085853-applied.plan/_1&quot; Oct 03 08:58:55 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:55Z&quot; level=info msg=&quot;Running command: /usr/bin/rancherd [probe]&quot; Oct 03 08:58:55 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:55Z&quot; level=info msg=&quot;[stderr]: time=\\&quot;2023-10-03T08:58:55Z\\&quot; level=info msg=\\&quot;Running probes defined in /var/lib/rancher/rancherd/plan/plan.json\\&quot;&quot; Oct 03 08:58:56 node-0 rancherd[2013]: time=&quot;2023-10-03T08:58:56Z&quot; level=info msg=&quot;[stderr]: time=\\&quot;2023-10-03T08:58:56Z\\&quot; level=info msg=\\&quot;Probe [kubelet] is unhealthy\\&quot;&quot; The above log shows that rancherd is waiting for kubelet to become healthy. rancherd is doing nothing wrong and is working as expected. The next step is to check the rancher-system-agent: $ journalctl -u rancher-system-agent.service Oct 03 09:12:18 node-0 systemd[1]: rancher-system-agent.service: Scheduled restart job, restart counter is at 153. Oct 03 09:12:18 node-0 systemd[1]: Stopped Rancher System Agent. Oct 03 09:12:18 node-0 systemd[1]: Started Rancher System Agent. Oct 03 09:12:18 node-0 rancher-system-agent[5217]: time=&quot;2023-10-03T09:12:18Z&quot; level=info msg=&quot;Rancher System Agent version v0.2.13 (4fa9427) is starting&quot; Oct 03 09:12:18 node-0 rancher-system-agent[5217]: time=&quot;2023-10-03T09:12:18Z&quot; level=info msg=&quot;Using directory /var/lib/rancher/agent/work for work&quot; Oct 03 09:12:18 node-0 rancher-system-agent[5217]: time=&quot;2023-10-03T09:12:18Z&quot; level=info msg=&quot;Starting remote watch of plans&quot; Oct 03 09:12:18 node-0 rancher-system-agent[5217]: time=&quot;2023-10-03T09:12:18Z&quot; level=info msg=&quot;Initial connection to Kubernetes cluster failed with error Get \\&quot;https://harvester.192.168.48.240.sslip.io/version\\&quot;: x509: certificate signed by unknown authority, removing CA data and trying again&quot; Oct 03 09:12:18 node-0 rancher-system-agent[5217]: panic: error while connecting to Kubernetes cluster with nullified CA data: Get &quot;https://harvester.192.168.48.240.sslip.io/version&quot;: x509: certificate signed by unknown authority Oct 03 09:12:18 node-0 rancher-system-agent[5217]: goroutine 37 [running]: Oct 03 09:12:18 node-0 rancher-system-agent[5217]: github.com/rancher/system-agent/pkg/k8splan.(*watcher).start(0xc00051a100, {0x18bd5c0?, 0xc000488800}) Oct 03 09:12:18 node-0 rancher-system-agent[5217]: /go/src/github.com/rancher/system-agent/pkg/k8splan/watcher.go:99 +0x9b4 Oct 03 09:12:18 node-0 rancher-system-agent[5217]: created by github.com/rancher/system-agent/pkg/k8splan.Watch Oct 03 09:12:18 node-0 rancher-system-agent[5217]: /go/src/github.com/rancher/system-agent/pkg/k8splan/watcher.go:63 +0x155 Oct 03 09:12:18 node-0 systemd[1]: rancher-system-agent.service: Main process exited, code=exited, status=2/INVALIDARGUMENT Oct 03 09:12:18 node-0 systemd[1]: rancher-system-agent.service: Failed with result 'exit-code'. If you see a similar log output, you need to manually add the CA to the trust list on each joining node with the following commands: # prepare the CA as embedded-rancher-ca.pem on the nodes $ sudo cp embedded-rancher-ca.pem /etc/pki/trust/anchors/ $ sudo update-ca-certificates After adding the CA to the trust list, the nodes can join to the cluster successfully.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.1"},{"title":"Hardware and Network Requirements","type":0,"sectionRef":"#","url":"/v1.1/install/requirements","content":"Hardware and Network Requirements As an HCI solution on bare metal servers, there are minimum node hardware and network requirements to install and run Harvester. Hardware Requirements​ Harvester nodes have the following hardware requirements and recommendations for installation and testing. Type\tRequirements and RecommendationsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum for testing; 16-core or above required for production Memory\t32 GB minimum for testing; 64 GB or above required for production Disk Capacity\t200 GB minimum for testing; 500 GB or above required for production Disk Performance\t5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for etcd Network Card\t1 Gbps Ethernet minimum for testing; 10 Gbps Ethernet minimum required for production Network Switch\tTrunking of ports required for VLAN support info A three-node cluster is required to realize the multi-node features of Harvester fully. The first node always defaults to be a management node of the cluster.When there are three or more nodes, the two other nodes added first are automatically promoted to management nodes to form a high availability (HA) cluster.We recommend server-class hardware for the best results. Laptops and nested virtualization are not officially supported.The product_uuid fetched from /sys/class/dmi/id/product_uuid in Linux must be unique in each node. Otherwise, features like VM live migration will be affected. For more information, refer to #4025. Network Requirements​ Harvester nodes have the following network requirements for installation. Port Requriements for Harvester Nodes​ Harvester nodes require the following port connections or inbound rules. Typically, all outbound traffic is allowed. Protocol\tPort\tSource\tDescriptionTCP\t2379\tHarvester management nodes\tEtcd client port TCP\t2381\tHarvester management nodes\tEtcd health checks TCP\t2380\tHarvester management nodes\tEtcd peer port TCP\t10010\tHarvester management and compute nodes\tContainerd TCP\t6443\tHarvester management nodes\tKubernetes API TCP\t9345\tHarvester management nodes\tKubernetes API TCP\t10252\tHarvester management nodes\tKube-controller-manager health checks TCP\t10257\tHarvester management nodes\tKube-controller-manager secure port TCP\t10251\tHarvester management nodes\tKube-scheduler health checks TCP\t10259\tHarvester management nodes\tKube-scheduler secure port TCP\t10250\tHarvester management and compute nodes\tKubelet TCP\t10256\tHarvester management and compute nodes\tKube-proxy health checks TCP\t10258\tHarvester management nodes\tCloud-controller-manager TCP\t9091\tHarvester management and compute nodes\tCanal calico-node felix TCP\t9099\tHarvester management and compute nodes\tCanal CNI health checks UDP\t8472\tHarvester management and compute nodes\tCanal CNI with VxLAN TCP\t2112\tHarvester management nodes\tKube-vip TCP\t6444\tHarvester management and compute nodes\tRKE2 agent TCP\t6060\tHarvester management and compute nodes\tNode-disk-manager TCP\t10246/10247/10248/10249\tHarvester management and compute nodes\tNginx worker process TCP\t8181\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t8444\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t10245\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t80\tHarvester management and compute nodes\tNginx TCP\t9796\tHarvester management and compute nodes\tNode-exporter TCP\t30000-32767\tHarvester management and compute nodes\tNodePort port range TCP\t22\tHarvester management and compute nodes\tsshd UDP\t68\tHarvester management and compute nodes\tWicked TCP\t3260\tHarvester management and compute nodes\tiscsid Port Requirements for Integrating Harvester with Rancher​ If you want to integrate Harvester with Rancher, you need to make sure that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. When provisioning VMs with Kubernetes clusters from Rancher into Harvester, you need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise, the cluster won't be manageable by Rancher. For more information, refer to Rancher Architecture. Port Requirements for K3s or RKE/RKE2 Clusters​ For the port requirements for guest clusters deployed inside Harvester VMs, refer to the following links: K3s NetworkingRKE PortsRKE2 Networking","keywords":"Installation Requirements","version":"v1.1"},{"title":"Update Harvester Configuration After Installation","type":0,"sectionRef":"#","url":"/v1.1/install/update-harvester-configuration","content":"Update Harvester Configuration After Installation Harvester's OS has an immutable design, which means most files in the OS revert to their pre-configured state after a reboot. The Harvester OS loads the pre-configured values of system components from configuration files during the boot time. This page describes how to edit some of the most-requested Harvester configurations. To update a configuration, you must first update the runtime value in the system and then update configuration files to make the changes persistent between reboots. note If you upgrade from a version before v1.1.2, the cloud-init file in examples will be /oem/99_custom.yaml. Please substitute the value if needed. DNS servers​ Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Edit /etc/sysconfig/network/config and update the following line. Use a space to separate DNS server addresses if there are multiple servers. NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot; Update and reload the configuration with the following command: netconfig update Confirm the file /etc/resolv.conf contains the correct DNS servers with the cat command: cat /etc/resolv.conf Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the value under the yaml path stages.initramfs[0].commands. The commands array must contain a line to manipulate the NETCONFIG_DNS_STATIC_SERVERS config. Add the line if the line doesn't exist. The following example adds a line to change the NETCONFIG_DNS_STATIC_SERVERS config: stages: initramfs: - commands: - sed -i 's/^NETCONFIG_DNS_STATIC_SERVERS.*/NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot;/' /etc/sysconfig/network/config Replace the DNS server addresses and save the file. Harvester sets up new servers after rebooting. NTP servers​ Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Edit /etc/systemd/timesyncd.conf and specify NTP servers in the NTP= setting: [Time] NTP = 0.suse.pool.ntp.org 1.suse.pool.ntp.org Restart the systemd-timesyncd.service service: systemctl restart systemd-timesyncd.service Display the timesync status: timedatectl timesync-status Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].timesyncd. The timesyncd map must be in the following format: stages: initramfs: - ... timesyncd: NTP: 0.suse.pool.ntp.org 1.suse.pool.ntp.org Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].systemctl.enable. The array must have the two services (systemd-timesyncd and systemd-time-wait-sync) enabled: stages: initramfs: - ... systemctl: enable: systemd-timesyncd systemd-time-wait-sync disable: [] start: [] mask: [] SSH keys of user rancher​ Runtime change​ Log in to a Harvester node as user rancher. See how to log into a Harvester node for more details.Edit /home/rancher/.ssh/authorized_keys to add or remove keys. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.network[0].authorized_keys.rancher. Add or remove keys in the rancher array: stages: network: - ... authorized_keys: rancher: - key1 - key2 Password of user rancher​ Runtime change​ Log in to a Harvester node as user rancher. See how to log into a Harvester node for more details.To reset the password for the user rancher, run the command passwd. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].users.rancher.passwd. Refer to the configuration os.password for details on how to specify the password in an encrypted form. Bonding slaves​ You can update the slave interfaces of Harvester's management bonding interface mgmt-bo. Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Identify the interface names with the following command: ip a Edit /etc/sysconfig/network/ifcfg-mgmt-bo and update the lines associated with bonding slaves and bonding mode: BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' Restart the network with the wicked ifreload command: wicked ifreload mgmt-bo caution A mistake in the configuration may disrupt the SSH session. Configuration persistence​ Beginning with v1.1.2, the persistent name of the cloud-init file is /oem/90_custom.yaml. Harvester now uses a newer version of Elemental, which creates the file during installation. When upgrading from an earlier version to v1.1.2 or later, Harvester retains the old file name (/oem/99_custom.yaml) to avoid confusion. You can manually rename the file to /oem/90_custom.yaml if necessary. Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].files. More specifically, update the content of the /etc/sysconfig/network/ifcfg-mgmt-bo file and edit the BONDING_SLAVE_X and BONDING_MODULE_OPTS entries accordingly: stages: initramfs: - ... files: - path: /etc/sysconfig/network/ifcfg-mgmt-bo permissions: 384 owner: 0 group: 0 content: |+ STARTMODE='onboot' BONDING_MASTER='yes' BOOTPROTO='none' POST_UP_SCRIPT=&quot;wicked:setup_bond.sh&quot; BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' DHCLIENT_SET_DEFAULT_ROUTE='no' encoding: &quot;&quot; ownerstring: &quot;&quot; - path: /etc/sysconfig/network/ifcfg-ens6 permissions: 384 owner: 0 group: 0 content: | STARTMODE='hotplug' BOOTPROTO='none' encoding: &quot;&quot; ownerstring: &quot;&quot; note If you didn't select an interface during installation, you must add an entry to initialize the interface. Please check the /etc/sysconfig/network/ifcfg-ens6 file creation in the above example. The file name should be /etc/sysconfig/network/ifcfg-&lt;interface-name&gt;.","keywords":"Harvester configuration Configuration","version":"v1.1"},{"title":"USB Installation","type":0,"sectionRef":"#","url":"/v1.1/install/usb-install","content":"USB Installation Create a bootable USB flash drive​ There are a couple of ways to create a USB installation flash drive. balenaEtcher​ balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive. dd command​ On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. caution Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb_device&gt; bs=64k Known issues​ How to locate grub.cfg and set the root partition​ Sometimes when using a USB to mount the Harvester ISO disk, it drops you in the GRUB2 shell while booting. In this case, it helps to know how to locate grub.cfg and set the root partition to proceed with the installation. To view the grub.cfg from the ISO, run the command cat /boot/grub2/grub.cfg. You can see the current root partition in the grub.cfg. To set a new root partition, run the command set root=(hdN,msdos1), where N denotes N numbers of possible drives. N should be the drive your USB is mounted to. Once the root partition has been set, where the ISO is located in the msdos1 partition, run the command chainloader /boot/grub2/x86_64-efi/grub.efi to get dropped into the GRUB2 Boot Menu for the Harvester Installer. When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens​ If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system. Graphics issue​ Firmwares of some graphic cards are not shipped in v0.3.0. You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Other issues​ Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot.For version v0.3.0 or above, try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","keywords":"","version":"v1.1"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.1/monitoring/harvester-monitoring","content":"Monitoring Available as of v0.3.0 Dashboard Metrics​ Harvester has provided a built-in monitoring integration using Prometheus. Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. note Only admin users are able to view the cluster dashboard metrics. Additionally, Grafana is provided by rancher-monitoring, so the default admin password is: prom-operator Reference: values.yaml VM Detail Metrics​ For VMs, you can view VM metrics by clicking on the VM details page &gt; VM Metrics. note The current Memory Usage is calculated based on (1 - free/total) * 100%, not (used/total) * 100%. For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100%, roughly 40%. How to Configure Monitoring Settings​ Available as of v1.0.1 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: PrometheusPrometheus Node Exporter(UI configurable as of v1.0.2) From WebUI​ On the Monitoring &amp; Logging page, you can view and change the resource settings as follows: Navigate to the Monitoring &gt; Configuration page. Click Save and the Monitoring resource will be restarted within a few seconds. Please be aware that the reboot can take some time to reload previous data. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. caution When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM(out of memory). In that case, you should increase the value of limits.memory. From CLI​ To update those values, you can also use the CLI command with: $kubectl edit managedchart rancher-monitoring -n fleet-local. For Harvester version &gt;= v1.0.1, the related path and default value are: # Prometheus configs spec.values.prometheus.prometheusSpec.resources.limits.cpu: 1000m spec.values.prometheus.prometheusSpec.resources.limits.memory: 2500Mi spec.values.prometheus.prometheusSpec.resources.requests.cpu: 750m spec.values.prometheus.prometheusSpec.resources.requests.memory: 1750Mi --- # node exporter configs spec.values.prometheus-node-exporter.resources.limits.cpu: 200m spec.values.prometheus-node-exporter.resources.limits.memory: 180Mi spec.values.prometheus-node-exporter.resources.requests.cpu: 100m spec.values.prometheus-node-exporter.resources.requests.memory: 30Mi For versions &lt;= v1.0.0, the related path and default value are not specified in the managedchart rancher-monitoring, you need to add them accordingly. Alertmanager​ Available as of v1.1.0 Harvester uses Alertmanager to collect and manage all the alerts that happened/happening in the cluster. Alertmanager Config​ Enable/Disable Alertmanager​ Alertmanager is enabled by default. You may disable it from the following config path. Change Resource Setting​ You can also change the resource settings of Alertmanager as shown in the picture above. Configure AlertmanagerConfig from WebUI​ To send the alerts to third-party servers, you need to config AlertmanagerConfig. On the WebUI, navigate to Monitoring &amp; Logging -&gt; Monitoring -&gt; Alertmanager Configs. On the Alertmanager Config: Create page, click Namespace to select the target namespace from the drop-down list and set the Name. After this, click Create in the lower right corner. Click the Alertmanager Configs you just created to continue the configuration. Click Add Receiver. Set the Name for the receiver. After this, select the receiver type, for example, Webhook, and click Add Webhook. Fill in the required parameters and click Create. Configure AlertmanagerConfig from CLI​ You can also add AlertmanagerConfig from the CLI. Exampe: a Webhook receiver in the default namespace. cat &lt;&lt; EOF &gt; a-single-receiver.yaml apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: amc-example # namespace: your value labels: alertmanagerConfig: example spec: route: continue: true groupBy: - cluster - alertname receiver: &quot;amc-webhook-receiver&quot; receivers: - name: &quot;amc-webhook-receiver&quot; webhookConfigs: - sendResolved: true url: &quot;http://192.168.122.159:8090/&quot; EOF # kubectl apply -f a-single-receiver.yaml alertmanagerconfig.monitoring.coreos.com/amc-example created # kubectl get alertmanagerconfig -A NAMESPACE NAME AGE default amc-example 27s Example of an Alert Received by Webhook​ Alerts sent to the webhook server will be in the following format: { 'receiver': 'longhorn-system-amc-example-amc-webhook-receiver', 'status': 'firing', 'alerts': [], 'groupLabels': {}, 'commonLabels': {'alertname': 'LonghornVolumeStatusWarning', 'container': 'longhorn-manager', 'endpoint': 'manager', 'instance': '10.52.0.83:9500', 'issue': 'Longhorn volume is Degraded.', 'job': 'longhorn-backend', 'namespace': 'longhorn-system', 'node': 'harv2', 'pod': 'longhorn-manager-r5bgm', 'prometheus': 'cattle-monitoring-system/rancher-monitoring-prometheus', 'service': 'longhorn-backend', 'severity': 'warning'}, 'commonAnnotations': {'description': 'Longhorn volume is Degraded for more than 5 minutes.', 'runbook_url': 'https://longhorn.io/docs/1.3.0/monitoring/metrics/', 'summary': 'Longhorn volume is Degraded'}, 'externalURL': 'https://192.168.122.200/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy', 'version': '4', 'groupKey': '{}/{namespace=&quot;longhorn-system&quot;}:{}', 'truncatedAlerts': 0 } note Different receivers may present the alerts in different formats. For details, please refer to the related documents. Known Limitation​ The AlertmanagerConfig is enforced by the namespace. Gloabl-level AlertmanagerConfig without a namespace is not supported. We have already created a GithHb issue to track upstream changes. Once the feature is available, Harvester will adopt it. View and Manage Alerts​ From Alertmanager Dashboard​ You can visit the original dashboard of Alertmanager from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy/#/alerts The overall view of the Alertmanager dashboard is as follows. You can view the details of an alert: From Prometheus Dashboard​ You can visit the original dashboard of Prometheus from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-prometheus:9090/proxy/ The Alerts menu in the top navigation bar shows all defined rules in Prometheus. You can use the filters Inactive, Pending, and Firing to quickly find the information that you need. Troubleshooting​ For Monitoring support and troubleshooting, please refer to the troubleshooting page .","keywords":"","version":"v1.1"},{"title":"Harvester Network Deep Dive","type":0,"sectionRef":"#","url":"/v1.1/networking/deep-dive","content":"Harvester Network Deep Dive The network topology below reveals how we implement the Harvester network. The diagram contains the built-in cluster network mgmt and a custom cluster network called oob. As shown above, the Harvester network primarily focuses on OSI model layer 2. We leverage Linux network devices and protocols to construct traffic paths for the communication between VM to VM, VM to host, and VM to external network devices. The Harvester network is composed of three layers, including: KubeVirt networking layer Harvester networking layer external networking layer KubeVirt Networking​ The general purpose of KubeVirt is to run VM inside the Kubernetes pod. The KubeVirt network builds the network path between the pod and VM. Please refer to the KubeVirt official document for more details. Harvester Networking​ Harvester networking is designed to build the network path between pods and the host network. It implements a management network, VLAN networks and untagged networks. We can refer to the last two networks as bridge networks, because bridge plays a vital role in their implementation. Bridge Network​ We leverage multus CNI and bridge CNI to implement the bridge network. Multus CNI is a Container Network Interface (CNI) plugin for Kubernetes that can attach multiple network interfaces to a pod. Its capability allows a VM to have one NIC for the management network and multiple NICs for the bridge network. Using the bridge CNI, the VM pod will be plugged into the L2 bridge specified in the Network Attachment Definition config. # Example 1 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;vlan100&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;mgmt-br&quot;, &quot;promiscMode&quot;: true, &quot;vlan&quot;: 100, } # Example 2 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;untagged-network&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;oob-br&quot;, &quot;promiscMode&quot;: true, &quot;ipam&quot;: {} } Example 1 is a typical VLAN configuration with VLAN ID 100, while Example 2 is an untagged network configuration with no VLAN ID. The VM pod configured using Example 1 will be plugged into the bridge mgmt-br, while the VM pod using Example 2 will be plugged into the bridge oob-br. To achieve high availability and fault tolerance, a bond device where the real NICs are bound is created to serve as the uplink of the bridge. By default, this bond device will allow the target tagged traffic/packets to pass through. harvester-0:/home/rancher # bridge -c vlan show dev oob-bo port vlan ids oob-bo 1 PVID Egress Untagged 100 200 The example above shows that the bond oob-bo allows packages with tag 1, 100 or 200. Management Network​ The management network is based on Canal. It is worth mentioning that the Canal interface where the Harvester configures the node IP is the bridge mgmt-br or a VLAN sub-interface of mgmt-br. This design has two benefits: The built-in mgmt cluster network supports both the management network and bridge network.With the VLAN network interface, we can assign a VLAN ID to the management network. As components of the mgmt cluster network, it's not allowed to delete or modify the bridge mgmt-br, the bond mgmt-bo and the VLAN device. External Networking​ External network devices typically refer to switches and DHCP servers. With a cluster network, we can group host NICs and connect them to different switches for traffic isolation. Below are some usage instructions. To allow tagged packets to pass, you need to set the port type of the external switch or other devices (such as a DHCP server) to trunk or hybrid mode and allow the specified VLAN tag. You need to configure link aggregation on the switch based on the bond mode of the peer host. Link aggregation can work in manual mode or LACP mode. The following lists the correspondence between bond mode and link aggregation mode. Bond Mode\tLink Aggregation Modemode 0(balance-rr)\tmanual mode 1(active-backup)\tnone mdoe 2(balance-oxr)\tmanual mode 3(broadcast)\tmanual mode 4(802.3ad)\tLACP mode 5(balance-tlb)\tnone mode 6(balance-alb)\tnone If you want VMs in a VLAN to be able to obtain IP addresses through the DHCP protocol, configure an IP pool for that VLAN in the DHCP server.","keywords":"Harvester Networking Topology","version":"v1.1"},{"title":"Network","type":0,"sectionRef":"#","url":"/v1.1/networking/harvester-network","content":"Network Harvester provides three types of virtual networks for virtual machines (VMs), including: Management NetworkVLAN NetworkUntagged Network The management network is usually used for VMs whose traffic only flows inside the cluster. If your VMs need to connect to the external network, use the VLAN network or untagged network. Available as of v1.0.1 Harvester also introduced storage networking to separate the storage traffic from other cluster-wide workloads. Please refer to the storage network document for more details. Management Network​ Harvester uses Canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, you can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. How to use management network​ Since the management network is built-in and doesn't require extra operations, you can add it directly when configuring the VM network. important Network interfaces of VMs connected to the management network have an MTU value of 1450. This is because a VXLAN overlay network typically has a slightly higher per-packet overhead. If any of your workloads involve transmission of network traffic, you must specify the appropriate MTU value for the affected VM network interfaces and bridges. VLAN Network​ The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. Create a VM Network​ Go to Networks &gt; VM Networks. Select Create. Configure the following settings: Namespace Name Description (optional) On the Basics tab, configure the following settings: Type: Select L2VlanNetwork.Vlan ID Cluster Network On the Route tab, select an option and then specify the related IPv4 addresses. Auto(DHCP): The Harvester network controller retrieves the CIDR and gateway addresses from the DHCP server. You can specify the DHCP server address. Manual: Specify the CIDR and gateway addresses. important Harvester uses the information to verify that all nodes can access the VM network you are creating. If that is the case, the Network connectivity column on the VM Networks screen indicates that the network is active. Otherwise, the screen indicates that an error has occurred. Create a VM with VLAN Network​ You can now create a new VM using the VLAN network configured above: Click the Create button on the Virtual Machines page.Specify the required parameters and click the Networks tab.Either configure the default network to be a VLAN network or select an additional network to add. Untagged Network​ As is known, the traffic under a VLAN network has a VLAN ID tag and we can use the VLAN network with PVID (default 1) to communicate with any normal untagged traffic. However, some network devices may not expect to receive an explicitly tagged VLAN ID that matches the native VLAN on the switch the uplink belongs to. That's the reason why we provide the untagged network. How to use untagged network​ The usage of untagged network is similar to the VLAN network. To create a new untagged network, go to the Networks &gt; Networks page and click the Create button. You have to specify the name, select the type Untagged Network and choose the cluster network. note Starting with Harvester v1.1.2, Harvester supports updating and deleting VM networks. Make sure to stop all affected VMs before updating or deleting VM networks.","keywords":"Harvester Network","version":"v1.1"},{"title":"Cluster Network","type":0,"sectionRef":"#","url":"/v1.1/networking/index","content":"Cluster Network Concepts​ Cluster Network​ Available as of v1.1.0 In Harvester v1.1.0, we introduced a new concept called cluster network for traffic isolation. The following diagram describes a typical network architecture that separates data-center (DC) traffic from out-of-band (OOB) traffic. We abstract the sum of devices, links, and configurations on a traffic-isolated forwarding path on Harvester as a cluster network. In the above case, there will be two cluster networks corresponding to two traffic-isolated forwarding paths. Network Configuration​ Specifications including network devices of the Harvester hosts can be different. To be compatible with such a heterogeneous cluster, we designed the network configuration. Network configuration only works under a certain cluster network. Each network configuration corresponds to a set of hosts with uniform network specifications. Therefore, multiple network configurations are required for a cluster network on non-uniform hosts. Network​ A network is an interface in a virtual machine that connects to the host network. As with network configuration, every network except the built-in management network must be under a cluster network. Harvester supports adding multiple networks to one VM. If a network's cluster network is not enabled on some hosts, the VM that owns this network will not be scheduled to those hosts. Please refer to network part for more details about networks. Relationship Between Cluster Network, Network Config, VM Network​ The following diagram shows the relationship between a cluster network, a network config, and a VM network. All Network Configs and VM Networks are grouped under a cluster network. A label can be assigned to each host to categorize hosts based on their network specifications. A network config can be added for each group of hosts using a node selector. For example, in the diagram above, the hosts in ClusterNetwork-A are divided into three groups as follows: The first group includes host0, which corresponds to network-config-A.The second group includes host1 and host2, which correspond to network-config-B.The third group includes the remaining hosts (host3, host4, and host5), which do not have any related network config and therefore do not belong to ClusterNetwork-A. The cluster network is only effective on hosts that are covered by the network configuration. A VM using a VM network under a specific cluster network can only be scheduled on a host where the cluster network is active. In the diagram above, we can see that: ClusterNetwork-A is active on host0, host1, and host2. VM0 uses VM-network-A, so it can be scheduled on any of these hosts.VM1 uses both VM-network-B and VM-network-C, so it can only be scheduled on host2 where both ClusterNetwork-A and ClusterNetwork-B are active.VM0, VM1, and VM2 cannot run on host3 where the two cluster networks are inactive. Overall, this diagram provides a clear visualization of the relationship between cluster networks, network configurations, and VM networks, as well as how they impact VM scheduling on hosts. Cluster Network Details​ Built-in Cluster Network​ Harvester provides a built-in cluster network called mgmt. It's different from the custom cluster network. The mgmt cluster network: Cannot be deleted.Does not need any network configuration.Is enabled on all hosts and cannot be disabled.Shares the same traffic egress with the management network. If there is no need for traffic separation, you can put all your network under the mgmt cluster network. Custom Cluster Network​ You are allowed to add the custom cluster network, which will not be available until it's enabled on some hosts by adding a network configuration. How to create a new cluster network​ To create a cluster network, go to the Networks &gt; ClusterNetworks/Configs page and click the Create button. You only need to specify the name. Click the Create Network Config button on the right of the cluster network to create a new network configuration. In the Node Selector tab, specify the name and choose one of the three methods to select nodes where the network configuration will apply. If you want to cover the unselected nodes, you can create another network configuration. Click the Uplink tab to add the NICs, and configure the bond options and link attributes. The bond mode defaults to active-backup. note The NICs drop-down list shows all the common NICs on all the selected nodes. The drop-down list will change as you select different nodes.The text enp7s3 (1/3 Down) in the NICs drop-down list indicates that the enp7s3 NIC is down in one of the three selected nodes. In this case, you need to find the NIC, set it up, and refresh this page. After this, it should be selectable. note Starting with Harvester v1.1.2, Harvester supports updating network configs. Make sure to stop all affected VMs before updating network configs.","keywords":"Harvester Networking ClusterNetwork NetworkConfig Network","version":"v1.1"},{"title":"Harvester CSI Driver","type":0,"sectionRef":"#","url":"/v1.1/rancher/csi-driver","content":"Harvester CSI Driver The Harvester Container Storage Interface (CSI) Driver provides a standard CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines that run the guest Kubernetes nodes are all in the same Harvester namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hope to remove this limitation in a future release of Harvester. note Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only(ROX) and read-write(RWX) support. Deploying with Harvester RKE1 Node Driver​ Select Harvester(Out-of-tree) option (optional. If you don't need to use the Cloud Provider feature at the same time, you can select the None option). Install Harvester CSI Driver from the Rancher marketplace. Deploying with Harvester RKE2 Node Driver​ When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected. Install CSI Driver Manually in the RKE2 Cluster​ If you prefer to deploy the Harvester CSI driver without enabling the Harvester cloud provider, you can choose either Default - RKE2 Embedded or External in the Cloud Provider field. If you are using Rancher v2.6, select None instead. Prerequisites​ Ensure that you have the following prerequisites in place: You have kubectl and jq installed on your system.You have the kubeconfig file for your bare-metal Harvester cluster. export KUBECONFIG=/path/to/your/harvester-kubeconfig Perform the following steps to deploy the Harvester CSI Driver manually: Deploy Harvester CSI Driver​ Generate cloud-config. You can generate the kubeconfig file using the generate_addon_csi.sh script. It is available on the harvester/harvester-csi-driver repo. You can follow the steps below to get the cloud-config and cloud-init data: The &lt;serviceaccount name&gt; usually corresponds to your guest cluster name (the value of &quot;Cluster Name&quot; in the figure below), and &lt;namespace&gt; should match the namespace (the value of &quot;Namespace&quot;) of the guest cluster. # ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; RKE2 ########## cloud-config ############ apiVersion: v1 clusters: - cluster: &lt;token&gt; server: https://&lt;YOUR HOST HARVESTER VIP&gt;:6443 name: default contexts: - context: cluster: default namespace: default user: rke2-guest-01-default-default name: rke2-guest-01-default-default current-context: rke2-guest-01-default-default kind: Config preferences: {} users: - name: rke2-guest-01-default-default user: token: &lt;token&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: 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 owner: root:root path: /var/lib/rancher/rke2/etc/config-files/cloud-provider-config permissions: '0644' Copy and paste the output below cloud-init user data to Machine Pools &gt;Show Advanced &gt; User Data. Set up cloud-provider-config. The cloud-provider-config should be created after you apply the above cloud-init user data. You can check again on the path /var/lib/rancher/rke2/etc/config-files/cloud-provider-config. note If you want to change the cloud-provider-config path, you should update the cloud-init user data. Install Harvester CSI Driver. Install the Harvester CSI Driver chart from the Rancher marketplace. (Note: by default, you do not need to change the cloud-config path). note If you prefer not to install the Harvester CSI driver using Rancher (Apps &gt; Charts), you can use Helm instead. The Harvester CSI driver is packaged as a Helm chart. For more information, see https://charts.harvesterhci.io. By following the above steps, you should be able to see those CSI driver pods are up and running, and you can verify it by provisioning a new PVC using the default storageClass harvester.. Deploying with Harvester K3s Node Driver​ You can follow the Deploy Harvester CSI Driver steps described in the RKE2 section for Prerequisites The only difference is that you need to change the script command as follows: # ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; k3s Passthrough Custom StorageClass​ Starting with Harvester CSI driver v0.1.15, you can create a PersistentVolumeClaim (PVC) based on a different StorageClass. Harvester CSI driver v0.1.15 is supported out of the box starting with the following RKE2 versions; for RKE1 you need to manually install the CSI driver chart: v1.23.16+rke2r1 and laterv1.24.10+rke2r1 and laterv1.25.6+rke2r1 and laterv1.26.1+rke2r1 and laterv1.27.1+rke2r1 and later Prerequisites​ Please add the following additional perquisites to your Harvester cluster. The Harvester CSI driver requires proper RBAC to display error messages. This is important for displaying an error message when creating a PVC with a StorageClass that does not exist, as shown in the following figure. Perform the following steps to setup RBAC to enable seeing error messages. Create a new clusterrole named harvesterhci.io:csi-driver with the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: apiserver app.kubernetes.io/name: harvester app.kubernetes.io/part-of: harvester name: harvesterhci.io:csi-driver rules: - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - get - list - watch Then create the clusterrolebinding to associate with the new clusterrole with the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: &lt;namespace&gt;-&lt;serviceaccount name&gt; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: harvesterhci.io:csi-driver subjects: - kind: ServiceAccount name: &lt;serviceaccount name&gt; namespace: &lt;namespace&gt; Make sure the serviceaccount name and namespace are the same as your cloud provider. Perform the following steps to see the serviceaccount name and namespace for your cloud provider. Find the rolebinding for your cloud provider . # kubectl get rolebinding -A |grep harvesterhci.io:cloudprovider default default-rke2-guest-01 ClusterRole/harvesterhci.io:cloudprovider 7d1h Get the subjects info on this rolebinding. kubectl get rolebinding default-rke2-guest-01 -n default -o yaml |yq -e '.subjects' Find the ServiceAccount info as follows: - kind: ServiceAccount name: rke2-guest-01 namespace: default Deploying​ Create a new StorageClass that you would like to use in your guest k8s cluster. You can refer to the StorageClasses for more details. As show in the following figure, create a new StorageClass named replica-2. For example, create a new StorageClass on the downstream cluster as follows associated with the StorageClass created on the Harvester Cluster named replica-2. note When choosing a Provisioner, select Harvester (CSI). The parameter Host StorageClass should be the StorageClass created on the Harvester Cluster. You can now create a PVC based on this new StorageClass, which uses the Host StorageClass to provision volumes on the bare-metal cluster.","keywords":"Harvester harvester Rancher Integration","version":"v1.1"},{"title":"Harvester Configuration","type":0,"sectionRef":"#","url":"/v1.1/install/harvester-configuration","content":"Harvester Configuration Configuration Example​ Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: scheme_version: 1 server_url: https://cluster-VIP:443 token: TOKEN_VALUE os: ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files: - encoding: &quot;&quot; content: test content owner: root path: /etc/test.txt permissions: '0755' hostname: myhost modules: - kvm - nvme sysctls: kernel.printk: &quot;4 4 1 7&quot; kernel.kptr_restrict: &quot;1&quot; dns_nameservers: - 8.8.8.8 - 1.1.1.1 ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password: rancher environment: http_proxy: http://myserver https_proxy: http://myserver labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7C&quot; method: dhcp force_efi: true device: /dev/sda data_disk: /dev/sdb silent: true iso_url: http://myserver/test.iso poweroff: true no_format: true debug: true tty: ttyS0 vip: 10.10.0.19 vip_hw_addr: 52:54:00:ec:0e:0b vip_mode: dhcp force_mbr: false system_settings: auto-disk-provision-paths: &quot;&quot; Configuration Reference​ Below is a reference of all configuration keys. caution Security Risks: The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. note Configuration Priority: When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname. scheme_version​ Definition​ The version of scheme reserved for future configuration migration. This configuration is mandatory for migrating the configuration to a new scheme version. It tells Harvester the previous version and the need to migrate. note This field didn't take any effect in the current Harvester version. caution Make sure that your custom configuration always has the correct scheme version. server_url​ Definition​ server_url is the URL of the Harvester cluster, which is used for the new node to join the cluster. This configuration is mandatory when the installation is in JOIN mode. The default format of server_url is https://cluster-VIP:443. note To ensure a high availability (HA) Harvester cluster, please use either the Harvester cluster VIP or a domain name in server_url. Example​ server_url: https://cluster-VIP:443 install: mode: join token​ Definition​ The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example​ token: myclustersecret Or a node token token: &quot;K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4&quot; os.ssh_authorized_keys​ Definition​ A list of SSH authorized keys that should be added to the default user, rancher. SSH keys can be obtained from GitHub user accounts by using the formatgithub:${USERNAME}. This is done by downloading the keys from https://github.com/${USERNAME}.keys. Example​ os: ssh_authorized_keys: - &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D&quot; - &quot;github:ibuildthecloud&quot; os.write_files​ A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: &quot;&quot;: content data are written in plain text. In this case, the encoding field can be also omitted.b64, base64: content data are base64-encoded.gz, gzip: content data are gzip-compressed.gz+base64, gzip+base64, gz+b64, gzip+b64: content data are gzip-compressed first and then base64-encoded. Example os: write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner: root:root path: /etc/connman/main.conf permissions: '0644' - content: | # My new /etc/sysconfig/samba file SMDBOPTIONS=&quot;-D&quot; path: /etc/sysconfig/samba - content: !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path: /bin/arch permissions: '0555' - content: | 15 * * * * root ship_logs path: /etc/crontab os.hostname​ Definition​ Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value. Example​ os: hostname: myhostname os.modules​ Definition​ A list of kernel modules to be loaded on start. Example​ os: modules: - kvm - nvme os.sysctls​ Definition​ Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf. Values must be specified as strings. Example​ os: sysctls: kernel.printk: 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict: &quot;1&quot; # force the YAML parser to read as a string os.dns_nameservers​ Definition​ Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example​ os: dns_nameservers: - 8.8.8.8 - 1.1.1.1 os.ntp_servers​ Definition​ Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines. Example​ os: ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org os.password​ Definition​ The password for the default user, rancher. By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from/etc/shadow. You can also encrypt a password using OpenSSL. For the encryption algorithms supported by Harvester, please refer to the table below. Algorithm\tCommand\tSupportSHA-512\topenssl passwd -6\tYes SHA-256\topenssl passwd -5\tYes (started from v1.1.2) MD5\topenssl passwd -1\tYes (started from v1.1.2) MD5, Apache variant\topenssl passwd -apr1\tYes (started from v1.1.2) AIX-MD5\topenssl passwd -aixmd5\tNo info Only SHA-512 is supported before v1.1.2 release. Example​ Encrypted: os: password: &quot;$6$kZYUnRaTxNdg4W8H$WSEJydGWsNpaRbbbRdTDLJ2hDLbkizxSFGW2RtexlqG6njEATaGQG9ssztjaKDCsaNUPBZ1E1YdsvSLMAi/IO/&quot; Or clear text: os: password: supersecure os.environment​ Definition​ Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example​ os: environment: http_proxy: http://myserver https_proxy: http://myserver note This example sets the HTTP(S) proxy for foundational OS components. To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy. os.labels​ Definition​ Labels to be added to this Node. Example​ os: labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue install.mode​ Definition​ Harvester installation mode: create: Creating a new Harvester installation.join: Join an existing Harvester installation. Need to specify server_url. Example​ install: mode: create install.management_interface​ Definition​ Configure network interfaces for the host machine. Valid configuration fields are: method: Method to assign an IP to this network. The following are supported: static: Manually assign an IP and gateway.dhcp: Request an IP from the DHCP server. ip: Static IP for this network. Required if static method is chosen.subnet_mask: Subnet mask for this network. Required if static method is chosen.gateway: Gateway for this network. Required if static method is chosen.interfaces: An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name: The name of the slave interface for the bonded network.interfaces.hwAddr: The hardware MAC address of the interface. bond_options: Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlbmiimon: 100 mtu: The MTU for the interface.vlan_id: The VLAN ID for the interface. note Harvester uses the systemd net naming scheme. Please make sure the interface name is present on the target machine before installation. Example​ install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7D&quot; # The hwAddr is optional method: dhcp bond_options: mode: balance-tlb miimon: 100 mtu: 1492 vlan_id: 101 install.force_efi​ Force EFI installation even when EFI is not detected. Default: false. install.device​ The device to install the OS. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. install.silent​ Reserved. install.iso_url​ ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff​ Shutdown the machine after installation instead of rebooting install.no_format​ Do not partition and format, assume layout exists already. install.debug​ Run the installation with additional logging and debugging enabled for the installed system. install.tty​ Definition​ The tty device used for the console. Example​ install: tty: ttyS0,115200n8 install.vip​ install.vip_mode​ install.vip_hw_addr​ Definition​ install.vip: The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://&lt;VIP&gt;.install.vip_mode dhcp: Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided.static: Harvester uses a static VIP. install.vip_hw_addr: The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp. See Management Address for more information. Example​ Configure a static VIP. install: vip: 192.168.0.100 vip_mode: static Configure a DHCP VIP. install: vip: 10.10.0.19 vip_mode: dhcp vip_hw_addr: 52:54:00:ec:0e:0b install.force_mbr​ Definition​ By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. note Harvester creates an additional partition for storing VM data ifinstall.data_disk is configured to use the same storage device as the one set for install.device. When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example​ install: force_mbr: true install.data_disk​ Available as of v1.0.1 Definition​ Sets the default storage device to store the VM data. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. Default: Same storage device as the one set for install.device Example​ install: data_disk: /dev/sdb install.harvester.storage_class.replica_count​ Available as of v1.1.2 Definition​ Sets the replica count of Harvester's default storage class harvester-longhorn. Default: 3 Supported values: 1, 2, 3. All other values are considered 3. In edge scenarios where users may deploy single-node Harvester clusters, they can set this value to 1. In most scenarios, it is recommended to keep the default value 3 for storage high availability. Please refer to longhorn-replica-count for more details. Example​ install: harvester: storage_class: replica_count: 1 system_settings​ Definition​ You can overwrite the default Harvester system settings by configuring system_settings. See the Settings page for additional information and the list of all the options. note Overwriting system settings only works when Harvester is installed in &quot;create&quot; mode. If you install Harvester in &quot;join&quot; mode, this setting is ignored. Installing in &quot;join&quot; mode will adopt the system settings from the existing Harvester system. Example​ The example below overwrites containerd-registry, http-proxy and ui-source settings. The values must be a string. system_settings: containerd-registry: '{&quot;Mirrors&quot;: {&quot;docker.io&quot;: {&quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;]}}, &quot;Configs&quot;: {&quot;myregistry.local:5000&quot;: {&quot;Auth&quot;: {&quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot;}, &quot;TLS&quot;: {&quot;InsecureSkipVerify&quot;: false}}}}' http-proxy: '{&quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc&quot;}' ui-source: auto ","keywords":"Harvester harvester Rancher rancher Harvester Configuration","version":"v1.1"},{"title":"Creating an K3s Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.1/rancher/node/k3s-cluster","content":"Creating an K3s Kubernetes Cluster You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ using the built-in Harvester node driver. note Harvester K3s node driver is in Tech Preview.VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create K3s Kubernetes Cluster​ You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. Add Node Affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules. Using Harvester K3s Node Driver in Air Gapped Environment​ K3s provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with required packages installed. Option 2. Configure the Show Advanced &gt; User Data to enable the VMs to install required packages via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.1"},{"title":"Harvester Node Driver","type":0,"sectionRef":"#","url":"/v1.1/rancher/node/node-driver","content":"Harvester Node Driver The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver, and the project repo is available at harvester/docker-machine-driver-harvester. You can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.3+ with the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as Harvester cluster storage passthrough support to the guest Kubernetes cluster. While you can upload and view .ISO images in the Harvester UI, the same capability is not available in the Rancher UI. For more information on this, see the Rancher docs. Harvester Node Driver​ The Harvester node driver is enabled by default from Rancher v2.6.3. You can go to Cluster Management &gt; Drivers &gt; Node Drivers page to manage the Harvester node driver manually. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. Support Matrix​ Refer to Rancher Downstream Cluster Support Matrix Known Issues​ Summary\tStatus\tLast UpdatedVolumes created by the Harvester CSI driver in the host Harvester cluster would be deleted after editing/deleting the guest cluster\tResolved\t2023-05-08 Volumes created by the Harvester CSI driver in the host Harvester cluster would be deleted after editing/deleting the guest cluster​ Status\tLast updatedResolved(Rancher &gt;=v2.7.2)\t2023-05-08 Workaround: You can temporarily change the Harvester node driver version to v0.6.3 from the Rancher UI. Go to the Rancher UI and click Cluster Management &gt; Drivers &gt; Node Drivers. In the Node Drivers list, find Harvester and then click ⋮ &gt; View in API.Click Edit.Uncheck the builtin checkbox.Change the *url to https://releases.rancher.com/harvester-node-driver/v0.6.3/docker-machine-driver-harvester-amd64.tar.gz.Change the checksum to 159516f8f438e9b1726418ec8608625384aba1857bc89dff4a6ff16b31357c28.Click Show Request &gt; Send Request.Click Reload until the value of status.appliedChecksum and status.appliedURL change to the value we set. caution Changes to the node driver cannot be persisted. In other words, the changes will be lost after you restart the Rancher container. caution To use this workaround, you need to ensure that the connection to the url is stable. If your environment is an air-gapped environment, you need to download the file and host it on the Intranet. caution Starting with v0.6.3, the Harvester node driver has removed the qemu-guest-agent auto-injection from the backend. If the image you are using does not contain the qemu-guest-agent package, you can use userdata config to install and boot qemu-guest-agent. Otherwise, the cluster will not be provisioned successfully. #cloud-config package_update: true packages: - qemu-guest-agent runcmd: - - systemctl - enable - '--now' - qemu-guest-agent.service Resolution: Rancher v2.7.2 has been released with the fixed node driver version v0.6.3 for this issue. And Rancher v2.7.2 UI will do the qemu-guest-agent auto-injection. Affected versions: Rancher: v2.6.x,v2.7.0,v2.7.1 RKE1 Kubernetes Cluster​ Click to learn how to create RKE1 Kubernetes Clusters. RKE2 Kubernetes Cluster​ Click to learn how to create RKE2 Kubernetes Clusters. K3s Kubernetes Cluster​ Click to learn how to create k3s Kubernetes Clusters. Topology Spread Constraints​ Available as of v1.0.3 In your guest Kubernetes cluster, you can use topology spread constraints to control how workloads are spread across the Harvester VMs among failure-domains such as regions and zones. This can help to achieve high availability as well as efficient resource utilization of your cluster resources. The minimum RKE2 versions required to support the sync topology label feature are as follows: Supported RKE2 Version&gt;= v1.24.3+rke2r1 &gt;= v1.23.9+rke2r1 &gt;= v1.22.12+rke2r1 In addition, the cloud provider version installed via the Apps of RKE/K3s must be &gt;= v0.1.4 Sync Topology Labels to the Guest Cluster Node​ During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone topology labels are supported. note Label synchronization will only take effect during guest node initialization. To avoid node drifts to another region or zone, it is recommended to add the node affinity rules during the cluster provisioning, so that the VMs can be scheduled to the same zone even after rebuilding. Configuring topology labels on the Harvester nodes through Hosts &gt; Edit Config &gt; Labels. e.g., add the topology labels as follows: topology.kubernetes.io/region: us-east-1 topology.kubernetes.io/zone: us-east-1a Creating a guest Kubernetes cluster using the Harvester node driver and it is recommended to add the node affinity rules, this will help to avoid node drifting to other zones after VM rebuilding. After the cluster is successfully deployed, confirm that guest Kubernetes node labels are successfully synchronized from the Harvester VM node. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints.","keywords":"Harvester harvester Rancher rancher Harvester Node Driver","version":"v1.1"},{"title":"Creating an RKE1 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.1/rancher/node/rke1-cluster","content":"Creating an RKE1 Kubernetes Cluster You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.3+ with the built-in Harvester node driver. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials, you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster.In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential name.Select &quot;Imported Harvester Cluster&quot;.Click Create. Create Node Template​ You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials.Configure Instance Options: Configure the CPU, memory, and diskSelect an OS image that is compatible with the cloud-init config.Select a network that the node driver is able to connect to; currently, only VLAN is supported.Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu. (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs:Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Add Node Affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration &gt; Node Templates: Check the Advanced Options tab and click Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Create RKE1 Kubernetes Cluster​ Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE1.Select Harvester node driver.Enter Cluster Name (required).Enter Name Prefix (required).Enter Template (required).Select etcd and Control Plane (required).On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver.Click Create. Using Harvester RKE1 Node Driver in Air Gapped Environment​ RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with qemu-guest-agent and docker installed. Option 2. Configure the cloud init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=&quot;http://192.168.0.1:3128&quot; HTTPS_PROXY=&quot;http://192.168.0.1:3128&quot; append: true ","keywords":"","version":"v1.1"},{"title":"Creating an RKE2 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.1/rancher/node/rke2-cluster","content":"Creating an RKE2 Kubernetes Cluster Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images. Create Your Cloud Credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create RKE2 Kubernetes Cluster​ Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).(optional) Configure the Show Advanced &gt; User Data to install the required packages of VM. #cloud-config packages: - iptables note Calico and Canal require the iptables or xtables-nft package to be installed on the node, for more details, please refer to the RKE2 known issues. Click Create. note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration.Currently only imported Harvester clusters are supported automatically. Add Node Affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Using Harvester RKE2 Node Driver in Air Gapped Environment​ RKE2 provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. Calico and Canal require the iptables or xtables-nft package to be installed on the node. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image with required packages installed. Option 2. Configure the Show Advanced &gt; User Data to enable the VMs to install required packages via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.1"},{"title":"Harvester Cloud Provider","type":0,"sectionRef":"#","url":"/v1.1/rancher/cloud-provider","content":"Harvester Cloud Provider RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and Harvester cluster storage passthrough support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2.How to use the Harvester load balancer. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying to the RKE1 Cluster with Harvester Node Driver​ When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Deploying to the RKE2 Cluster with Harvester Node Driver​ When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Deploying to the K3s Cluster with Harvester Node Driver [Experimental]​ When spinning up a K3s cluster using the Harvester node driver, you can perform the following steps to deploy the harvester cloud provider: Generate and inject cloud config for harvester-cloud-provider The cloud provider needs a kubeconfig file to work, a limited scoped one can be generated using the generate_addon.sh script available in the harvester/cloud-provider-harvester repo. note The script depends on kubectl and jq to operate the Harvester cluster The script needs access to the Harvester Cluster kubeconfig to work. The namespace needs to be the namespace in which the guest cluster will be created. ./deploy/generate_addon.sh &lt;serviceaccount name&gt; &lt;namespace&gt; The output will look as follows: # ./deploy/generate_addon.sh harvester-cloud-provider default Creating target directory to hold files in ./tmp/kube...done Creating a service account in default namespace: harvester-cloud-provider W1104 16:10:21.234417 4319 helpers.go:663] --dry-run is deprecated and can be replaced with --dry-run=client. serviceaccount/harvester-cloud-provider configured Creating a role in default namespace: harvester-cloud-provider role.rbac.authorization.k8s.io/harvester-cloud-provider unchanged Creating a rolebinding in default namespace: harvester-cloud-provider W1104 16:10:21.986771 4369 helpers.go:663] --dry-run is deprecated and can be replaced with --dry-run=client. rolebinding.rbac.authorization.k8s.io/harvester-cloud-provider configured Getting uid of service account harvester-cloud-provider on default Service Account uid: ea951643-53d2-4ea8-a4aa-e1e72a9edc91 Creating a user token secret in default namespace: harvester-cloud-provider-token Secret name: harvester-cloud-provider-token Extracting ca.crt from secret...done Getting user token from secret...done Setting current context to: local Cluster name: local Endpoint: https://HARVESTER_ENDPOINT/k8s/clusters/local Preparing k8s-harvester-cloud-provider-default-conf Setting a cluster entry in kubeconfig...Cluster &quot;local&quot; set. Setting token credentials entry in kubeconfig...User &quot;harvester-cloud-provider-default-local&quot; set. Setting a context entry in kubeconfig...Context &quot;harvester-cloud-provider-default-local&quot; created. Setting the current-context in the kubeconfig file...Switched to context &quot;harvester-cloud-provider-default-local&quot;. ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: &lt;CACERT&gt; server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: &lt;TOKEN&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: &lt;CONTENT&gt; owner: root:root path: /etc/kubernetes/cloud-config permissions: '0644' Copy and paste the output below cloud-init user data to Machine Pools &gt;Show Advanced &gt; User Data. Add the following HelmChart yaml of harvester-cloud-provider to Cluster Configuration &gt; Add-On Config &gt; Additional Manifest apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.1.13 helmVersion: v3 Disable the in-tree cloud provider by Click the Edit as YAML button Disable servicelb and Set disable-cloud-controller: true to disable default k3s cloud controller. machineGlobalConfig: disable: - servicelb disable-cloud-controller: true Add cloud-provider=external to use harvester cloud provider. machineSelectorConfig: - config: kubelet-arg: - cloud-provider=external protect-kernel-defaults: false With these settings in place a K3s cluster should provision successfully while using the external cloud provider. Upgrade Cloud Provider​ Upgrade RKE2​ The cloud provider can be upgraded by upgrading the RKE2 version. You can upgrade the RKE2 cluster via the Rancher UI as follows: Click ☰ &gt; Cluster Management.Find the guest cluster that you want to upgrade and select ⋮ &gt; Edit Config.Select Kubernetes Version.Click Save. Upgrade RKE/K3s​ RKE/K3s upgrade cloud provider via the Rancher UI, as follows: Click ☰ &gt; RKE/K3s Cluster &gt; Apps &gt; Installed Apps.Find the cloud provider chart and select ⋮ &gt; Edit/Upgrade.Select Version. Click Next &gt; Update. Load Balancer Support​ After deploying the Harvester Cloud provider, you can use the Kubernetes LoadBalancer service to expose a microservice inside the guest cluster to the external world. When you create a Kubernetes LoadBalancer service, a Harvester load balancer is assigned to the service and you can edit it through the Add-on Config in the Rancher UI. IPAM​ Harvester's built-in load balancer supports both pool and dhcp modes. You can select the mode in the Rancher UI. Harvester adds the annotation cloudprovider.harvesterhci.io/ipam to the service behind. pool: You should configure an IP address pool in Harvester's Settings in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server. note It is not allowed to modify the IPAM mode. You need to create a new service if you want to modify the IPAM mode. Health Checks​ The Harvester load balancer supports TCP health checks. You can specify the parameters in the Rancher UI if you enable the Health Check option. Alternatively, you can specify the parameters by adding annotations to the service manually. The following annotations are supported: Annotation Key\tValue Type\tRequired\tDescriptioncloudprovider.harvesterhci.io/healthcheck-port\tstring\ttrue\tSpecifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold\tstring\tfalse\tSpecifies the health check success threshold. The default value is 1. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-failure-threshold\tstring\tfalse\tSpecifies the health check failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds\tstring\tfalse\tSpecifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds\tstring\tfalse\tSpecifies the timeout of every health check. The default value is 3 seconds.","keywords":"Harvester harvester RKE rke RKE2 rke2 Harvester Cloud Provider","version":"v1.1"},{"title":"Rancher Integration","type":0,"sectionRef":"#","url":"/v1.1/rancher/rancher-integration","content":"Rancher Integration Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Starting with Rancher v2.6.1, Rancher has integrated Harvester by default to centrally manage VMs and containers. Rancher &amp; Harvester Support Matrix​ For the support matrix, please see Harvester &amp; Rancher Support Matrix. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support. Deploying Rancher Server​ To use Rancher with Harvester, please install the Rancher and Harvester in two separated servers. If you want to try out the integration features, you can create a VM in Harvester and install Rancher v2.6.3 or above (the latest stable version is recommended). Use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix Metal caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM)An on-premises VMA bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection.From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart=unless-stopped -v /opt/rancher:/var/lib/rancher -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.9 Virtualization Management​ With Rancher's Virtualization Management feature, you can now import and manage Harvester clusters. By clicking on one of the clusters, you are able to view and manage the imported Harvester cluster resources like Hosts, VMs, images, volumes, etc. Additionally, the Virtualization Management leverages existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please check the virtualization management page. Creating Kubernetes Clusters using the Harvester Node Driver​ Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage guest Kubernetes clusters. Starting with Rancher v2.6.1, the Harvester node driver has been added by default. Users can reference the node-driver page for more details.","keywords":"Harvester harvester Rancher rancher Rancher Integration","version":"v1.1"},{"title":"Virtualization Management","type":0,"sectionRef":"#","url":"/v1.1/rancher/virtualization-management","content":"Virtualization Management For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6 and above. As a prerequisite, Harvester v1.1.0 integration requires Rancher server v2.6.9 or above. In production, use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix Metal caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged and your cluster may not be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. To install Rancher with Docker: Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM)An on-premises VMA bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection.From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart=unless-stopped -v /opt/rancher:/var/lib/rancher -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.9 Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create. You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy​ In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication, users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions: Define user authorization outside the scope of any particular cluster. Cluster and Project Roles: Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings.A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example​ The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users &amp; Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project.A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab.Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save.Open an incognito browser and log in as project-owner.After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned.Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed.Create a VM with one of the images that you have uploaded.Log in with another user, e.g., project-readonly, and this user will only have the read permission of this project. Delete Imported Harvester Cluster​ Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management &gt; Harvester Clusters. Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. caution Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","keywords":"Harvester Rancher","version":"v1.1"},{"title":"Harvester Terraform Provider","type":0,"sectionRef":"#","url":"/v1.1/terraform/terraform-provider","content":"Harvester Terraform Provider Support Matrix​ Harvester Version\tSupported Terraform Provider Harvester\tSupported Terraformer Harvesterv1.1.2\tv0.6.3\tv1.1.1-harvester v1.1.1\tv0.6.3\tv1.1.1-harvester v1.1.0\tv0.6.3\tv1.1.1-harvester v1.0.3\tv0.5.4\tv1.0.1-harvester Requirements​ Terraform &gt;= 0.13.xGo 1.18 to build the provider plugin Install The Provider​ copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = &quot;harvester/harvester&quot; version = &quot;&lt;replace to the latest release version&gt;&quot; } } } provider &quot;harvester&quot; { # Configuration options } Using the provider​ More details about the provider-specific configurations can be found in the docs. Github Repo: https://github.com/harvester/terraform-provider-harvester","keywords":"","version":"v1.1"},{"title":"Harvester","type":0,"sectionRef":"#","url":"/v1.1/troubleshooting/harvester","content":"Harvester Fail to Deploy a Multi-node Cluster Due to Incorrect HTTP Proxy Setting​ ISO Installation Without a Harvester Configuration File​ Configure HTTP Proxy During Harvester Installation​ In some environments, you configure http-proxy of OS Environment during Harvester installation. Configure HTTP Proxy After First Node is Ready​ After the first node is installed successfully, you login into the Harvester GUI to configure http-proxy of Harvester System Settings. Then you continue to add more nodes to the cluster. One Node Becomes Unavailable​ The issue you may encounter: The first node is installed successfully. The second node is installed successfully. The third node is installed successfully. Then the second node changes to Unavialable state and cannot recover automatically. Solution​ When the nodes in the cluster do not use the HTTP Proxy to communicate with each other, after the first node is installed successfully, you need to configure http-proxy.noProxy against the CIDR used by those nodes. For example, your cluster assigns IPs from CIDR 172.26.50.128/27 to nodes via DHCP/static setting, please add this CIDR to noProxy. After setting this, you can continue to add new nodes to the cluster. For more details, please refer to Harvester issue 3091. ISO Installation With a Harvester Configuration File​ When a Harvester configuration file is used in ISO installation, please configure proper http-proxy in Harvester System Settings. PXE Boot Installation​ When PXE Boot Installation is adopted, please configure proper http-proxy in OS Environment and Harvester System Settings. Generate a Support Bundle​ Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Access Embedded Rancher and Longhorn Dashboards​ Available as of v1.1.0 You can now access the embedded Rancher and Longhorn dashboards directly on the Support page, but you must first go to the Preferences page and check the Enable Extension developer features box under Advanced Features (in Harvester v1.1.0 and v1.1.1, the option name is Developer Tools &amp; Features). For previous versions, you can access them manually through: https://{{HARVESTER_IP}}/dashboard/c/local/explorer (Embedded Rancher)https://{{HARVESTER_IP}}/dashboard/c/local/longhorn (Embedded Longhorn) note We only support using the embedded Rancher and Longhorn dashboards for debugging and validation purposes. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here. I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers​ If you changedSSL/TLS enabled protocols and ciphers settingsand you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl: # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{&quot;protocols&quot;:&quot;TLS99&quot;,&quot;ciphers&quot;:&quot;WRONG_CIPHER&quot;}' # &lt;- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly.","keywords":"","version":"v1.1"},{"title":"Installation","type":0,"sectionRef":"#","url":"/v1.1/troubleshooting/index","content":"Installation The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS)​ Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancherPassword: rancher Meeting hardware requirements​ Check that your hardware meets the minimum requirements to complete installation. Receiving the message &quot;Loading images. This may take a few minutes...&quot;​ Because the system doesn't have a default route, your installer may become &quot;stuck&quot; in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev mgmt-br proto dhcp &lt;-- Does a default route exist? 10.10.0.0/24 dev mgmt-br proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. For more information, see DHCP Server Configuration. Modifying cluster token on agent nodes​ When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg=&quot;Bootstrapping Rancher (master-head/v1.21.5+rke2r1)&quot; msg=&quot;failed to bootstrap system, will retry: generating plan: insecure cacerts download from https://192.168.122.115:443/cacerts: Get \\&quot;https://192.168.122.115:443/cacerts\\&quot;: EOF&quot; To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml. For example, if the cluster token setup in the server node is ThisIsTheCorrectOne, you will update the token value as follow: token: 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/99_custom.yaml: name: Harvester Configuration stages: ... initramfs: - commands: - rm -f /etc/sysconfig/network/ifroute-mgmt-br files: - path: /etc/rancher/rancherd/config.yaml permissions: 384 owner: 0 group: 0 content: | role: cluster-init token: 'ThisIsTheCorrectOne' # &lt;- Update this value kubernetesVersion: v1.21.5+rke2r1 labels: - harvesterhci.io/managed=true encoding: &quot;&quot; ownerstring: &quot;&quot; note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml. For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting troubleshooting information​ Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file. Before v1.0.2 Please help capture the content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* And output of these commands: blkid dmesg ","keywords":"","version":"v1.1"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/v1.1/troubleshooting/os","content":"Operating System Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit. The following sections contain information and tips to help users troubleshoot OS-related issues. How to log into a Harvester node​ Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~&gt; sudo blkid # Or become root rancher@node1:~&gt; sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only?​ The OS file system, like a container image, is image-based and immutable except in some directories. We recommend using a toolbox container to run programs not packaged in the Harvester OS for debugging purposes. Please see this article to learn how to build and run a toolbox container. The Harvester OS also provides a way to enable the read-write mode temporarily. Please follow the following steps: caution Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0, we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat &gt; /oem/91_hack.yaml &lt;&lt;'EOF' name: &quot;Rootfs Layout Settings for debugrw&quot; stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline &amp;&amp; grep -q rd.cos.debugrw /proc/cmdline' name: &quot;Layout configuration for debugrw&quot; environment_file: /run/cos/cos-layout.env environment: RW_PATHS: &quot; &quot; EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters​ note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry &quot;Harvester ea6e7f5-dirty&quot; --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry​ To change the default entry, first check the --id attribute of a menu entry. Grub menu entries are located in the following files: /run/initramfs/cos-state/grub2/grub.cfg: Contains the default, fallback, and recovery entries/run/initramfs/cos-state/grubmenu: Contains the debug entry In the following example, the id of the entry is debug. # cat \\ /run/initramfs/cos-state/grub2/grub.cfg \\ /run/initramfs/cos-state/grubmenu &lt;...&gt; menuentry &quot;${display_name} (debug)&quot; --id debug { search --no-floppy --set=root --label COS_STATE set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd ${extra_cmdline} ${extra_passive_cmdline} ${crash_kernel_params} initrd (loop0)$initramfs } You can configure the default entry by running the following commands: # mount -o remount,rw /run/initramfs/cos-state # grub2-editenv /run/initramfs/cos-state/grub_oem_env set saved_entry=debug If necessary, you can undo the change by running the command grub2-editenv /run/initramfs/cos-state/grub_oem_env unset saved_entry. How to debug a system crash or hang​ Collect crash log​ If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps​ For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/&lt;time&gt; directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","keywords":"","version":"v1.1"},{"title":"Logging","type":0,"sectionRef":"#","url":"/v1.1/logging/harvester-logging","content":"Logging Available as of v1.1.0 It is important to know what is happening/has happened in the Harvester Cluster. Harvester collects the cluster running log, kubernetes audit and event log right after the cluster is powered on, which is helpful for monitoring, logging, auditing and troubleshooting. Harvester supports sending those logs to various types of log servers. note The size of logging data is related to the cluster scale, workload and other factors. Harvester does not use persistent storage to store log data inside the cluster. Users need to set up a log server to receive logs accordingly. High-level Architecture​ The Banzai Cloud Logging operator now powers both Harvester and Rancher as an in-house logging solution. In Harvester's practice, the Logging, Audit and Event shares one architecture, the Logging is the infrastructure, while the Audit and Event are on top of it. Logging​ The Harvester logging infrastructure allows you to aggregate Harvester logs into an external service such as Graylog, Elasticsearch, Splunk, Grafana Loki and others. Collected Logs​ See below for a list logs that are collected: Logs from all cluster PodsKernel logs from each nodeLogs from select systemd services from each node rke2-serverrke2-agentrancherdrancher-system-agentwickediscsid note Users are able to configure and modify where the aggregated logs are sent, as well as some basic filtering. It is not supported to change which logs are collected. Configuring Log Resources​ Underneath Banzai Cloud's logging operator are fluentd and fluent-bit, which handle the log routing and collecting respectively. If desired, you can modify how many resources are dedicated to those components. From UI​ Navigate to the Configuration page under Monitoring &amp; Logging &gt; Logging.Under the Fluentbit tab, change the resource requests and limits.Under the Fluentd tab, change the resource requests and limits.Click Save on the bottom right of the screen. From CLI​ You can also change the resource configurations from the command line using kubectl edit managedchart -n fleet-local rancher-logging and modifying the relevant files. For harvester version &gt;= v1.1.0, the related paths and default values are: # fluentbit values.fluentbit.resources.limits.cpu: 200m values.fluentbit.resources.limits.memory: 200mi values.fluentbit.resources.requests.cpu: 50m values.fluentbit.resources.requests.memory: 50mi --- #fluentd values.fluentbit.resources.limits.cpu: 200m values.fluentbit.resources.limits.memory: 200mi values.fluentbit.resources.requests.cpu: 50m values.fluentbit.resources.requests.memory: 50mi Configuring Log Destinations​ Logging is backed by the Banzai Cloud Logging Operator, and so is controlled by Flows/ClusterFlows and Outputs/ClusterOutputs. You can route and filter logs as you like by applying these CRDs to the Harvester cluster. When applying new Ouptuts and Flows to the cluster, it can take some time for the logging operator to effectively apply them. So please allow a few minutes for the logs to start flowing. Clustered vs Namespaced​ One important thing to understand when routing logs is the difference between ClusterFlow vs Flow and ClusterOutput vs Output. The main difference between the clustered and non-clustered version of each is that the non-clustered versions are namespaced. The biggest implication of this is that Flows can only access Outputs that are within the same namespace, but can still access any ClusterOutput. For more information, see the documentation: Flows/ClusterFlowsOutputs/ClusterOutputs From UI​ note UI images are for Output and Flow whose configuration process is almost identical to their clustered counterparts. Any differences will be noted in the steps below. Creating Outputs​ Choose the option to create a new Output or ClusterOutput.If creating an Output, select the desired namespace.Add a name for the resources.Select the logging type.Select the logging output type. Configure the output buffer if necessary. Add any labels or annotations. Once done, click Create on the lower right. note Depending on the output selected (Splunk, Elasticsearch, etc), there will be additional fields to specify in the form. Output​ The fields present in the Output form will change depending on the Output chosen, in order to expose the fields present for each output plugin. Output Buffer​ The Output Buffer editor allows you to describe how you want the output buffer to behave. You can find the documentation for the buffer fields here. Labels &amp; Annotations​ You can append labels and annotations to the created resource. Creating Flows​ Choose the option to create a new Flow or ClusterFlow.If creating a Flow, select the desired namespace.Add a name for the resource.Select any nodes whose logs to include or exclude. Select target Outputs and ClusterOutputs. Add any filters if desired. Once done, click Create on the lower left. Matches​ Matches allow you to filter which logs you want to include in the Flow. The form only allows you to include or exclude node logs, but if needed, you can add other match rules supported by the resource by selecting Edit as YAML. For more information about the match directive, see Routing your logs with match directive. Outputs​ Outputs allow you to select one or more OutputRefs to send the aggregated logs to. When creating or editing a Flow / ClusterFlow, it is required that the user selects at least one Output. note There must be at least one existing ClusterOutput or Output that can be attached to the flow, or you will not be able to create / edit the flow. Filters​ Filters allow you to transform, process, and mutate the logs. In the text edit, you will find descriptions of the supported filters, but for more information, you can visit the list of supported filters. From CLI​ To configure log routes via the command line, you only need to define the YAML files for the relevant resources: # elasticsearch-logging.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: elasticsearch-example namespace: fleet-local labels: example-label: elasticsearch-example annotations: example-annotation: elasticsearch-example spec: elasticsearch: host: &lt;url-to-elasticsearch-server&gt; port: 9200 --- apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: elasticsearch-example namespace: fleet-local spec: match: - select: {} globalOutputRefs: - elasticsearch-example And then apply them: kubectl apply -f elasticsearch-logging.yaml Referencing Secrets​ There are 3 ways Banzai Cloud allows specifying secret values via yaml values. The simplest is to use the value key, which is a simple string value for the desired secret. This method should only be used for testing and never in production: aws_key_id: value: &quot;secretvalue&quot; The next is to use valueFrom, which allows referencing a specific value from a secret by a name and key pair: aws_key_id: valueFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; Some plugins require a file to read from rather than simply receiving a value from the secret (this is often the case for CA cert files). In these cases, you need to use mountFrom, which will mount the secret as a file to the underlying fluentd deployment and point the plugin to the file. The valueFrom and mountFrom object look the same: tls_cert_path: mountFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; For more information, you can find the related documentation here. Example Outputs​ Elasticsearch​ For the simplest deployment, you can deploy Elasticsearch on your local system using docker: docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled=false -e node.name=es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23 Make sure that you have set vm.max_map_count to be &gt;= 262144 or the docker command above will fail. Once the Elasticsearch server is up, you can create the yaml file for the ClusterOutput and ClusterFlow: cat &lt;&lt; EOF &gt; elasticsearch-example.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: elasticsearch-example namespace: cattle-logging-system spec: elasticsearch: host: 192.168.0.119 port: 9200 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: elasticsearch-example namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - elasticsearch-example EOF And apply the file: kubectl apply -f elasticsearch-example.yaml After allowing some time for the logging operator to apply the resources, you can test that the logs are flowing: $ curl localhost:9200/fluentd/_search { &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 11603, &quot;max_score&quot;: 1, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;fluentd&quot;, &quot;_type&quot;: &quot;fluentd&quot;, &quot;_id&quot;: &quot;yWHr0oMBXcBggZRJgagY&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;stream&quot;: &quot;stderr&quot;, &quot;logtag&quot;: &quot;F&quot;, &quot;message&quot;: &quot;I1013 02:29:43.020384 1 csi_handler.go:248] Attaching \\&quot;csi-974b4a6d2598d8a7a37b06d06557c428628875e077dabf8f32a6f3aa2750961d\\&quot;&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;csi-attacher-5d4cc8cfc8-hd4nb&quot;, &quot;namespace_name&quot;: &quot;longhorn-system&quot;, &quot;pod_id&quot;: &quot;c63c2014-9556-40ce-a8e1-22c55de12e70&quot;, &quot;labels&quot;: { &quot;app&quot;: &quot;csi-attacher&quot;, &quot;pod-template-hash&quot;: &quot;5d4cc8cfc8&quot; }, &quot;annotations&quot;: { &quot;cni.projectcalico.org/containerID&quot;: &quot;857df09c8ede7b8dee786a8c8788e8465cca58f0b4d973c448ed25bef62660cf&quot;, &quot;cni.projectcalico.org/podIP&quot;: &quot;10.52.0.15/32&quot;, &quot;cni.projectcalico.org/podIPs&quot;: &quot;10.52.0.15/32&quot;, &quot;k8s.v1.cni.cncf.io/network-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;k8s.v1.cni.cncf.io/networks-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;kubernetes.io/psp&quot;: &quot;global-unrestricted-psp&quot; }, &quot;host&quot;: &quot;harvester-node-0&quot;, &quot;container_name&quot;: &quot;csi-attacher&quot;, &quot;docker_id&quot;: &quot;f10e4449492d4191376d3e84e39742bf077ff696acbb1e5f87c9cfbab434edae&quot;, &quot;container_hash&quot;: &quot;sha256:03e115718d258479ce19feeb9635215f98e5ad1475667b4395b79e68caf129a6&quot;, &quot;container_image&quot;: &quot;docker.io/longhornio/csi-attacher:v3.4.0&quot; } } }, ... ] } } Graylog​ You can follow the instructions here to deploy and view cluster logs via Graylog: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;all-logs-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;example-gelf-hs&quot; --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;example-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: gelf: host: &quot;192.168.122.159&quot; port: 12202 protocol: &quot;udp&quot; Splunk​ You can follow the instructions here to deploy and view cluster logs via Splunk. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: splunkHec: hec_host: 192.168.122.101 hec_port: 8088 insecure_ssl: true index: harvester-log-index hec_token: valueFrom: secretKeyRef: key: HECTOKEN name: splunk-hec-token2 buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: globalOutputRefs: - harvester-logging-splunk Loki​ You can follow the instructions in the logging HEP on deploying and viewing cluster logs via Grafana Loki. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-loki namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - harvester-loki --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-loki namespace: cattle-logging-system spec: loki: url: http://loki-stack.cattle-logging-system.svc:3100 extra_labels: logOutput: harvester-loki Audit​ Harvester collects Kubernetes audit and is able to send the audit to various types of log servers. The policy file to guide kube-apiserver is here. Audit Definition​ In kubernetes, the audit data is generated by kube-apiserver according to defined policy. ... Audit policy Audit policy defines rules about what events should be recorded and what data they should include. The audit policy object structure is defined in the audit.k8s.io API group. When an event is processed, it's compared against the list of rules in order. The first matching rule sets the audit level of the event. The defined audit levels are: None - don't log events that match this rule. Metadata - log request metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body. Request - log event metadata and request body but not response body. This does not apply for non-resource requests. RequestResponse - log event metadata, request and response bodies. This does not apply for non-resource requests. Audit Log Format​ Audit Log Format in Kubernetes​ Kubernetes apiserver logs audit with following JSON format into a local file. { &quot;kind&quot;:&quot;Event&quot;, &quot;apiVersion&quot;:&quot;audit.k8s.io/v1&quot;, &quot;level&quot;:&quot;Metadata&quot;, &quot;auditID&quot;:&quot;13d0bf83-7249-417b-b386-d7fc7c024583&quot;, &quot;stage&quot;:&quot;RequestReceived&quot;, &quot;requestURI&quot;:&quot;/apis/flowcontrol.apiserver.k8s.io/v1beta2/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1&quot;, &quot;verb&quot;:&quot;create&quot;, &quot;user&quot;:{&quot;username&quot;:&quot;system:apiserver&quot;,&quot;uid&quot;:&quot;d311c1fe-2d96-4e54-a01b-5203936e1046&quot;,&quot;groups&quot;:[&quot;system:masters&quot;]}, &quot;sourceIPs&quot;:[&quot;::1&quot;], &quot;userAgent&quot;:&quot;kube-apiserver/v1.24.7+rke2r1 (linux/amd64) kubernetes/e6f3597&quot;, &quot;objectRef&quot;:{&quot;resource&quot;:&quot;prioritylevelconfigurations&quot;, &quot;apiGroup&quot;:&quot;flowcontrol.apiserver.k8s.io&quot;, &quot;apiVersion&quot;:&quot;v1beta2&quot;}, &quot;requestReceivedTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot;, &quot;stageTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot; } Audit Log Format before Being Sent to Log Servers​ Harvester keeps the audit log unchanged before sending it to the log server. Audit Log Output/ClusterOutput​ To output audit related log, the Output/ClusterOutput requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit Only from the Type drpo-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: http: endpoint: &quot;http://192.168.122.159:8096/&quot; open_timeout: 3 format: type: &quot;json&quot; buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Audit Log Flow/ClusterFlow​ To route audit related logs, the Flow/ClusterFlow requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit. When you config from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;harvester-audit-webhook&quot; loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Harvester​ Event​ Harvester collects Kubernetes event and is able to send the event to various types of log servers. Event Definition​ Kubernetes events are objects that show you what is happening inside a cluster, such as what decisions were made by the scheduler or why some pods were evicted from the node. All core components and extensions (operators/controllers) may create events through the API Server. Events have no direct relationship with log messages generated by the various components, and are not affected with the log verbosity level. When a component creates an event, it often emits a corresponding log message. Events are garbage collected by the API Server after a short time (typically after an hour), which means that they can be used to understand issues that are happening, but you have to collect them to investigate past events. Events are the first thing to look at for application, as well as infrastructure operations when something is not working as expected. Keeping them for a longer period is essential if the failure is the result of earlier events, or when conducting post-mortem analysis. Event Log Format​ Event Log Format in Kubernetes​ A kubernetes event example: { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;count&quot;: 1, &quot;eventTime&quot;: null, &quot;firstTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;involvedObject&quot;: { &quot;apiVersion&quot;: &quot;kubevirt.io/v1&quot;, &quot;kind&quot;: &quot;VirtualMachineInstance&quot;, &quot;name&quot;: &quot;vm-ide-1&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604601&quot;, &quot;uid&quot;: &quot;1bd4133f-5aa3-4eda-bd26-3193b255b480&quot; }, &quot;kind&quot;: &quot;Event&quot;, &quot;lastTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;message&quot;: &quot;VirtualMachineInstance defined.&quot;, &quot;metadata&quot;: { &quot;creationTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;name&quot;: &quot;vm-ide-1.170e43cbdd833b62&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604626&quot;, &quot;uid&quot;: &quot;0114f4e7-1d4a-4201-b0e5-8cc8ede202f4&quot; }, &quot;reason&quot;: &quot;Created&quot;, &quot;reportingComponent&quot;: &quot;&quot;, &quot;reportingInstance&quot;: &quot;&quot;, &quot;source&quot;: { &quot;component&quot;: &quot;virt-handler&quot;, &quot;host&quot;: &quot;harv1&quot; }, &quot;type&quot;: &quot;Normal&quot; }, Event Log Format before Being Sent to Log Servers​ Each event log has the format of: {&quot;stream&quot;:&quot;&quot;,&quot;logtag&quot;:&quot;F&quot;,&quot;message&quot;:&quot;&quot;,&quot;kubernetes&quot;:{&quot;&quot;}}. The kubernetes event is in the field message. { &quot;stream&quot;:&quot;stdout&quot;, &quot;logtag&quot;:&quot;F&quot;, &quot;message&quot;:&quot;{ \\\\&quot;verb\\\\&quot;:\\\\&quot;ADDED\\\\&quot;, \\\\&quot;event\\\\&quot;:{\\\\&quot;metadata\\\\&quot;:{\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1.170e446c3f890433\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;0b44b6c7-b415-4034-95e5-a476fcec547f\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612482\\\\&quot;,\\\\&quot;creationTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;managedFields\\\\&quot;:[{\\\\&quot;manager\\\\&quot;:\\\\&quot;virt-controller\\\\&quot;,\\\\&quot;operation\\\\&quot;:\\\\&quot;Update\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;v1\\\\&quot;,\\\\&quot;time\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;}]},\\\\&quot;involvedObject\\\\&quot;:{\\\\&quot;kind\\\\&quot;:\\\\&quot;VirtualMachineInstance\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;1bd4133f-5aa3-4eda-bd26-3193b255b480\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;kubevirt.io/v1\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612477\\\\&quot;},\\\\&quot;reason\\\\&quot;:\\\\&quot;SuccessfulDelete\\\\&quot;,\\\\&quot;message\\\\&quot;:\\\\&quot;Deleted PodDisruptionBudget kubevirt-disruption-budget-hmmgd\\\\&quot;,\\\\&quot;source\\\\&quot;:{\\\\&quot;component\\\\&quot;:\\\\&quot;disruptionbudget-controller\\\\&quot;},\\\\&quot;firstTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;lastTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;count\\\\&quot;:1,\\\\&quot;type\\\\&quot;:\\\\&quot;Normal\\\\&quot;,\\\\&quot;eventTime\\\\&quot;:null,\\\\&quot;reportingComponent\\\\&quot;:\\\\&quot;\\\\&quot;,\\\\&quot;reportingInstance\\\\&quot;:\\\\&quot;\\\\&quot;} }&quot;, &quot;kubernetes&quot;:{&quot;pod_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;namespace_name&quot;:&quot;cattle-logging-system&quot;,&quot;pod_id&quot;:&quot;d3453153-58c9-456e-b3c3-d91242580df3&quot;,&quot;labels&quot;:{&quot;app.kubernetes.io/instance&quot;:&quot;harvester-default-event-tailer&quot;,&quot;app.kubernetes.io/name&quot;:&quot;event-tailer&quot;,&quot;controller-revision-hash&quot;:&quot;harvester-default-event-tailer-747b9d4489&quot;,&quot;statefulset.kubernetes.io/pod-name&quot;:&quot;harvester-default-event-tailer-0&quot;},&quot;annotations&quot;:{&quot;cni.projectcalico.org/containerID&quot;:&quot;aa72487922ceb4420ebdefb14a81f0d53029b3aec46ed71a8875ef288cde4103&quot;,&quot;cni.projectcalico.org/podIP&quot;:&quot;10.52.0.178/32&quot;,&quot;cni.projectcalico.org/podIPs&quot;:&quot;10.52.0.178/32&quot;,&quot;k8s.v1.cni.cncf.io/network-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;k8s.v1.cni.cncf.io/networks-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;kubernetes.io/psp&quot;:&quot;global-unrestricted-psp&quot;},&quot;host&quot;:&quot;harv1&quot;,&quot;container_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;docker_id&quot;:&quot;455064de50cc4f66e3dd46c074a1e4e6cfd9139cb74d40f5ba00b4e3e2a7ab2d&quot;,&quot;container_hash&quot;:&quot;docker.io/banzaicloud/eventrouter@sha256:6353d3f961a368d95583758fa05e8f4c0801881c39ed695bd4e8283d373a4262&quot;,&quot;container_image&quot;:&quot;docker.io/banzaicloud/eventrouter:v0.1.0&quot;} } Event Log Output/ClusterOutput​ Events share the Output/ClusterOutput with Logging. Select Logging/Event from the Type drop-down list. Event Log Flow/ClusterFlow​ Compared with the normal Logging Flow/ClusterFlow, the Event related Flow/ClusterFlow, has one more match field with the value of event-tailer. When you configure from the Harvester dashboard, the field is added automatically. Select Event from the Type drop-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-event-webhook namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: - select: labels: app.kubernetes.io/name: event-tailer globalOutputRefs: - harvester-event-webhook ","keywords":"Harvester Logging Audit Event","version":"v1.1"},{"title":"Upgrading Harvester","type":0,"sectionRef":"#","url":"/v1.1/upgrade/index","content":"Upgrading Harvester Upgrade support matrix​ The following table shows the upgrade path of all supported versions. Upgrade from version\tSupported new version(s)v1.1.1/v1.1.2\tv1.1.3 v1.1.0, v1.1.1\tv1.1.2 v1.0.3\tv1.1.0, v1.1.1 (v1.1.1 is recommended) v1.0.2\tv1.0.3 v1.0.1\tv1.0.2 v1.0.0\tv1.0.1 Start an upgrade​ Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: caution Before you upgrade your Harvester cluster, we highly recommend: For upgrading from v1.0.3 to v1.1.0/v1.1.1, please shut down all your VMs first (Harvester GUI -&gt; Virtual Machines -&gt; Select VMs -&gt; Actions -&gt; Stop). Check the link for more information.Back up your VMs if needed. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 25 GB of free space (df -h /usr/local/). caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Prepare an air-gapped upgrade​ caution Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages. Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso. Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml. Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~&gt; sudo -i rancher@node1:~&gt; kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v1.1"},{"title":"Upgrade from v1.0.0 to v1.0.1","type":0,"sectionRef":"#","url":"/v1.1/upgrade/previous-releases/v1-0-0-to-v1-0-1","content":"Upgrade from v1.0.0 to v1.0.1 This document describes how to upgrade from Harvester v1.0.0 to v1.0.1. Note we are still working towards zero-downtime upgrade, due to some known issues please follow the steps below before you upgrade your Harvester cluster: caution Before you upgrade your Harvester cluster, we highly recommend: Shutting down all your VMs (Harvester GUI -&gt; Virtual Machines -&gt; Select VMs -&gt; Actions -&gt; Stop).Back up your VMs. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 25 GB of free space (df -h /usr/local/). caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Create a version​ Log in to one of your server nodes. Become root and create a version: rancher@node1:~&gt; sudo -i node1:~ # kubectl create -f https://releases.rancher.com/harvester/v1.0.1/version.yaml version.harvesterhci.io/1.0.1 created note By default, the ISO image is downloaded from the Harvester release server. To speed up the upgrade and make the upgrade progress smoother, the user can also download the ISO file to a local HTTP server first and substitute the isoURL value in the version.yaml manifest. e.g., # Download the ISO from release server first, assume it's store in http://10.10.0.1/harvester.iso $ sudo -i $ curl -fL https://releases.rancher.com/harvester/v1.0.1/version.yaml -o version.yaml $ vim version.yaml apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.1 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso releaseDate: '20220408' Start the upgrade​ Make sure to read the Warning paragraph at the top of this document first. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Known issues​ Fail to download upgrade image​ Description Downloading the upgrade image can't complete. Workaround We can delete the current upgrade and start over. # log in to one of the server nodes $ sudo -i # list current upgrade, the name changes between deployments $ kubectl get upgrades.harvesterhci.io -n harvester-system NAMESPACE NAME AGE harvester-system hvst-upgrade-77cks 119m $ kubectl delete upgrades.harvesterhci.io hvst-upgrade-77cks -n harvester-system We recommend mirroring the ISO file to a local webserver, please check the notes in the previous section. Stuck in Upgrading System Service​ Description The upgrade is stuck at Upgrading System service. Similar logs are found in rancher pods: [ERROR] available chart version (100.0.2+up0.3.8) for fleet is less than the min version (100.0.3+up0.3.9-rc1) [ERROR] Failed to find system chart fleet will try again in 5 seconds: no chart name found Workaround Delete rancher cluster repositories and restart rancher pods. # login to a server node and become root first kubectl delete clusterrepos.catalog.cattle.io rancher-charts kubectl delete clusterrepos.catalog.cattle.io rancher-rke2-charts kubectl delete clusterrepos.catalog.cattle.io rancher-partner-charts kubectl delete settings.management.cattle.io chart-default-branch kubectl rollout restart deployment rancher -n cattle-system Related issues [BUG] Rancher upgrade fail: Failed to find system chart &quot;fleet&quot; VMs fail to migrate​ Description A node keeps waiting in Pre-draining state.There are VMs on that node (checking for virt-launcher-xxx pods) and they can't be live-migrated out of the node. Workaround Shutdown the VMs, you can do this by: Using the GUI.Using the virtctl command. Related issues [BUG] Upgrade: VMs fail to live-migrate to other hosts in some cases fleet-local/local: another operation (install/upgrade/rollback) is in progress​ Description You see bundles have fleet-local/local: another operation (install/upgrade/rollback) is in progress status in the output: kubectl get bundles -A Related issues [BUG] Upgrade: rancher-monitoring charts can't be upgraded Single node upgrade might fail if node name is too long (&gt;24 characters)​ Related issues https://github.com/harvester/harvester/issues/2114","keywords":"","version":"v1.1"},{"title":"Upgrade from v1.0.1 to v1.0.2","type":0,"sectionRef":"#","url":"/v1.1/upgrade/previous-releases/v1-0-1-to-v1-0-2","content":"Upgrade from v1.0.1 to v1.0.2 General information​ The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ Please check Known issues here.","keywords":"","version":"v1.1"},{"title":"Upgrade from v1.0.2 to v1.0.3","type":0,"sectionRef":"#","url":"/v1.1/upgrade/previous-releases/v1-0-2-to-v1-0-3","content":"Upgrade from v1.0.2 to v1.0.3 General information​ The Harvester GUI Dashboard page should have an upgrade button to perform an upgrade. For more details please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. Fail to download the upgrade image​ Description Downloading the upgrade image can't be complete or fails with an error. Related issues [BUG] failed to create upgrade image Workaround Delete the current upgrade and start over. Please see &quot;Start over an upgrade&quot;. 2. An upgrade is stuck, a node is in &quot;Pre-drained&quot; state (case 1)​ Description Users might see a node is stuck at the Pre-drained state for a while (&gt; 30 minutes). This might be caused by instance-manager-r-* pod on node harvester-z7j2g can’t be drained. To verify the above case: Check rancher server logs: kubectl logs deployment/rancher -n cattle-system Example output: error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. evicting pod longhorn-system/instance-manager-r-10dd59c4 error when evicting pods/&quot;instance-manager-r-10dd59c4&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Verify the pod longhorn-system/instance-manager-r-10dd59c4 is on the stuck node: kubectl get pod instance-manager-r-10dd59c4 -n longhorn-system -o=jsonpath='{.spec.nodeName}' Example output: harvester-z7j2g Check degraded volumes: kubectl get volumes -n longhorn-system Example output: NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-08c34593-8225-4be6-9899-10a978df6ea1 attached healthy True 10485760 harvester-279l2 3d13h pvc-526600f5-bde2-4244-bb8e-7910385cbaeb attached healthy True 21474836480 harvester-x9jqw 3d1h pvc-7b3fc2c3-30eb-48b8-8a98-11913f8314c2 attached healthy True 10737418240 harvester-x9jqw 3d pvc-8065ed6c-a077-472c-920e-5fe9eacff96e attached healthy True 21474836480 harvester-x9jqw 3d pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 attached degraded True 10737418240 harvester-x9jqw 2d23h pvc-9a6539b8-44e5-430e-9b24-ea8290cb13b7 attached healthy True 53687091200 harvester-x9jqw 3d13h Here we can see volume pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 is degraded. note The user needs to check all degraded volumes one by one. Check degraded volume’s replica state: kubectl get replicas -n longhorn-system --selector longhornvolume=pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599 -o json | jq '.items[] | {replica: .metadata.name, healthyAt: .spec.healthyAt, nodeID: .spec.nodeID, state: .status.currentState}' Example output: { &quot;replica&quot;: &quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246&quot;, &quot;healthyAt&quot;: &quot;2022-07-25T07:33:16Z&quot;, &quot;nodeID&quot;: &quot;harvester-z7j2g&quot;, &quot;state&quot;: &quot;running&quot; } { &quot;replica&quot;: &quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-22974d0f&quot;, &quot;healthyAt&quot;: &quot;&quot;, &quot;nodeID&quot;: &quot;harvester-279l2&quot;, &quot;state&quot;: &quot;running&quot; } { &quot;replica&quot;: &quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5&quot;, &quot;healthyAt&quot;: &quot;&quot;, &quot;nodeID&quot;: &quot;harvester-x9jqw&quot;, &quot;state&quot;: &quot;stopped&quot; } Here the only healthy replica is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-15e31246 and it’s on node harvester-z7j2g. So we can confirm the instance-manager-r-* pod resides on node harvester-z7j2g and avoids the drain. Related issues [BUG] Upgrade: longhorn-system can't be evicted Workaround We need to start the “Stopped” replica, from the previous example, the stopped replica’s name is pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5. Check the Longhorn manager log, we should see a replica waiting for the backing image. First, we need to get the manager's name: kubectl get pods -n longhorn-system --selector app=longhorn-manager --field-selector spec.nodeName=harvester-x9jqw Example output: NAME READY STATUS RESTARTS AGE longhorn-manager-zmfbw 1/1 Running 0 3d10h Get pod log: kubectl logs longhorn-manager-zmfbw -n longhorn-system | grep pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 Example output: (...) time=&quot;2022-07-28T04:35:34Z&quot; level=debug msg=&quot;Prepare to create instance pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5&quot; time=&quot;2022-07-28T04:35:34Z&quot; level=debug msg=&quot;Replica pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 is waiting for backing image harvester-system-harvester-iso-n7bxh downloading file to node harvester-x9jqw disk 3830342d-c13d-4e55-ac74-99cad529e9d4, the current state is in-progress&quot; controller=longhorn-replica dataPath= node=harvester-x9jqw nodeID=harvester-x9jqw ownerID=harvester-x9jqw replica=pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5 time=&quot;2022-07-28T04:35:34Z&quot; level=info msg=&quot;Event(v1.ObjectReference{Kind:\\&quot;Replica\\&quot;, Namespace:\\&quot;longhorn-system\\&quot;, Name:\\&quot;pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5\\&quot;, UID:\\&quot;c511630f-2fe2-4cf9-97a4-21bce73782b1\\&quot;, APIVersion:\\&quot;longhorn.io/v1beta1\\&quot;, ResourceVersion:\\&quot;632926\\&quot;, FieldPath:\\&quot;\\&quot;}): type: 'Normal' reason: 'Start' Starts pvc-9a40e5b9-543a-4c90-aafd-ac78b05d7599-r-bc6f7fa5&quot; Here we can determine the replica is waiting for the backing image harvester-system-harvester-iso-n7bxh.Get the disk file map from the backing image: kubectl describe backingimage harvester-system-harvester-iso-n7bxh -n longhorn-system Example output: (...) Disk File Status Map: 3830342d-c13d-4e55-ac74-99cad529e9d4: Last State Transition Time: 2022-07-25T08:30:34Z Message: Progress: 29 State: in-progress 3aa804e1-229d-4141-8816-1f6a7c6c3096: Last State Transition Time: 2022-07-25T08:33:20Z Message: Progress: 100 State: ready 92726efa-bfb3-478e-8553-3206ad34ce70: Last State Transition Time: 2022-07-28T04:31:49Z Message: Progress: 100 State: ready The disk file with UUID 3830342d-c13d-4e55-ac74-99cad529e9d4 has the state in-progress.Next, we need to find backing-image-manager that contains this disk file: kubectl get pod -n longhorn-system --selector=longhorn.io/disk-uuid=3830342d-c13d-4e55-ac74-99cad529e9d4 Example output: NAME READY STATUS RESTARTS AGE backing-image-manager-c00e-3830 1/1 Running 0 3d1h Restart the backing-image-manager by deleting its pod: kubectl delete pod -n longhorn-system backing-image-manager-c00e-3830 3. An upgrade is stuck, a node is in &quot;Pre-drained&quot; state (case 2)​ Description Users might see a node is stuck at the Pre-drained state for a while (&gt; 30 minutes). Here are some steps to verify this issue has happened: Visit the Longhorn GUI: https://{{VIP}}/k8s/clusters/local/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/volume (replace VIP with an appropriate value) and check degraded volumes. The degraded volume might contain a healthy replica only (with blue background) and the healthy replica resides on the &quot;Pre-drained&quot; node: Hover the mouse to the red scheduled icon, the reason is toomanysnapshots: Related issues [BUG] Upgrade is stuck in &quot;Pre-drained&quot; state (Volume has too many system snapshots) Workaround In the &quot;Snapshots and Backup&quot; panel, toggle the &quot;Show System Hidden&quot; switch and delete the latest system snapshot (which is just before the &quot;Volume Head&quot;). The volume will continue rebuilding to resume the upgrade.","keywords":"","version":"v1.1"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.1/troubleshooting/monitoring","content":"Monitoring The following sections contain tips to troubleshoot Harvester Monitoring. Monitoring is unusable​ When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons. Monitoring is unusable due to Pod being stuck in Terminating status​ Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0/3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1/1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod &quot;prometheus-rancher-monitoring-prometheus-0&quot; force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ... Expand PV/Volume Size​ Harvester integrates Longhorn as the default storage provider. Harvester Monitoring uses Persistent Volume (PV) to store running data. When a cluster has been running for a certain time, the Persistent Volume may need to expand its size. Based on the Longhorn Volume expansion guide, Harvester illustrates how to expand the volume size. View Volume​ From Embedded Longhorn WebUI​ Access the embedded Longhorn WebUI according to this document. The default view. Click Volume to list all existing volumes. From CLI​ You can also use kubectl to get all Volumes. # kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 5Gi RWO harvester-longhorn 43h cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 50Gi RWO harvester-longhorn 43h cattle-monitoring-system rancher-monitoring-grafana Bound pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 2Gi RWO harvester-longhorn 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 attached degraded 2147483648 harv31 43h Scale Down a Deployment​ To detach the Volume, you need to scale down the deployment that uses the Volume. The example below is against the PVC claimed by rancher-monitoring-grafana. Find the deployment in the namespace cattle-monitoring-system. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // target deployment rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h Scale down the deployment rancher-monitoring-grafana to 0. # kubectl scale --replicas=0 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment and the volume. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 0/0 0 0 43h // scaled down rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 detached unknown 2147483648 43h // volume is detached Expand Volume​ In the Longhorn WebUI, the related volume becomes Detached. Click the icon in the Operation column, and select Expand Volume. Input a new size, and Longhorn will expand the volume to this size. Scale Up a Deployment​ After the Volume is expanded to target size, you need to scale up the aforementioned deployment to its original replicas. For the above example of rancher-monitoring-grafana, the original replicas is 1. # kubectl scale --replicas=1 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment again. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // scaled up rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h The Volume is attached to the new POD. To now, the Volume is expanded to the new size and the POD is using it smoothly.","keywords":"","version":"v1.1"},{"title":"Upgrade from v1.0.3 to v1.1.0","type":0,"sectionRef":"#","url":"/v1.1/upgrade/previous-releases/v1-0-3-to-v1-1-0","content":"Upgrade from v1.0.3 to v1.1.0 danger Please do not upgrade a running cluster to v1.1.0 if the cluster has the following configuration: The harvester-mgmt network contains two or more network interfaces. networks: harvester-mgmt: interfaces: - name: ens5 - name: ens6 method: dhcp Defining a cluster_network in the configuration file with harvester-mgmt network: cluster_networks: vlan: enable: true description: &quot;harvester-mgmt&quot; config: defaultPhysicalNIC: harvester-mgmt Related issue: [BUG] Harvester Upgrade 1.0.3 to 1.1.0 does not handle multiple SLAVE in BOND for management interface General information​ caution Starting from version v1.1.0, Harvester brings in the new VLAN enhancement feature. Due to the implementation changes, all user VMs must shut down during an upgrade. Please stop the VMs before an upgrade. We introduce Storage Network feature in v1.1.0. Due to a known issue, please create required CRDs before using the feature. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node counts &gt;= 3) before upgrading a node. Generally, users can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. 2. An upgrade is stuck after a node is pre-drained​ An upgrade is stuck, as shown in the screenshot below: Related issue: [BUG] Upgrade stop at upgrading node3 stage, stuck in Pre-drained status Workaround: https://github.com/harvester/harvester/issues/3021#issuecomment-1288747614 3. The monitor dashboard displays nothing after an upgrade​ After an upgrade, the user might see the embedded Grafana dashboard doesn't work: Related issue: [BUG] Cant's display monitoring dashboard after upgrade, alertmanager, prometheus and grafana monitoring pods Terminating Workaround: https://github.com/harvester/harvester/issues/2984#issuecomment-1286517922","keywords":"","version":"v1.1"},{"title":"VM","type":0,"sectionRef":"#","url":"/v1.1/troubleshooting/vm","content":"VM The following sections contain information useful in troubleshooting issues related to Harvester VM management. VM Start Button is Not Visible​ Issue Description​ On rare occasions, the Start button is unavailable on the Harvester UI for VMs that are Off. Without that button, users are unable to start the VMs. VM General Operations​ On the Harvester UI, the Stop button is visible after a VM is created and started. The Start button is visible after the VM is stopped. When the VM is powered off from inside the VM, both the Start and Restart buttons are visible. General VM Related Objects​ A Running VM​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of all three objects is Running. # kubectl get vm NAME AGE STATUS READY vm8 7m25s Running True # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 78s Running 10.52.0.199 harv41 True # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 1/1 Running 0 80s A VM Stopped Using the Harvester UI​ Only the object vm exists and its status is Stopped. Both vmi and pod disappear. # kubectl get vm NAME AGE STATUS READY vm8 123m Stopped False # kubectl get vmi No resources found in default namespace. # kubectl get pod No resources found in default namespace. # A VM Stopped Using the VM's Poweroff Command​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of vm is Stopped, while the status of pod is Completed. # kubectl get vm NAME AGE STATUS READY vm8 134m Stopped False # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 2m49s Succeeded 10.52.0.199 harv41 False # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 0/1 Completed 0 2m54s Issue Analysis​ When the issue occurs, the objects vm, vmi, and pod exist. The status of the objects is similar to that of A VM Stopped Using the VM's Poweroff Command. Example: The VM ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachine ... status: conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready Similarly, the VMI (virtual machine instance) ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance ... name: ocffm031v000 ... status: activePods: ec36a1eb-84a5-4421-b57b-2c14c1975018: aibfredg02 conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready On the other hand, the pod virt-launcher-ocffm031v000-rrkss is not ready (status: &quot;False&quot;) because the pod has run to completion (reason: &quot;PodCompleted&quot;). The underlying container 0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb is terminated, and the exitCode is 0. - apiVersion: v1 kind: Pod ... name: virt-launcher-ocffm031v000-rrkss ... ownerReferences: - apiVersion: kubevirt.io/v1 ... kind: VirtualMachineInstance name: ocffm031v000 uid: 8d2cf524-7e73-4713-86f7-89e7399f25db uid: ec36a1eb-84a5-4421-b57b-2c14c1975018 ... status: conditions: - lastProbeTime: &quot;2023-07-18T13:48:56Z&quot; lastTransitionTime: &quot;2023-07-18T13:48:56Z&quot; message: the virtual machine is not paused reason: NotPaused status: &quot;True&quot; type: kubevirt.io/virtual-machine-unpaused - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-18T13:48:55Z&quot; reason: PodCompleted status: &quot;True&quot; type: Initialized - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: Ready - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: ContainersReady ... containerStatuses: - containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb image: registry.suse.com/suse/sles/15.4/virt-launcher:0.54.0-150400.3.3.2 imageID: sha256:43bb08efdabb90913534b70ec7868a2126fc128887fb5c3c1b505ee6644453a2 lastState: {} name: compute ready: false restartCount: 0 started: false state: terminated: containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb exitCode: 0 finishedAt: &quot;2023-07-20T08:38:55Z&quot; reason: Completed startedAt: &quot;2023-07-18T13:50:17Z&quot; A critical difference is that the Stop and Start actions appear in the stateChangeRequests property of vm. status: conditions: ... printableStatus: Stopped stateChangeRequests: - action: Stop uid: 8d2cf524-7e73-4713-86f7-89e7399f25db - action: Start Root Cause​ The root cause of this issue is under investigation. It is notable that the source code checks the status of vm and assumes that the object is starting. No Start and Restart operations are added to the object. func (vf *vmformatter) canStart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } .. } func (vf *vmformatter) canRestart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } ... } func (vf *vmformatter) isVMStarting(vm *kubevirtv1.VirtualMachine) bool { for _, req := range vm.Status.StateChangeRequests { if req.Action == kubevirtv1.StartRequest { return true } } return false } Workaround​ To address the issue, you can force delete the pod using the command kubectl delete pod virt-launcher-ocffm031v000-rrkss -n namespace --force. After the pod is successfully deleted, the Start button becomes visible again on the Harvester UI. Related Issue​ https://github.com/harvester/harvester/issues/4659","keywords":"","version":"v1.1"},{"title":"Upgrade from v1.0.3/v1.1.0 to v1.1.1","type":0,"sectionRef":"#","url":"/v1.1/upgrade/v1-0-3-to-v1-1-1","content":"Upgrade from v1.0.3/v1.1.0 to v1.1.1 General information​ caution Starting from version v1.1.0, Harvester brings in the new VLAN enhancement feature. Due to the implementation changes, all user VMs must shut down if the upgrade is from v1.0.3 to v1.1.1. Please stop the VMs before an upgrade. Upgrading from v1.1.0 to v1.1.1 doesn't have this restriction (using live migration). Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, users can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. 2. An upgrade is stuck when pre-draining a node (case 2)​ An upgrade is stuck, as shown in the screenshot below: And the user can also observe multiple nodes have scheduling disabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. An upgrade is stuck after a node is pre-drained​ An upgrade is stuck, as shown in the screenshot below: Related issue: [BUG] Upgrade stop at upgrading node3 stage, stuck in Pre-drained status Workaround: https://github.com/harvester/harvester/issues/3021#issuecomment-1288747614 5. Additional certificates disappear after an upgrade​ After upgrading from v1.0.3, the file /etc/pki/trust/anchors/additional-ca.pem configured via the additional-ca setting disappears. To fix this, the user needs to access the Settings page (Harvester GUI, Advanced -&gt; Settings): Edit the additional-ca setting. Back up the current value first, clear the current value, and click Save.Edit the additional-ca setting again. Input the certificates again and click Save.","keywords":"","version":"v1.1"},{"title":"Upgrade from v1.1.1/v1.1.2 to v1.1.3","type":0,"sectionRef":"#","url":"/v1.1/upgrade/v1-1-1-to-v1-1-3","content":"Upgrade from v1.1.1/v1.1.2 to v1.1.3 General information​ An Upgrade button appears on the Dashboard screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see Start an upgrade. For air-gapped environments, see Prepare an air-gapped upgrade. Known Issues​ 1. The upgrade process is stuck when pre-draining a node. (Case 1)​ Starting from v1.1.0, Harvester waits for all volumes to become healthy before upgrading a node (for clusters with three or more nodes). When this issue occurs, you can check the health of the affected volumes on the embedded Longhorn UI. You can also check the pre-drain job logs. For more troubleshooting information, see Phase 4: Upgrade nodes. 2. The upgrade process is stuck when pre-draining a node. (Case 2)​ An upgrade is stuck, as shown in the screenshot below: Harvester is unable to proceed with the upgrade and the status of two or more nodes is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. The upgrade process is stuck on the first node.​ Harvester attempts to upgrade the first node but is unable to proceed. The upgrade eventually fails because the job is not completed by the expected end time. Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. The status of a Fleet bundle after the upgrade indicates that deployment errors occurred.​ After an upgrade is completed, the status of a bundle managed by Fleet may be ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]. The errors that occurred while deploying the bundle may block the next Harvester upgrade or managedChart update if not addressed. To check the status of bundles, run the following command: kubectl get bundles -A The following output indicates that the issue exists in your cluster. NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. The upgrade process stops after the upgrade repository is created.​ Harvester is unable to retrieve the harvester-release.yaml file and proceed with the upgrade. The following error message is displayed: Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)context deadline exceeded (Client.Timeout exceeded while awaiting headers)` message: This issue was fixed in v1.1.2. For v1.1.0 and v1.1.1 users, however, the workaround is to restart the upgrade process. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. The upgrade is stuck in the &quot;Pre-drained&quot; state.​ This issue could be caused by a misconfigured pod disruption budget (PDB). You can perform the following steps to confirm the cause and use the current workaround. In this example, the affected node is harvester-node-1. Check the name of the instance-manager-e or instance-manager-r pod on the node. $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output shows that the instance-manager-r-d4ed2788 pod is on the node. Check the Rancher logs and verify that the instance-manager-e or instance-manager-r pod cannot be drained. $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check if a PDB is associated with the node. $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager for the associated PDB. $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output does not show the affected node, the issue exists in your cluster. In this example, the output shows harvester-node-2 instead of harvester-node-1. Check the health of all volumes. kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should show that all volumes are marked healthy. If not, consider uncordoning nodes to improve volume health. Remove the misconfigured PDB. kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.1"},{"title":"Upgrade from v1.1.0/v1.1.1 to v1.1.2","type":0,"sectionRef":"#","url":"/v1.1/upgrade/v1-1-to-v1-1-2","content":"Upgrade from v1.1.0/v1.1.1 to v1.1.2 danger Please do not upgrade a running cluster to v1.1.2 if your machine has an Intel E810 NIC card. We saw some reports that the NIC card has a problem when added to a bonding device. Please check this issue for more info: https://github.com/harvester/harvester/issues/3860. General information​ Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known Issues​ 1. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 2. An upgrade is stuck when pre-draining a node (case 2)​ An upgrade is stuck, as shown in the screenshot below: And you can also observe that multiple nodes' status is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. After an upgrade, a fleet bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]​ There is a chance fleet-managed bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] after an upgrade. To check if this happened, run the following command: kubectl get bundles -A If you see the following output, it's possible that your cluster has hit the issue: NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. An upgrade stops because it can't retrieve the harvester-release.yaml file​ An upgrade is stopped with the Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) message: We have fixed this issue in v1.1.2. But for v1.1.0 and v1.1.1 users, the workaround is to start over an upgrade. Please refer to Start over an upgrade. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: This could be caused by a misconfigured PDB. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.1"},{"title":"Upload Images","type":0,"sectionRef":"#","url":"/v1.1/upload-image","content":"Upload Images Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL​ To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File​ Currently, qcow2, raw, and ISO images are supported. note Please do not refresh the page until the file upload is finished. Create Images via Volumes​ On the Volumes page, click Export Image. Enter the image name and select a StorageClass to create an image. Image StorageClass​ When creating an image, you can select a StorageClass and use its pre-defined parameters like replicas, node selectors and disk selectors . note The image will not use the StorageClass selected here directly. It's just a StorageClass template. Instead, it will create a special StorageClass under the hood with a prefix name of longhorn-. This is automatically done by the Harvester backend, but it will inherit the parameters from the StorageClass you have selected. Image Labels​ You can add labels to the image, which will help identify the OS type more accurately. Also, you can add any custom labels for filtering if needed. If your image name or URL contains any valid information, the UI will automatically recognize the OS type and image category for you. If not, you can also manually specify those corresponding labels on the UI. Known Issues​ If you remove an image before the upload process is completed, the related Longhorn backing image may become stuck in the deletion state. To determine if the issue has occurred, locate the backing image on the Dashboard screen of the Longhorn UI. If the deletion state has lasted for more than 5 minutes, perform the following steps: Verify that the deletion state has lasted for more than 5 minutes. $ kubectl get backingimagedatasources.longhorn.io -A -o json | jq -r '.items[] | select(.metadata.deletionTimestamp != null) | .metadata.name + &quot; deleted at: &quot; + .metadata.deletionTimestamp ' Example: Edit the affected backing image data source. $ kubectl -n longhorn-system edit backingimagedatasources.longhorn.io &lt;backingimagedatasource name&gt; Example: $ kubectl -n longhorn-system edit backingimagedatasources.longhorn.io default-ubuntu-bionic Delete the finalizers metadata of the affected backing image data source. Example:You must delete finalizers: and - longhorn.io. The backing image should no longer be stuck in the deletion state. Related issue: [BUG] Backing Image deletion stuck if it's deleted during uploading process and bids is ready-for-transfer state Workaround: https://github.com/harvester/harvester/issues/4155#issuecomment-1738348596","keywords":"Harvester harvester Rancher rancher Import Images","version":"v1.1"},{"title":"Access to the Virtual Machine","type":0,"sectionRef":"#","url":"/v1.1/vm/access-to-the-vm","content":"Access to the Virtual Machine Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI​ VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, e.g., the Ubuntu-Minimal-Cloud image, the VM can only be accessed with the serial console. SSH Access​ Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection, which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection, which allows keys or basic auth to be updated dynamically at runtime. Static SSH Key Injection via cloud-init​ You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place. Example of SSH key cloud-init configuration:​ #cloud-config ssh_authorized_keys: - &gt;- ssh-rsa #replace with your public key Dynamic SSH Key Injection via Qemu guest agent​ Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the qemuGuestAgent propagation method. note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click ⋮ button.Click the Edit Config button and go to the Access Credentials tab.Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE).Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI. Access with the SSH Client​ Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@&lt;ip-address-or-hostname&gt; ","keywords":"Harvester harvester Rancher rancher Access to the VM","version":"v1.1"},{"title":"VM Backup, Snapshot & Restore","type":0,"sectionRef":"#","url":"/v1.1/vm/backup-restore","content":"VM Backup, Snapshot &amp; Restore VM Backup &amp; Restore​ Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. note A backup target must be set up. For more information, see Configure Backup Target. If the backup target has not been set, you’ll be prompted with a message to do so. Configure Backup Target​ A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings &gt; backup-target. Parameter\tType\tDescriptionType\tstring\tChoose S3 or NFS Endpoint\tstring\tA hostname or an IP address. It can be left empty for AWS S3. BucketName\tstring\tName of the bucket BucketRegion\tstring\tRegion of the bucket AccessKeyID\tstring\tA user-id that uniquely identifies your account SecretAccessKey\tstring\tThe password to your account Certificate\tstring\tPaste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle\tbool\tUse VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup​ Once the backup target is set, go to the Virtual Machines page.Click Take Backup of the VM actions to create a new VM backup.Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Backup &amp; Snapshot &gt; VM Backups page to view all VM backups. The State will be set to Ready once the backup is complete. Users can either restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup​ To restore a new VM from a backup, follow these steps: Go to the VM Backups page.Specify the new VM name and click Create.A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a backup​ You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the VM Backups page.Click Replace Existing.You can view the restore process from the Virtual Machines page. Restore a new VM on another Harvester cluster​ Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata &amp; content backup feature. prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster​ Check the existing image name (normally starts with image-) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: &quot;&quot; pvcNamespace: &quot;&quot; sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster​ Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster.Go to the VM Backups page.Select the synced VM backup metadata and choose to restore a new VM with a specified VM name.A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page. VM Snapshot &amp; Restore​ Available as of v1.1.0 VM snapshots are created from the Virtual Machines page. The VM snapshot volumes will be stored in the cluster, and they can be used to either restore a new VM or replace an existing VM. Create a VM snapshot​ Go to the Virtual Machines page.Click Take VM Snapshot of the VM actions to create a new VM snapshot.Set a custom snapshot name and click Create to create a new VM snapshot. Result: The snapshot is created. You can also go to the Backup &amp; Snapshot &gt; VM Snapshots page to view all VM snapshots. The State will be set to Ready once the snapshot is complete. Users can either restore a new VM or replace an existing VM using this snapshot. Restore a new VM using a snapshot​ To restore a new VM from a snapshot, follow these steps: Go to the VM Snapshots page.Specify the new VM name and click Create.A new VM will be restored using the snapshot volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a snapshot​ You can replace an existing VM using the snapshot. note You can only choose to retain the previous volumes. Go to the VM Snapshots page.Click Replace Existing.You can view the restore process from the Virtual Machines page.","keywords":"Harvester harvester Rancher rancher VM Backup  Snapshot & Restore","version":"v1.1"},{"title":"Clone VM","type":0,"sectionRef":"#","url":"/v1.1/vm/clone-vm","content":"Clone VM Available as of v1.1.0 VM can be cloned with/without data. This function doesn't need to take a VM snapshot or set up a backup target first. Clone VM with volume data​ On the Virtual Machines page, click Clone of the VM actions.Set a new VM name and click Create to create a new VM. Clone VM without volume data​ On the Virtual Machines page, click Clone of the VM actions.Unclick the clone volume data checkbox.Set a new VM name and click Create to create a new VM.","keywords":"Harvester harvester Rancher rancher Clone VM","version":"v1.1"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/v1.1/upgrade/troubleshooting","content":"Troubleshooting Overview​ Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes. You can click the version in the support matrix table to see if there are any known issues.Dive into the upgrade design proposal. The following section briefly describes phases within an upgrade and possible diagnostic methods. Diagnose the upgrade flow​ A Harvester upgrade process contains several phases. Phase 1: Provision upgrade repository VM.​ The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s Phase 2: Preload container images​ The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase. Phase 3: Upgrade system services​ In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ... Phase 4: Upgrade nodes​ The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node.post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-&lt;hostname&gt;). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... caution Please do not start over an upgrade if the upgrade fails at this phase. Phase 5: Clean-up​ The Harvester controller deletes the upgrade repository VM and all files that are no longer needed. Common operations​ Start over an upgrade​ Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again. Download upgrade logs​ We have designed and implemented a mechanism to automatically collect all the upgrade-related logs and display the upgrade procedure. By default, this is enabled. You can also choose to opt out of such behavior. You can click the Download Log button to download the log archive during an upgrade. Log entries will be collected as files for each upgrade-related Pod, even for intermediate Pods. The support bundle provides a snapshot of the current state of the cluster, including logs and resource manifests, while the upgrade log preserves any logs generated during an upgrade. By combining these two, you can further investigate the issues during upgrades. After the upgrade ended, Harvester stops collecting the upgrade logs to avoid occupying the disk space. In addition, you can click the Dismiss it button to purge the upgrade logs. For more details, please refer to the upgrade log HEP. caution The storage volume for storing upgrade-related logs is 1GB by default. If an upgrade went into issues, the logs may consume all the available space of the volume. To work around such kind of incidents, try the following steps: Detach the log-archive Volume by scaling down the fluentd StatefulSet and downloader Deployment. # Locate the StatefulSet and Deployment $ kubectl -n harvester-system get statefulsets -l harvesterhci.io/upgradeLogComponent=aggregator NAME READY AGE hvst-upgrade-xxxxx-upgradelog-infra-fluentd 1/1 43s $ kubectl -n harvester-system get deployments -l harvesterhci.io/upgradeLogComponent=downloader NAME READY UP-TO-DATE AVAILABLE AGE hvst-upgrade-xxxxx-upgradelog-downloader 1/1 1 1 38s # Scale down the resources to terminate any Pods using the volume $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=0 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=0 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled Find out and expand the log-archive volume size via the Longhorn dashboard. For more details, please refer to the volume expansion guide. # Here's how to find out the actual name of the target volume $ kubectl -n harvester-system get pvc -l harvesterhci.io/upgradeLogComponent=log-archive -o jsonpath='{.items[].spec.volumeName}' pvc-63355afb-ce61-46c4-8781-377cf962278a Recover the fluentd StatefulSet and downloader Deployment. $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=1 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=1 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled ","keywords":"","version":"v1.1"},{"title":"Create a Windows Virtual Machine","type":0,"sectionRef":"#","url":"/v1.1/vm/create-windows-vm","content":"Create a Windows Virtual Machine Create one or more virtual machines from the Virtual Machines page. note For creating Linux virtual machines, please refer to this page. How to Create a Windows VM​ Header Section​ Create a single VM instance or multiple VM instances.Set the VM name.(Optional) Provide a description for the VM.(Optional) Select the VM template windows-iso-image-base-template. This template will add a volume with the virtio drivers for Windows. Basics Tab​ Configure the number of CPU cores assigned to the VM.Configure the amount of Memory assigned to the VM. note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. caution The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab​ The first volume is an Image Volume with the following values: Name: The value cdrom-disk is set by default. You can keep it or change it.Type: Select cd-rom.Image: Select the Windows image to be installed. See Upload Images for the full description on how to create new images.Size: The value 20 is set by default. You can change it if your image has a bigger size.Bus: The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name: The value rootdisk is set by default. You can keep it or change it.Type: Select disk.StorageClass: You can use the default StorageClass harvester-longhorn or specify a custom one.Size: The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value.Bus: The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI. The third volume is a Container with the following values: Name: The value virtio-container-disk is set by default. You can keep it or change it.Type: Select cd-rom.Docker Image: The value registry.suse.com/suse/vmdp/vmdp:2.5.3 is set by default. It's recommended you don't change it.Bus: The value SATA is set by default. It's recommended you don't change it. You can add additional disks using the buttons Add Volume, Add Existing Volume, Add VM Image, or Add Container. Networks Tab​ The Management Network is added by default with the following values: Name: The value default is set by default. You can keep it or change it.Model: The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown.Network: The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks.Type: The value masquerade is set by default. You can keep it or change it to the other available option, bridge. You can add additional networks by clicking Add Network. caution Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration. Node Scheduling Tab​ Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab​ OS Type: The value Windows is set by default. It's recommended you don't change it.Machine Type: The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value.(Optional) Hostname: Set the VM hostname.(Optional) Cloud Config: Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs. Footer Section​ Once all the settings are in place, click on Create to create the VM. note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML. And if you want to cancel all changes made, click Cancel. Installation of Windows​ Select the VM you just created, and click Start to boot up the VM. Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template, the instruction is as follows: Click on Load driver, and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside.Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows, and click Next to load the driver.Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version\tSupported\tDriver pathWindows 7\tNo\tN/A Windows Server 2008\tNo\tN/A Windows Server 2008r2\tNo\tN/A Windows 8 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly. note For full instructions on how to install the VMDP guest driver and tools see the documentation at https://documentation.suse.com/sle-vmdp/2.5/html/vmdp/index.html Known Issues​ Windows ISO unable to boot when using EFI mode​ When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO. VM crashes when reserved memory not enough​ There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. If 256MiB doesn't work, try 512MiB. BSoD (Blue Screen of Death) at first boot time of Windows​ There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name: Harvester Configuration stages: initramfs: - commands: # ... files: - path: /etc/modprobe.d/kvm.conf permissions: 384 owner: 0 group: 0 content: | options kvm ignore_msrs=1 encoding: &quot;&quot; ownerstring: &quot;&quot; # ... note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","keywords":"Harvester harvester Rancher rancher Windows windows Virtual Machine virtual machine Create a Windows VM","version":"v1.1"},{"title":"Edit a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.1/vm/edit-vm","content":"Edit a Virtual Machine How to Edit a VM​ After creating a virtual machine, you can edit your virtual machine by clicking the ⋮ button and selecting the Edit Config button. note In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect. Basics​ On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS. Networks​ You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server. For more details about the network implementation, please refer to the Networking page. Volumes​ You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, edit the size of the expanded volume. After restarting the VM and waiting for the resize to complete, your disk will automatically finish expanding. Access Credentials​ Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has QEMU guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Edit a VM","version":"v1.1"},{"title":"Hot-Plug Volumes","type":0,"sectionRef":"#","url":"/v1.1/vm/hotplug-volume","content":"Hot-Plug Volumes Harvester supports adding hot-plug volumes to a running VM. info Currently, KubeVirt only supports disk bus scsi for hot-plug volumes. For more information, see this issue. Adding Hot-Plug Volumes to a Running VM​ The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page.Find the VM that you want to add a volume to and select ⋮ &gt; Add Volume.Enter the Name and select the Volume.Click Apply.","keywords":"Harvester Hot-plug Volume","version":"v1.1"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.1/vm/index","content":"Create a Virtual Machine How to Create a VM​ You can create one or more virtual machines from the Virtual Machines page. note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances.Select the namespace of your VMs, only the harvester-public namespace is visible to all users.The VM Name is a required field.(Optional) VM template is optional, you can choose iso-image, raw-image or windows-iso-image template to speed up your VM instance creation.Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision).Select SSH keys or upload new keys.Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured.You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced &gt; Networks first. Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable. Volumes​ You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type\tdescriptiondisk\tA disk disk will expose the volume as an ordinary disk to the VM. cd-rom\tA cd-rom disk will expose the volume as a cd-rom drive to the VM. It is read-only by default. A volume's StorageClass can be specified when adding a new empty volume; for other volumes (such as VM images), the StorageClass is defined during image creation. Adding a container disk​ A container disk is an ephemeral storage volume that can be assigned to any number of VMs and provides the ability to store and distribute VM disks in the container image registry. A container disk is: An ideal tool if you want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Ephemeral volumes are designed for VMs that need more storage but don't care whether that data is stored persistently across VM restarts or only expect some read-only input data to be present in files, like configuration data or secret keys.Not a good solution for any workload that requires persistent root disks across VM restarts. A container disk is added when creating a VM by providing a Docker image. When creating a VM, follow these steps: Go to the Volumes tab.Select Add Container.Enter a Name for the container disk.Choose a disk Type.Add a Docker Image. Images must be placed into the /disk directory.Raw and qcow2 formats are supported, but qcow2 is recommended in order to reduce the container image's size. If you use an unsupported image format, the VM will get stuck in a Running state.A container disk also allows you to store disk images in any folder in the /disk directory by adding a custom disk image path. Choose a Bus type. Networks​ You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type\tdescriptionbridge\tConnect using a Linux bridge masquerade\tConnect using iptables rules to NAT the traffic Management Network​ A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes. Secondary Network​ It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks. In bridge VLAN, virtual machines are connected to the host network through a linux bridge. The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Advanced Options​ Run Strategy​ Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true. RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false. Cloud Configuration​ Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Cloud-init​ Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password: password chpasswd: { expire: False } ssh_pwauth: True Example of network-data configuration using DHCP: network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp You can also use the Advanced &gt; Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM. Installing the QEMU guest agent​ The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service. One-time Boot For ISO Installation​ When creating a VM to boot from cd-rom, you can use the bootOrder option so that the OS can boot from cd-rom during image installation, and boot from the disk when the installation is complete without unmounting the cd-rom. The following example describes how to install an ISO image using openSUSE Leap 15.4: Click Images in the left sidebar and download the openSUSE Leap 15.4 ISO image.Click Virtual Machines in the left sidebar, then create a VM. You need to fill up those VM basic configurations.Click the Volumes tab, In the Image field, select the image downloaded in step 1 and ensure Type is cd-romClick Add Volume and select an existing StorageClass.Drag Volume to the top of Image Volume as follows. In this way, the bootOrder of Volume will become 1. Click Create.Open the VM web-vnc you just created and follow the instructions given by the installer.After the installation is complete, reboot the VM as instructed by the operating system (you can remove the installation media after booting the system).After the VM reboots, it will automatically boot from the disk volume and start the operating system.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Create a VM","version":"v1.1"},{"title":"Live Migration","type":0,"sectionRef":"#","url":"/v1.1/vm/live-migration","content":"Live Migration Live migration means moving a virtual machine to a different host without downtime. note Live migration is not allowed when the virtual machine is using a management network of bridge interface type.Live migration is not allowed when the virtual machine has any volume of the CD-ROM type. Such volumes should be ejected before live migration.Live migration is not allowed when the virtual machine has any volume of the Container Disk type. Such volumes should be removed before live migration.Live migration is not allowed when the virtual machine has any PCIDevice passthrough enabled. Such devices need to be removed before live migration. Starting a Migration​ Go to the Virtual Machines page.Find the virtual machine that you want to migrate and select ⋮ &gt; Migrate.Choose the node to which you want to migrate the virtual machine. Click Apply. Aborting a Migration​ Go to the Virtual Machines page.Find the virtual machine in migrating status that you want to abort. Select ⋮ &gt; Abort Migration. Migration Timeouts​ Completion Timeout​ The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout​ Live migration will also be aborted when copying memory doesn't make any progress in 150s.","keywords":"Harvester harvester Rancher rancher Live Migration","version":"v1.1"},{"title":"Resource Overcommit","type":0,"sectionRef":"#","url":"/v1.1/vm/resource-overcommit","content":"Resource Overcommit Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config, this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600%RAM allocation ratio: 150%Storage allocation ratio: 200% note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated. Configure the global setting overcommit-config​ Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced &gt; Settings page.Find the overcommit-config setting.Configure the desired CPU, Memory, and Storage ratio. Configure overcommit for a single virtual machine​ If you need to configure individual virtual machines without involving global configuration, consider adjusting the spec.template.spec.domain.resources.&lt;memory|cpu&gt; value on the target VirtualMachine resource individually. Note that by modifying these values, you are taking over control of virtual machine resource management from Harvester. Reserve more memory for the system overhead​ By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ... Why my virtual machines are scheduled unevenly?​ The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling.","keywords":"Harvester Overcommit Overprovision ballooning","version":"v1.1"},{"title":"Clone a Volume","type":0,"sectionRef":"#","url":"/v1.1/volume/clone-volume","content":"Clone a Volume After creating a volume, you can clone the volume by following the steps below: Click the ⋮ button and select the Clone option. Select clone volume data. Configure the Name of the new volume and click Create. (Optional) A cloned volume can be added to a VM using Add Existing Volume.","keywords":"Volume","version":"v1.1"},{"title":"Edit a Volume","type":0,"sectionRef":"#","url":"/v1.1/volume/edit-volume","content":"Edit a Volume After creating a volume, you can edit your volume by clicking the ⋮ button and selecting the Edit Config option. Expand a Volume​ You can expand a volume by increasing the value of the Size parameter directly. To prevent the expansion from interference by unexpected data R/W, Harvester supports offline expansion only. You must shut down the VM or detach the volume first if it is attached to a VM, and the detached volume will automatically attach to a random node with maintenance mode to expand automatically. Cancel a Failed Volume Expansion​ If you specify a size larger than Longhorn's capacity during the expansion, the status of the volume expansion will be stuck in Resizing. You can cancel the failed volume expansion by clicking the ⋮ button and selecting the Cancel Expand option.","keywords":"Volume","version":"v1.1"},{"title":"Export a Volume to Image","type":0,"sectionRef":"#","url":"/v1.1/volume/export-volume","content":"Export a Volume to Image You can select and export an existing volume to an image by following the steps below: Click the ⋮ button and select the Export Image option. Select the Namespace of the new image. Configure the Name of the new image. Select an existing StorageClass. (Optional) You can download the exported image from the Images page by clicking the ⋮ button and selecting the Download option.","keywords":"Volume","version":"v1.1"},{"title":"Create a Volume","type":0,"sectionRef":"#","url":"/v1.1/volume/index","content":"Create a Volume Create an Empty Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose New in Source.Select an existing StorageClass.Configure the Size of the volume. Create an Image Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose VM Image in Source.Select an existing Image.Configure the Size of the volume. Known Issues​ The Volumes page does not show the created volume​ Issue\tAffected versions\tStatus\tLast updatedThe Volumes page does not show the created volume\tHarvester v1.1.2\tResolved (Harvester &gt; v1.1.2)\t2023-07-28 Summary​ After creating a volume when using Harvester from Rancher, users with the project role Project Member cannot find the newly created volume on the Volumes page. Workaround​ You can temporarily change the Harvester plugin version to v1.2.1-patch1 from the Harvester UI. Go to the Advanced &gt; Settings page. Find the ui-plugin-index and select ⋮ &gt; Edit Setting.Change the Value to https://releases.rancher.com/harvester-ui/plugin/harvester-release-harvester-v1.1.2-patch1/harvester-release-harvester-v1.1.2-patch1.umd.min.js.On the Settings page, find ui-source and select ⋮ &gt; Edit Setting.Change the Value to External to use an external UI source.Log in again as a Project Member user in Rancher to view the newly created volume for your Harvester cluster.","keywords":"Volume","version":"v1.1"},{"title":"Volume Snapshots","type":0,"sectionRef":"#","url":"/v1.1/volume/volume-snapshots","content":"Volume Snapshots A volume snapshot represents a snapshot of a volume on a storage system. After creating a volume, you can create a volume snapshot and restore a volume to the snapshot's state. With volume snapshots, you can easily copy or restore a volume's configuration. Create Volume Snapshots​ You can create a volume snapshot from an existing volume by following these steps: Go to the Volumes page. Choose the volume that you want to take a snapshot of and select ⋮ &gt; Take Snapshot. Enter a Name for the snapshot. Select Create to finish creating a new volume snapshot. Check the status of this operation and view all volume snapshots by going to the Volumes page and selecting the Snapshots tab. When the Ready To Use becomes √, the volume snapshot is ready to use. note A recurring snapshot is currently not supported and is tracked via harvester/harvester#572. Restore a new volume from a volume snapshot​ You can restore a new volume from an existing volume snapshot by following these steps: Go to the Backup &amp; Snapshot &gt; Volume Snapshots page or select a Volume from the Volumes page and go to the Snapshots tab. Select ⋮ &gt; Restore. Specify the Name of the new volume. If the source volume is not an image volume, you can select a different StorageClass. You can not change the StorageClass if the source volume is an image volume. Select Create to finish restoring a new volume.","keywords":"Volume Snapshot Volume Snapshots","version":"v1.1"},{"title":"Harvester Overview","type":0,"sectionRef":"#","url":"/v1.2/","content":"Harvester Overview Harvester is a modern, open, interoperable, hyperconverged infrastructure (HCI) solution built on Kubernetes. It is an open-source alternative designed for operators seeking a cloud-native HCI solution. Harvester runs on bare metal servers and provides integrated virtualization and distributed storage capabilities. In addition to traditional virtual machines (VMs), Harvester supports containerized environments automatically through integration with Rancher. It offers a solution that unifies legacy virtualized infrastructure while enabling the adoption of containers from core to edge locations. Harvester Architecture​ The Harvester architecture consists of cutting-edge open-source technologies: Linux OS. Elemental for SLE-Micro 5.3 is at the core of Harvester and is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Built on top of Kubernetes. Kubernetes has become the predominant infrastructure language across all form factors, and Harvester is an HCI solution with Kubernetes under the hood.Virtualization management with Kubevirt. Kubevirt provides virtualization management using KVM on top of Kubernetes.Storage management with Longhorn. Longhorn provides distributed block storage and tiering.Observability with Grafana and Prometheus. Grafana and Prometheus provide robust monitoring and logging. Harvester Features​ Harvester is an enterprise-ready, easy-to-use infrastructure platform that leverages local, direct attached storage instead of complex external SANs. It utilizes Kubernetes API as a unified automation language across container and VM workloads. Some key features of Harvester include: Easy to get started. Since Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image or automatically install it using iPXE scripts.VM lifecycle management. Easily create, edit, clone, and delete VMs, including SSH-Key injection, cloud-init, and graphic and serial port console.VM live migration. Move a VM to a different host or node with zero downtime.VM backup, snapshot, and restore. Back up your VMs from NFS, S3 servers, or NAS devices. Use your backup to restore a failed VM or create a new VM on a different cluster.Storage management. Harvester supports distributed block storage and tiering. Volumes represent storage; you can easily create, edit, clone, or export a volume.Network management. Supports using a virtual IP (VIP) and multiple Network Interface Cards (NICs). If your VMs need to connect to the external network, create a VLAN or untagged network.Integration with Rancher. Access Harvester directly within Rancher through Rancher’s Virtualization Management page and manage your VM workloads alongside your Kubernetes clusters. Harvester Dashboard​ Harvester provides a powerful and easy-to-use web-based dashboard for visualizing and managing your infrastructure. Once you install Harvester, you can access the IP address for the Harvester Dashboard from the node's terminal.","keywords":"Harvester harvester Rancher rancher Harvester Intro","version":"v1.2"},{"title":"Addons","type":0,"sectionRef":"#","url":"/v1.2/advanced/addons","content":"Addons Harvester makes optional functionality available as Addons. One of the key reasons for the same is to ensure that Harvester installation footprint can be kept low while allowing users to enable/disable functionality based on their use case or requirements. Some level of customization is allowed for each addon, which depends on the underlying addon. Available as of v1.1.0 Harvester v1.2.0 ships with five Addons: pcidevices-controllervm-import-controllerrancher-monitoringrancher-loggingharvester-seeder note harvester-seeder is released as an experimental feature in Harvester v1.2.0 and has an Experimental label added to the Name. You can enable a Disabled by choosing an addon and selecting ⋮ &gt; Enable from the Basic tab. When the addon is enabled successfully, the State will be DeploySuccessful. You can disable an Enabled by choosing an addon and selecting ⋮ &gt; Disable or from the Basic tab. When the addon is disabled successfully, the State will be Disabled. note When an addon is disabled, the configuration data is stored to reuse when the addon is enabled again.","keywords":"","version":"v1.2"},{"title":"Rancher Manager (Experimental)","type":0,"sectionRef":"#","url":"/v1.2/advanced/addons/rancher-vcluster","content":"Rancher Manager (Experimental) Available as of v1.2.0 The rancher-vcluster addon allows you to run Rancher Manager as a workload on the underlying Harvester cluster and is implemented using vcluster. The addon runs a nested K3s cluster in the rancher-vcluster namespace and deploys Rancher to this cluster. During the installation, the ingress for Rancher is synced to the Harvester cluster, allowing end users to access Rancher. Installing rancher-vcluster​ The rancher-vcluster addon is not packaged with Harvester, but you can find it in the expreimental-addon repo. Assuming you are using the Harvester kubeconfig, you can run the following commands to install the addon: kubectl apply -f https://raw.githubusercontent.com/harvester/experimental-addons/main/rancher-vcluster/rancher-vcluster.yaml Configuring rancher-vcluster​ After installing the addon, you need to configure it from the Harvester UI as follows: Select Advanced &gt; Addons.Find the rancher-vcluster addon and select ⋮ &gt; Edit Config. In the Hostname field, enter a valid DNS record pointing to the Harvester VIP. This is essential as the vcluster ingress is synced to the parent Harvester cluster. A valid hostname is used to filter ingress traffic to the vcluster workload.In the Bootstrap Password field, enter the bootstrap password for the new Rancher deployed on the vcluster. Once the addon is deployed, Rancher can take a few minutes to become available. You can then access Rancher via the hostname DNS record that you provided. See Rancher Integration for more information. Disabling rancher-vcluster The rancher-vcluster addon is deployed on a vcluster Statefulset that uses a Longhorn PVC. When rancher-vcluster is disabled, the PVC data-rancher-vcluster-0 will remain in the rancher-vcluster namespace. If you enable the addon again, the PVC is re-used, and Rancher will have the old state available again. If you want to wipe the data, ensure that the PVC is deleted.","keywords":"","version":"v1.2"},{"title":"PCI Devices","type":0,"sectionRef":"#","url":"/v1.2/advanced/addons/pcidevices","content":"PCI Devices Available as of v1.1.0 A PCIDevice in Harvester represents a host device with a PCI address. The devices can be passed through the hypervisor to a VM by creating a PCIDeviceClaim resource, or by using the UI to enable passthrough. Passing a device through the hypervisor means that the VM can directly access the device, and effectively owns the device. A VM can even install its own drivers for that device. This is accomplished by using the pcidevices-controller addon. To use the PCI devices feature, users need to enable the pcidevices-controller addon first. Once the pcidevices-controller addon is deployed successfully, it can take a few minutes for it to scan and the PCIDevice CRDs to become available. Enabling Passthrough on a PCI Device​ Now go to the Advanced -&gt; PCI Devices page: Search for your device by vendor name (e.g. NVIDIA, Intel, etc.) or device name. Select the devices you want to enable for passthrough: Then click Enable Passthrough and read the warning message. If you still want to enable these devices, click Enable and wait for all devices to be Enabled. caution Please do not use host-owned PCI devices (e.g., management and VLAN NICs). Incorrect device allocation may cause damage to your cluster, including node failure. Attaching PCI Devices to a VM​ After enabling these PCI devices, you can navigate to the Virtual Machines page and select Edit Config to pass these devices. Select PCI Devices and use the Available PCI Devices drop-down. Select the devices you want to attach from the list displayed and then click Save. Using a passed-through PCI Device inside the VM​ Boot the VM up, and run lspci inside the VM, the attached PCI devices will show up, although the PCI address in the VM won't necessarily match the PCI address in the host. Installing drivers for your PCI device inside the VM​ This is just like installing drivers in the host. The PCI passthrough feature will bind the host device to the vfio-pci driver, which gives VMs the ability to use their own drivers. Here is a screenshot of NVIDIA drivers being installed in a VM. It includes a CUDA example that proves that the device drivers work. SRIOV Network Devices​ Available as of v1.2.0 The pcidevices-controller addon can now scan network interfaces on the underlying hosts and check if they support SRIOV Virtual Functions (VFs). If a valid device is found, pcidevices-controller will generate a new SRIOVNetworkDevice object. To create VFs on a SriovNetworkDevice, you can click ⋮ &gt; Enable and then define the Number of Virtual Functions. The pcidevices-controller will define the VFs on the network interface and report the new PCI device status for the newly created VFs. On the next re-scan, the pcidevices-controller will create the PCIDevices for VFs. This can take up to 1 minute. You can now navigate to the PCI Devices page to view the new devices. We have also introduced a new filter to help you filter PCI devices by the underlying network interface. The newly created PCI device can be passed through to virtual machines like any other PCI device.","keywords":"","version":"v1.2"},{"title":"Air Gapped Environment","type":0,"sectionRef":"#","url":"/v1.2/airgap","content":"Air Gapped Environment This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy​ In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation​ You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings​ You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI.Find the http-proxy setting, click ⋮ &gt; Edit settingEnter the value(s) for http-proxy, https-proxy and no-proxy. note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. When the nodes in the cluster do not use a proxy to communicate with each other, the CIDR needs to be added to http-proxy.noProxy after the first node is installed successfully. Please refer to fail to deploy a multi-node cluster. Guest Cluster Images​ All necessary images to install and run Harvester are conveniently packaged into the ISO, eliminating the need to pre-load images on bare-metal nodes. A Harvester cluster manages them independently and effectively behind the scenes. However, it's essential to understand a guest K8s cluster (e.g., RKE2 cluster) created by the Harvester node driver is a distinct entity from a Harvester cluster. A guest cluster operates within VMs and requires pulling images either from the internet or a private registry. If the Cloud Provider option is configured to Harvester in a guest K8s cluster, it deploys the Harvester cloud provider and Container Storage Interface (CSI) driver. As a result, we recommend monitoring each RKE2 release in your air gapped environment and pulling the required images into your private registry. Please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section on the Harvester support matrix page for the best Harvester cloud provider and CSI driver capability support.","keywords":"Harvester offline Air-gap HTTP proxy","version":"v1.2"},{"title":"Seeder","type":0,"sectionRef":"#","url":"/v1.2/advanced/addons/seeder","content":"Seeder Available as of v1.2.0 The harvester-seeder addon lets you perform out-of-band operations on underlying nodes. This addon can also discover hardware and hardware events for bare-metal nodes that support redfish-based access and then associate the hardware with the corresponding Harvester nodes. You must enable the harvester-seeder addon from the Addons page to get started. Once the addon is enabled, find the desired host and select Edit Config and go to the Out-Of-Band Access tab. seeder leverages ipmi to manage the underlying node hardware. Hardware discovery and event detection require redfish support. Power operations​ Once you've defined the out-of-band config for a node, you can put the node into Maintenance mode, which allows you to shut down or reboot the node as needed. If a node is shut down, you can also select Power On to power it on again: Hardware event aggregation​ If you've enabled Event in Out-of-Band Access, seeder will leverage redfish to query the underlying hardware for information about component failures and fan temperatures. This information is associated with Harvester nodes and can be used as Kubernetes events. info Sometimes, the Out-Of-Band Access section may be stuck with the message Waiting for &quot;inventories.metal.harvesterhci.io&quot; to be ready. In this case, you need to refresh the page. For more information, see this issue.","keywords":"","version":"v1.2"},{"title":"Third-Party Storage Support","type":0,"sectionRef":"#","url":"/v1.2/advanced/csidriver","content":"Third-Party Storage Support Available as of v1.2.0 Harvester now offers the capability to install a Container Storage Interface (CSI) in your Harvester cluster. This allows you to leverage external storage for the Virtual Machine's non-system data disk, allowing you to use different drivers tailored for specific needs, whether for performance optimization or seamless integration with your existing in-house storage solutions. note The Virtual Machine (VM) image provisioner in Harvester still relies on Longhorn. Before version 1.2.0, Harvester exclusively supported Longhorn for storing VM data and did not offer support for external storage as a destination for VM data. Prerequisites​ For the Harvester functions to work well, the third-party CSI driver needs to have the following capabilities: Support expansionSupport snapshotSupport cloneSupport block deviceSupport Read-Write-Many (RWX), for Live Migration Create Harvester cluster​ Harvester's operating system follows an immutable design, meaning that most OS files revert to their pre-configured state after a reboot. Therefore, you might need to perform additional configurations before installing the Harvester cluster for third-party CSI drivers. Some CSI drivers require additional persistent paths on the host. You can add these paths to os.persistent_state_paths. Some CSI drivers require additional software packages on the host. You can install these packages with os.after_install_chroot_commands. note Upgrading Harvester causes the changes to the OS in the after-install-chroot stage to be lost. You must also configure the after-upgrade-chroot to make your changes persistent across an upgrade. Refer to Runtime persistent changes before upgrading Harvester. Install the CSI driver​ After installing the Harvester cluster is complete, refer to How can I access the kubeconfig file of the Harvester cluster? to get the kubeconfig of the cluster. With the kubeconfig of the Harvester cluster, you can install the third-party CSI drivers into the cluster by following the installation instructions for each CSI driver. You must also refer to the CSI driver documentation to create the StorageClass and VolumeSnapshotClass in the Harvester cluster. Configure Harvester Cluster​ Before you can make use of Harvester's Backup &amp; Snapshot features, you need to set up some essential configurations through the Harvester csi-driver-config setting. Follow these steps to make these configurations: Login to the Harvester UI, then navigate to Advanced &gt; Settings.Find and select csi-driver-config, and then select ⋮ &gt; Edit Setting to access the configuration options.Set the Provisioner to the third-party CSI driver in the settings.Next, Configure the Volume Snapshot Class Name. This setting points to the name of the VolumeSnapshotClass used for creating volume snapshots or VM snapshots.Similarly, Configure the Backup Volume Snapshot Class Name. This corresponds to the name of the VolumeSnapshotClass responsible for creating VM backups. Use the CSI driver​ After successfully configuring these settings, you can utilize the third-party StorageClass. You can apply the third-party StorageClass when creating an empty volume or adding a new block volume to a VM, enhancing your Harvester cluster's storage capabilities. With these configurations in place, your Harvester cluster is ready to make the most of the third-party storage integration. References​ Using Rook Ceph Storage in Harvester","keywords":"","version":"v1.2"},{"title":"Single-Node Clusters","type":0,"sectionRef":"#","url":"/v1.2/advanced/singlenodeclusters","content":"Single-Node Clusters As of Harvester release v1.2.0, single-node clusters are supported for implementations that require minimal initial deployment resources or that can tolerate lower resiliency. You can create single-node clusters using the standard installation methods (ISO, USB, and PXE boot). Single-node clusters support most Harvester features, including the creation of RKE2 clusters and node upgrades (with some limitations). However, this deployment type has the following key disadvantages: No high availability: Errors and updates that require rebooting of the node cause downtime to running VMs.No multi-replica support: Only one replica is created for each volume in Longhorn.No live migration and zero-downtime support during upgrades. Prerequisites​ Before you begin deploying your single-node cluster, ensure that the following requirements are addressed. Hardware: Use server-class hardware with sufficient resources to run Harvester and a production workload. Laptops and nested virtualization are not supported. Network: Configure ports based on the type of traffic to be transmitted among VMs. StorageClass: Create a new default StorageClass with the Number of Replicas parameter set to &quot;1&quot;. This ensures that only one replica is created for each volume in Longhorn. important The default StorageClass harvester-longhorn has a replica count value of 3 for high availability. If you use this StorageClass to create volumes for your single-node cluster, Longhorn is unable to create the configured number of replicas. This results in volumes being marked as &quot;Degraded&quot; on the Longhorn UI.","keywords":"Single Node","version":"v1.2"},{"title":"StorageClass","type":0,"sectionRef":"#","url":"/v1.2/advanced/storageclass","content":"StorageClass A StorageClass allows administrators to describe the classes of storage they offer. Different Longhorn StorageClasses might map to replica policies, or to node schedule policies, or disk schedule policies determined by the cluster administrators. This concept is sometimes called profiles in other storage systems. note For support with other storage, please refer to Third-Party Storage Support Creating a StorageClass​ You can create one or more StorageClasses from the Advanced &gt; StorageClasses page. note After a StorageClass is created, nothing can be changed except Description. Header Section​ Name: name of the StorageClassDescription (optional): description of the StorageClass Parameters Tab​ Number of Replicas​ The number of replicas created for each volume in Longhorn. Defaults to 3. Stale Replica Timeout​ Determines when Longhorn would clean up an error replica after the replica's status is ERROR. The unit is minute. Defaults to 30 minutes in Harvester. Node Selector (Optional)​ Select the node tags to be matched in the volume scheduling stage. You can add node tags by going to Host &gt; Edit Config. Disk Selector (Optional)​ Select the disk tags to be matched in the volume scheduling stage. You can add disk tags by going to Host &gt; Edit Config. Migratable​ Whether Live Migration is supported. Defaults to Yes. Customize Tab​ Reclaim Policy​ Volumes dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class. The Delete mode is used by default. Delete: Deletes volumes and the underlying devices when the volume claim is deleted.Retain: Retains the volume for manual cleanup. Allow Volume Expansion​ Volumes can be configured to be expandable. This feature is Enabled by default, which allows users to resize the volume by editing the corresponding PVC object. note You can only use the volume expansion feature to grow a Volume, not to shrink it. Volume Binding Mode​ The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. The Immediate mode is used by default. Immediate: Binds and provisions a persistent volume once the PersistentVolumeClaim is created.WaitForFirstConsumer: Binds and provisions a persistent volume once a VM using the PersistentVolumeClaim is created. Appendix - Use Case​ HDD Scenario​ With the introduction of StorageClass, users can now use HDDs for tiered or archived cold storage. caution HDD is not recommended for guest RKE2 clusters or VMs with good performance disk requirements. Recommended Practice​ First, add your HDD on the Host page and specify the disk tags as needed, such asHDD or ColdStorage. For more information on how to add extra disks and disk tags, see Multi-disk Management for details. Then, create a new StorageClass for the HDD (use the above disk tags). For hard drives with large capacity but slow performance, the number of replicas can be reduced to improve performance. You can now create a volume using the above StorageClass with HDDs mostly for cold storage or archiving purpose.","keywords":"","version":"v1.2"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/v1.2/authentication","content":"Authentication After installation, user will be prompted to set the password for the default admin user on the first-time login. note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","keywords":"Harvester harvester Rancher rancher Authentication","version":"v1.2"},{"title":"VM Import","type":0,"sectionRef":"#","url":"/v1.2/advanced/addons/vmimport","content":"VM Import Available as of v1.1.0 Beginning with v1.1.0, users can import their virtual machines from VMWare and OpenStack into Harvester. This is accomplished using the vm-import-controller addon. To use the VM Import feature, users need to enable the vm-import-controller addon. By default, vm-import-controller leverages ephemeral storage, which is mounted from /var/lib/kubelet. During the migration, a large VM's node could run out of space on this mount, resulting in subsequent scheduling failures. To avoid this, users are advised to enable PVC-backed storage and customize the amount of storage needed. According to the best practice, the PVC size should be twice the size of the largest VM being migrated. This is essential as the PVC is used as scratch space to download the VM, and convert the disks into raw image files. vm-import-controller​ Currently, the following source providers are supported: VMWareOpenStack API​ The vm-import-controller introduces two CRDs. Sources​ Sources allow users to define valid source clusters. For example: apiVersion: migration.harvesterhci.io/v1beta1 kind: VmwareSource metadata: name: vcsim namespace: default spec: endpoint: &quot;https://vscim/sdk&quot; dc: &quot;DCO&quot; credentials: name: vsphere-credentials namespace: default The secret contains the credentials for the vCenter endpoint: apiVersion: v1 kind: Secret metadata: name: vsphere-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; As part of the reconciliation process, the controller will log into vCenter and verify whether the dc specified in the source spec is valid. Once this check is passed, the source is marked as ready and can be used for VM migrations. $ kubectl get vmwaresource.migration NAME STATUS vcsim clusterReady For OpenStack-based source clusters, an example definition is as follows: apiVersion: migration.harvesterhci.io/v1beta1 kind: OpenstackSource metadata: name: devstack namespace: default spec: endpoint: &quot;https://devstack/identity&quot; region: &quot;RegionOne&quot; credentials: name: devstack-credentials namespace: default The secret contains the credentials for the OpenStack endpoint: apiVersion: v1 kind: Secret metadata: name: devstack-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; &quot;project_name&quot;: &quot;admin&quot; &quot;domain_name&quot;: &quot;default&quot; &quot;ca_cert&quot;: &quot;pem-encoded-ca-cert&quot; The OpenStack source reconciliation process attempts to list VMs in the project and marks the source as ready. $ kubectl get opestacksource.migration NAME STATUS devstack clusterReady VirtualMachineImport​ The VirtualMachineImport CRD provides a way for users to define a source VM and map to the actual source cluster to perform VM export/import. A sample VirtualMachineImport looks like this: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: alpine-export-test namespace: default spec: virtualMachineName: &quot;alpine-export-test&quot; networkMapping: - sourceNetwork: &quot;dvSwitch 1&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;dvSwitch 2&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: vcsim namespace: default kind: VmwareSource apiVersion: migration.harvesterhci.io/v1beta1 This will trigger the controller to export the VM named &quot;alpine-export-test&quot; on the VMWare source cluster to be exported, processed and recreated into the harvester cluster This can take a while based on the size of the virtual machine, but users should see VirtualMachineImages created for each disk in the defined virtual machine. The list of items in networkMapping will define how the source network interfaces are mapped to the Harvester Networks. If a match is not found, each unmatched network interface is attached to the default managementNetwork. Once the virtual machine has been imported successfully, the object will reflect the status: $ kubectl get virtualmachineimport.migration NAME STATUS alpine-export-test virtualMachineRunning openstack-cirros-test virtualMachineRunning Similarly, users can define a VirtualMachineImport for an OpenStack source as well: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: openstack-demo namespace: default spec: virtualMachineName: &quot;openstack-demo&quot; #Name or UUID for instance networkMapping: - sourceNetwork: &quot;shared&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;public&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: devstack namespace: default kind: OpenstackSource apiVersion: migration.harvesterhci.io/v1beta1 note OpenStack allows users to have multiple instances with the same name. In such a scenario, users are advised to use the Instance ID. The reconciliation logic tries to perform a name-to-ID lookup when a name is used.","keywords":"","version":"v1.2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/v1.2/faq","content":"FAQ This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node?​ $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard?​ username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster?​ Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM?​ # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/reference/cli.html#clean How can I reset the administrator password?​ In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher --no-headers | head -1 | awk '{ print $1 }') -c rancher -- reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt; I added an additional disk with partitions. Why is it not getting detected?​ As of Harvester v1.0.2, we no longer support adding additional partitioned disks, so be sure to delete all partitions first (e.g., using fdisk). Why are there some Harvester pods that become ErrImagePull/ImagePullBackOff?​ This is likely because your Harvester cluster is an air-gapped setup, and some pre-loaded container images are missing. Kubernetes has a mechanism that does garbage collection against bloated image stores. When the partition which stores container images is over 85% full, kubelet tries to prune the images based on the last time they were used, starting with the oldest, until the occupancy is lower than 80%. These numbers (85% and 80%) are default High/Low thresholds that come with Kubernetes. To recover from this state, do one of the following depending on the cluster's configuration: Pull the missing images from sources outside of the cluster (if it's an air-gapped environment, you might need to set up an HTTP proxy beforehand).Manually import the images from the Harvester ISO image. note Take v1.1.2 as an example, download the Harvester ISO image from the official URL. Then extract the image list from the ISO image to decide which image tarball we're going to import. For instance, we want to import the missing container image rancher/harvester-upgrade $ curl -sfL https://releases.rancher.com/harvester/v1.1.2/harvester-v1.1.2-amd64.iso -o harvester.iso $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images-lists images-lists $ grep -R &quot;rancher/harvester-upgrade&quot; images-lists/ images-lists/harvester-images-v1.1.2.txt:docker.io/rancher/harvester-upgrade:v1.1.2 Find out the location of the image tarball, and extract it from the ISO image. Decompress the extracted zstd image tarball. $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images/harvester-images-v1.1.2.tar.zst harvester.tar.zst $ zstd -d --rm harvester.tar.zst Upload the image tarball to the Harvester nodes that need recover. Finally, execute the following command to import the container images on each of them. $ ctr -n k8s.io images import harvester.tar $ rm harvester.tar Find the missing images on that node from the other nodes, then export the images from the node where the images still exist and import them on the missing node. To prevent this from happening, we recommend cleaning up unused container images from the previous version after each successful Harvester upgrade if the image store disk space is stressed. We provided a harv-purge-images script that makes cleaning up disk space easy, especially for container image storage. The script has to be executed on each Harvester node. For example, if the cluster was originally in v1.1.2, and now it gets upgraded to v1.2.0, you can do the following to discard the container images that are only used in v1.1.2 but no longer needed in v1.2.0: # on each node $ ./harv-purge-images.sh v1.1.2 v1.2.0 caution The script only downloads the image lists and compares the two to calculate the difference between the two versions. It does not communicate with the cluster and, as a result, does not know what version the cluster was upgraded from.We published image lists for each version released since v1.1.0. For clusters older than v1.1.0, you have to clean up the old images manually.","keywords":"","version":"v1.2"},{"title":"Host Management","type":0,"sectionRef":"#","url":"/v1.2/host/","content":"Host Management Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance​ For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node​ Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you’re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node​ caution Before removing a node from a Harvester cluster, determine if the remaining nodes have enough computing and storage resources to take on the workload from the node to be removed. Check the following: Current resource usage in the cluster (on the Hosts screen of the Harvester UI)Ability of the remaining nodes to maintain enough replicas for all volumes If the remaining nodes do not have enough resources, VMs might fail to migrate and volumes might degrade when you remove a node. 1. Check if the node can be removed from the cluster.​ You can safely remove a control plane node depending on the quantity and availability of other nodes in the cluster. The cluster has three control plane nodes and one or more worker nodes. When you remove a control plane node, a worker node will be promoted to control plane node randomly. The cluster has three control plane nodes and no worker nodes. You must add a new node to the cluster before removing a control plane node. This ensures that the cluster always has three control plane nodes and that a quorum can be formed even if one control plane node fails. The cluster has only two control plane nodes and no worker nodes. Removing a control plane node in this situation is not recommended because etcd data is not replicated in a single-node cluster. Failure of a single node can cause etcd to lose its quorum and shut the cluster down. 2. Check the status of volumes.​ Access the embedded Longhorn UI. Go to the Volume screen. Verify that the state of all volumes is Healthy. 3. Evict replicas from the node to be removed.​ Access the embedded Longhorn UI. Go to the Node screen. Select the node that you want to remove, click the icon in the Operation column, and then select Edit node and disks. Configure the following settings: Node Scheduling: Select Disable.Evict Requested&quot; Select True. Click Save. Go back to the Node screen and verify that Replicas value for the node to be removed is 0. important Eviction cannot be completed if the remaining nodes cannot accept replicas from the node to be removed. In this case, some volumes will remain in the Degraded state until you add more nodes to the cluster. 4. Manage non-migratable VMs.​ Live migration cannot be performed for VMs with certain properties. The VM has PCI passthrough devices or vGPU devices. A PCI device is bound to a node. You must remove the PCI device from the VM, or delete the VM and then create a new VM from a backup or snapshot. The VM has a node selector or affinity rules that bind it to the node to be removed. You must change the node selector or affinity rules. The VM is on a VM network that binds it to the node to be removed. You must select a different VM network. tip Create a backup or snapshot for each non-migratable VM before modifying the settings that bind it to the node that you want to remove. 5. Evict workloads from the node to be removed.​ If your cluster is running Harvester v1.1.2 or later, you can enable Maintenance Mode on the node to automatically live-migrate VMs and workloads. You can also manually live-migrate VMs to other nodes. All workloads have been successfully evicted if the node state is Maintenance. important If a cluster has only two control plane nodes, Harvester does not allow you to enable Maintenance Mode on any node. You can manually drain the node to be removed using the following command: kubectl drain &lt;node_name&gt; --force --ignore-daemonsets --delete-local-data --pod-selector='app!=csi-attacher,app!=csi-provisioner' Again, removing a control plane node in this situation is not recommended because etcd data is not replicated. Failure of a single node can cause etcd to lose its quorum and shut the cluster down. 6. Remove the node.​ On the Harvester UI, go to the Hosts screen. Locate the node that you want to remove, and then click ⋮ &gt; Delete. 7. Delete RKE2 services on the node.​ Log in to the node using the root account. Run the script /opt/rke2/bin/rke2-uninstall.sh. note There's a known issue about node hard delete. Once resolved, you can skip this step. Multi-disk Management​ Add Additional Disks​ Users can view and add multiple disks as additional data volumes from the edit host page. Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config. Select the Storage tab and click Add Disk. caution As of Harvester v1.0.2, we no longer support adding partitions as additional disks. If you want to add it as an additional disk, be sure to delete all partitions first (e.g., using fdisk). Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. Last, you can click ⋮ &gt; Edit Config again to check the newly added disk. Meanwhile, you can also add the &quot;Host/Disk&quot; tag (details are described in the next section). note In order for Harvester to identify the disks, each disk needs to have a unique WWN. Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above. However, you can still add a virtual disk with the SCSI controller. The WWN information could be added manually along with the disk attach operation. For more details, please refer to the script. Storage Tags​ The storage tag feature enables only certain nodes or disks to be used for storing Longhorn volume data. For example, performance-sensitive data can use only the high-performance disks which can be tagged as fast, ssd or nvme, or only the high-performance nodes tagged as baremetal. This feature supports both disks and nodes. Setup​ The tags can be set up through the Harvester UI on the host page: Click Hosts -&gt; Edit Config -&gt; StorageClick Add Host/Disk Tags to start typing and hit enter to add new tags.Click Save to update tags.On the StorageClasses page, create a new storage class and select those defined tags on the Node Selector and Disk Selector fields. All the existing scheduled volumes on the node or disk won’t be affected by the new tags. note When multiple tags are specified for a volume, the disk and the nodes (that the disk belongs to) must have all the specified tags to become usable. Remove disks​ Before removing a disk, you must first evict Longhorn replicas on the disk. note The replica data would be rebuilt to another disk automatically to keep the high availability. Identify the disk to remove (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select the node name and go to the Storage tab.Find the disk you want to remove. Let's assume we want to remove /dev/sdb, and the disk's mount point is /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04. Evict replicas (Longhorn dashboard)​ Please follow this session to enable the embedded Longhorn dashboard.Visit the Longhorn dashboard and go to the Node page.Expand the node containing the disk. Confirm the mount point /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04 is in the disks list. Select Edit node and disks. Scroll to the disk you want to remove. Set Scheduling to Disable.Set Eviction Requested to True.Select Save. Do not select the delete icon. The disk will be disabled. Please wait until the disk replica count becomes 0 to proceed with removing the disk. Remove the disk (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select ⋮ &gt; Edit Config.Go to the Storage tab and select x to remove the disk. Select Save to remove the disk. Ksmtuned Mode​ Available as of v1.1.0 Ksmtuned is a KSM automation tool deployed as a DaemonSet to run Ksmtuned on each node. It will start or stop the KSM by watching the available memory percentage ratio (i.e. Threshold Coefficient). By default, you need to manually enable Ksmtuned on each node UI. You will be able to see the KSM statistics from the node UI after 1-2 minutes.(check KSM for more details). Quick Run​ Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config.Select the Ksmtuned tab and select Run in Run Strategy.(Optional) You can modify Threshold Coefficient as needed. Click Save to update.Wait for about 1-2 minutes and you can check its Statistics by clicking Your Node &gt; Ksmtuned tab. Parameters​ Run Strategy: Stop: Stop Ksmtuned and KSM. VMs can still use shared memory pages.Run: Run Ksmtuned.Prune: Stop Ksmtuned and prune KSM memory pages. Threshold Coefficient: configures the available memory percentage ratio. If the available memory is less than the threshold, KSM will be started; otherwise, KSM will be stopped. Merge Across Nodes: specifies if pages from different NUMA nodes can be merged. Mode: Standard: The default mode. The control node ksmd uses about 20% of a single CPU. It uses the following parameters: Boost: 0 Decay: 0 Maximum Pages: 100 Minimum Pages: 100 Sleep Time: 20 High-performance: Node ksmd uses 20% to 100% of a single CPU and has higher scanning and merging efficiency. It uses the following parameters: Boost: 200 Decay: 50 Maximum Pages: 10000 Minimum Pages: 100 Sleep Time: 20 Customized: You can customize the configuration to reach the performance that you want. Ksmtuned uses the following parameters to control KSM efficiency: Parameters\tDescriptionBoost\tThe number of scanned pages is incremented each time if the available memory is less than the Threshold Coefficient. Decay\tThe number of scanned pages is decremented each time if the available memory is greater than the Threshold Coefficient. Maximum Pages\tMaximum number of pages per scan. Minimum Pages\tThe minimum number of pages per scan, also the configuration for the first run. Sleep Time (ms)\tThe interval between two scans, which is calculated with the formula (Sleep Time * 16 * 1024* 1024 / Total Memory). Minimum: 10ms. For example, assume you have a 512GiB memory node that uses the following parameters: Boost: 300 Decay: 100 Maximum Pages: 5000 Minimum Pages: 1000 Sleep Time: 50 When Ksmtuned starts, initialize pages_to_scan in KSM to 1000 (Minimum Pages) and set sleep_millisecs to 10 (50 * 16 * 1024 * 1024 / 536870912 KiB &lt; 10). KSM starts when the available memory falls below the Threshold Coefficient. If it detects that it is running, pages_to_scan increments by 300 (Boost) every minute until it reaches 5000 (Maximum Pages). KSM will stop when the available memory is above the Threshold Coefficient. If it detects that it is stopped, pages_to_scan decrements by 100 (Decay) every minute until it reaches 1000 (Minimum Pages). NTP Configuration​ Time synchronization is an important aspect of distributed cluster architecture. Because of this, Harvester now provides a simpler way for configuring NTP settings. In previous Harvester versions, NTP settings were mainly configurable during the installation process. To modify the settings, you needed to manually update the configuration file on each node. Beginning with version v1.2.0, Harvester is supporting NTP configuration on the Harvester UI Settings screen (Advanced &gt; Settings). You can configure NTP settings for the entire Harvester cluster at any time, and the settings are applied to all nodes in the cluster. You can set up multiple NTP servers at once. You can check the settings in the node.harvesterhci.io/ntp-service annotation in Kubernetes nodes: ntpSyncStatus: Status of the connection to NTP servers (possible values: disabled, synced and unsynced)currentNtpServers: List of existing NTP servers $ kubectl get nodes harvester-node-0 -o yaml |yq -e '.metadata.annotations.[&quot;node.harvesterhci.io/ntp-service&quot;]' {&quot;ntpSyncStatus&quot;:&quot;synced&quot;,&quot;currentNtpServers&quot;:&quot;0.suse.pool.ntp.org 1.suse.pool.ntp.org&quot;} Note: Do not modify the NTP configuration file on each node. Harvester will automatically sync the settings that you configured on the Harvester UI to the nodes.If you upgraded Harvester from an earlier version, the ntp-servers list on the Settings screen will be empty (see screenshot). You must manually configure the NTP settings because Harvester is unaware of the previous settings and is unable to detect conflicts.","keywords":"","version":"v1.2"},{"title":"Settings","type":0,"sectionRef":"#","url":"/v1.2/advanced/index","content":"Settings This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca​ This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example​ -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- caution Changing this setting might cause a short downtime for single-node clusters. auto-disk-provision-paths [Experimental]​ This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. note This setting only adds formatted disks mounted to the system. caution This setting is applied to every Node in the cluster.All the data in these storage devices will be destroyed. Use at your own risk. Default: none Example​ The following example will add disks matching the glob pattern /dev/sd* or /dev/hd*: /dev/sd*,/dev/hd* backup-target​ This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation. Default: none Example​ { &quot;type&quot;: &quot;s3&quot;, &quot;endpoint&quot;: &quot;https://s3.endpoint.svc&quot;, &quot;accessKeyId&quot;: &quot;test-access-key-id&quot;, &quot;secretAccessKey&quot;: &quot;test-access-key&quot;, &quot;bucketName&quot;: &quot;test-bup&quot;, &quot;bucketRegion&quot;: &quot;us‑east‑2&quot;, &quot;cert&quot;: &quot;&quot;, &quot;virtualHostedStyle&quot;: false } cluster-registration-url​ This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example​ https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml note When you configure this setting, a new pod called cattle-cluster-agent-* is created in the namespace cattle-system for registration purposes. This pod uses the container image rancher/rancher-agent:related-version, which is not packed into the Harvester ISO and is instead determined by Rancher. The related-version is usually the same as the Rancher version. For example, when you register Harvester to Rancher v2.7.9, the image is rancher/rancher-agent:v2.7.9. For more information, see Find the required assets for your Rancher version in the Rancher documentation. Depending on your Harvester settings, the image is downloaded from either of the following locations: Harvester containerd-registry: You can configure a private registry for the Harvester cluster. Docker Hub (docker.io): This is the default option when you do not configure a private registry in Rancher. Alternatively, you can obtain a copy of the image and manually upload it to all Harvester nodes. containerd-registry​ This setting allows you to configure a private registry for the Harvester cluster. The value will be set in /etc/rancher/rke2/registries.yaml of each node. You can read RKE2 - Containerd Registry Configuration for more information. note If you set a username and password for a private registry, the system will automatically remove it to protect the credential after the system saves it in registries.yaml. Example​ { &quot;Mirrors&quot;: { &quot;docker.io&quot;: { &quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;], &quot;Rewrites&quot;: null } }, &quot;Configs&quot;: { &quot;myregistry.local:5000&quot;: { &quot;Auth&quot;: { &quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot; }, &quot;TLS&quot;: { &quot;InsecureSkipVerify&quot;: false } } } } csi-driver-config​ Available as of v1.2.0 If you install third-party CSI drivers in the Harvester cluster, you must configure some necessary information through this setting before using Backup &amp; Snapshot related features. Default: { &quot;driver.longhorn.io&quot;: { &quot;volumeSnapshotClassName&quot;: &quot;longhorn-snapshot&quot;, &quot;backupVolumeSnapshotClassName&quot;: &quot;longhorn&quot; } } Add the provisioner for the newly added CSI driver.Configure Volume Snapshot Class Name, which refers to the name of the VolumeSnapshotClass used to create volume snapshots or VM snapshots.Configure Backup Volume Snapshot Class Name, which refers to the name of the VolumeSnapshotClass used to create VM backups. default-vm-termination-grace-period-seconds​ Available as of v1.2.0 This setting allows you to specify a default termination grace period for stopping a virtual machine in seconds. Default: 120 http-proxy​ This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: &quot;httpProxy&quot;: &quot;http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Proxy URL for HTTPS requests: &quot;httpsProxy&quot;: &quot;https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Comma-separated list of hostnames and/or CIDRs: &quot;noProxy&quot;: &quot;&lt;hostname | CIDR&gt;&quot; caution If you configure httpProxy and httpsProxy, you must also put Harvester node's CIDR into noProxy, otherwise the Harvester cluster will be broken. If you also configure cluster-registration-url, you usually need to add the host of cluster-registration-url to noProxy as well, otherwise you cannot access the Harvester cluster from Rancher. Example​ { &quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc,172.16.0.0/16&quot; } note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. caution Changing this setting might cause a short downtime for single-node clusters. log-level​ This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panicfatalerrorwarn, warninginfodebugtrace Example​ debug ntp-servers​ Available as of v1.2.0 This setting allows you to configure NTP servers for time synchronization on the Harvester nodes. Using this setting, you can define NTP servers during installation or update NTP servers after installation. caution Modifying the NTP servers will replace the previous values for all nodes. Default: &quot;&quot; Example​ { &quot;ntpServers&quot;: [ &quot;0.suse.pool.ntp.org&quot;, &quot;1.suse.pool.ntp.org&quot; ] } overcommit-config​ This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { &quot;cpu&quot;:1600, &quot;memory&quot;:150, &quot;storage&quot;:200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100mCPU for it from Kubernetes scheduler. Example​ { &quot;cpu&quot;: 1000, &quot;memory&quot;: 200, &quot;storage&quot;: 300 } release-download-url​ Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester Example of the version.yaml​ apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL} server-version​ This setting displays the version of Harvester server. Example​ v1.0.0-abcdef-head ssl-certificates​ This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example​ { &quot;ca&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;publicCertificate&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;privateKey&quot;: &quot;-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----&quot; } caution Changing this setting might cause a short downtime on single-node clusters. ssl-parameters​ This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols: Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers: Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list isECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305. Default: none note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example​ The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list toECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305. { &quot;protocols&quot;: &quot;TLSv1.2 TLSv1.3&quot;, &quot;ciphers&quot;: &quot;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305&quot; } storage-network​ By default, Longhorn uses the default management network in the Harvester cluster that is limited to a single interface and shared with other workloads cluster-wide. This setting allows you to configure a segregated storage network when network isolation is preferred. For details, please refer to the Harvester Storage Network caution Any change to storage-network requires shutting down all VMs before applying this setting. IP Range should be IPv4 CIDR format and 4 times the number of your cluster nodes. Default: &quot;&quot; Example​ { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } support-bundle-image​ Available as of v1.2.0 This setting allows you to configure the support bundle image, with various versions available in rancher/support-bundle-kit. Default: { &quot;repository&quot;: &quot;rancher/support-bundle-kit&quot;, &quot;tag&quot;: &quot;v0.0.25&quot;, &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot; } support-bundle-namespaces​ Available as of v1.2.0 This setting allows you to specify additional namespaces when collecting a support bundle. The support bundle will only capture resources from pre-defined namespaces by default. Here is the pre-defined namespaces list: cattle-dashboardscattle-fleet-local-systemcattle-fleet-systemcattle-fleet-clusters-systemcattle-monitoring-systemfleet-localharvester-systemlocallonghorn-systemcattle-logging-system If you select more namespaces, it will append to the pre-defined namespaces list. Default: none support-bundle-timeout​ Available as of v1.2.0 This setting allows you to define the number of minutes Harvester allows for the completion of the support bundle generation process. The process is considered to have failed when the data collection and file packing tasks are not completed within the configured number of minutes. Harvester will not continue or retry support bundle generation processes that have timed out. When the value is &quot;0&quot;, the timeout feature is disabled. Default: 10 upgrade-checker-enabled​ This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example​ false upgrade-checker-url​ This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example​ https://your.upgrade.checker-url/v99/checkupgrade vip-pools​ Deprecated as of v1.2.0, use IP Pool instead This setting allows you to configure the global or namespace IP address pools of the VIP by CIDR or IP range. Default: {} note Configuring multi-CIDR or IP range from UI is only available from Harvester v1.1.1. Example​ { &quot;default&quot;: &quot;172.16.0.0/24,172.16.1.0/24&quot;, &quot;demo&quot;: &quot;172.16.2.50-172.16.2.100,172.16.2.150-172.16.3.200&quot; } vm-force-reset-policy​ This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready, it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {&quot;enable&quot;:true, &quot;period&quot;:300} note When a host is unavailable or is powered off, the VM only reboots and does not migrate. Example​ { &quot;enable&quot;: &quot;true&quot;, &quot;period&quot;: 300 } UI Settings​ branding​ Available as of v1.2.0 This setting allows you to globally re-brand the UI by customizing the Harvester product name, logos, and color scheme. Default: Harvester You can set the following options and values: Private Label: This option replaces &quot;Harvester&quot; with the value you provide in most places.Logo: Upload light and dark logos to replace the Harvester logo in the top-level navigation header.Favicon: Upload an icon to replace the Harvester favicon in the browser tab.Primary Color: You can override the primary color used throughout the UI with a custom color of your choice.Link Color: You can override the link color used throughout the UI with a custom color of your choice. ui-index​ This setting allows you to configure the HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example​ https://your.static.dashboard-ui/index.html ui-plugin-index​ This setting allows you to configure the JS address for the Harvester plugin (when accessing Harvester from Rancher). Default: https://releases.rancher.com/harvester-ui/plugin/harvester-latest/harvester-latest.umd.min.js Example​ https://your.static.dashboard-ui/*.umd.min.js ui-source​ This setting allows you to configure how to load the UI source. You can set the following values: auto: The default. Auto-detect whether to use bundled UI or not.external: Use external UI source.bundled: Use the bundled UI source. Example​ external ","keywords":"","version":"v1.2"},{"title":"Install Harvester Binaries Only","type":0,"sectionRef":"#","url":"/v1.2/install/install-binaries-mode","content":"Install Harvester Binaries Only Available as of v1.2.0 The Install Harvester binaries only mode allows you to install and configure binaries only, making it ideal for cloud and edge use cases. Background​ Currently when a new Harvester node is launched it needs to be the first node in the cluster or join an existing cluster. These two modes are useful when you already know enough about the environment to install the Harvester node. However, for use cases such as bare-metal cloud providers and the edge, these installation modes load the OS and Harvester content to the node without letting you configure the network. Moreover, the K8s and networking configuration will not be applied. If you choose Install Harvester binaries only, you will need to perform additional configuration after the first bootup: Create/Join option for HarvesterManagement network interface detailsCluster tokenNode password Then, the installer will apply the endpoint configuration and boot Harvester. No further reboots will be required. Stream disk mode​ Harvester has published a raw image artifact for pre-installed Harvester. The Harvester installer now allows streaming a pre-installed image directly to disk to support better integration with cloud providers. On Equinix Metal, you can use the following kernel arguments to use the streaming mode: ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl root=live:http://${artifactEndpoint}/harvester-v1.2.0-rootfs-amd64.squashfs harvester.install.automatic=true harvester.scheme_version=1 harvester.install.device=/dev/vda harvester.os.password=password harvester.install.raw_disk_image_path=http://${artifactEndpoint}/harvester-v1.2.0-amd64.raw harvester.install.mode=install console=tty1 harvester.install.tty=tty1 harvester.install.config_url=https://metadata.platformequinix.com/userdata harvester.install.management_interface.interfaces=&quot;name:enp1s0&quot; harvester.install.management_interface.method=dhcp harvester.install.management_interface.bond_options.mode=balance-tlb harvester.install.management_interface.bond_options.miimon=100 note When streaming to disk, it is recommended to host the raw disk artifact closer to the targets, as the raw disk artifact is nearly 16G in size.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.2"},{"title":"Storage Network","type":0,"sectionRef":"#","url":"/v1.2/advanced/storagenetwork","content":"Storage Network Harvester uses Longhorn as its built-in storage system to provide block device volumes for VMs and Pods. If the user wishes to isolate Longhorn replication traffic from the Kubernetes cluster network (i.e. the management network) or other cluster-wide workloads. Users can allocate a dedicated storage network for Longhorn replication traffic to get better network bandwidth and performance. For more information, please see Longhorn Storage Network note Configuring Longhorn settings directly is not recommended, as this can lead to untested situations. Prerequisites​ There are some prerequisites before configuring the Harvester Storage Network setting. Well-configured Cluster Network and VLAN Config. Users have to ensure the Cluster Network is configured and VLAN Config will cover all nodes and ensure the network connectivity is working and expected in all nodes. All VMs should be stopped. We recommend checking the VM status with the following command and should get an empty result.kubectl get -A vmi All pods that are attached to Longhorn Volumes should be stopped. Users could skip this step with the Harvester Storage Network setting. Harvester will stop Longhorn-related pods automatically. All ongoing image uploads or downloads should be either completed or deleted. caution If the Harvester cluster was upgraded from v1.0.3, please check if Whereabouts CNI is installed properly before you move on to the next step. We will always recommend following this guide to check. Issue 3168 describes that the Harvester cluster will not always install Whereabouts CNI properly. Verify the ippools.whereabouts.cni.cncf.io CRD exists with the following command. kubectl get crd ippools.whereabouts.cni.cncf.io If the Harvester cluster doesn't have ippools.whereabouts.cni.cncf.io, please add these two CRDs before configuring storage-network setting. kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_ippools.yaml kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml Configuration Example​ VLAN ID Please check with your network switch setting, and provide a dedicated VLAN ID for Storage Network. Well-configured Cluster Network and VLAN Config Please refer Networking page for more details and configure Cluster Network and VLAN Config but not Networks. IP range for Storage Network IP range should not conflict or overlap with Kubernetes cluster networks(10.42.0.0/16, 10.43.0.0/16, 10.52.0.0/16 and 10.53.0.0/16 are reserved).IP range should be in IPv4 CIDR format and Longhorn pods use Storage Network as follows: instance-manger-e and instance-manager-r pods: These require 2 IPs per node. During an upgrade, two versions of these pods will exist (old and new), and the old version will be deleted once the upgrade is successful.backing-image-ds pods: These are employed to process on-the-fly uploads and downloads of backing image data sources. These pods will be removed once the image uploads or downloads are completed.backing-image-manager pods: 1 IP per disk, similar to the instance manager pods. Two versions of these will coexist during an upgrade, and the old ones will be removed after the upgrade is completed.The required number of IPs is calculated using a simple formula: Required Number of IPs = Number of Nodes * 4 + Number of Disks * 2 + Number of Images to Download/Upload For example, if your cluster has five nodes, each node has two disks, and ten images will be uploaded simultaneously, the IP range should be greater than or equal to /26 (5 * 4 + 5 * 2 * 2 + 10 = 50). We will take the following configuration as an example to explain the details of the Storage Network VLAN ID for Storage Network: 100Cluster Network: storageIP range: 192.168.0.0/24 Configuration Process​ Harvester will create Multus NetworkAttachmentDefinition from the configuration, stop pods related to Longhorn Volume, update Longhorn setting, and restart previous pods. Before Applying Harvester Storage Network Setting​ Here we have two cases. Expect that VM VLAN traffic and Longhorn Storage Network use the same group of physical interfaces.Expect that VM VLAN traffic and Longhorn Storage Network use different physical interfaces. Longhorn will send replication traffic through the specific interfaces shown as the red line in the figure. Same Physical Interfaces​ Take eth2 and eth3 as an example for VM VLAN traffic and Longhorn Storage Network simultaneously. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth2 and eth3 and remember the ClusterNetwork name for the further step. Different Physical Interfaces​ eth2 and eth3 are for VM VLAN Traffic. eth4 and eth5 are for Longhorn Storage Network. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth4 and eth5 for Storage Network and remember the ClusterNetwork name for the further step. Harvester Storage Network Setting​ Harvester Storage Network setting will need range, clusterNetwork, vlan field to construct Multus NetworkAttachmentDefinition for Storage Network usage. You could apply this setting via Web UI or CLI. Web UI​ Harvester Storage Network setting could be easily modified on the Settings &gt; storage-network page. CLI​ Users could use this command to edit Harvester Storage Network setting. kubectl edit settings.harvesterhci.io storage-network The value format is JSON string or empty string as shown in below. { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } The full configuration will be like this example. apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: name: storage-network value: '{&quot;vlan&quot;:100,&quot;clusterNetwork&quot;:&quot;storage&quot;,&quot;range&quot;:&quot;192.168.0.0/24&quot;}' caution Because of the design, Harvester will treat extra and insignificant characters in JSON string as a different configuration. After Applying Harvester Storage Network Setting​ After applying Harvester's Storage Network setting, Harvester will stop all pods that are related to Longhorn volumes. Currently, Harvester has some pods listed below that will be stopped during setting. PrometheusGrafanaAlertmanagerVM Import Controller Harvester will also create a new NetworkAttachmentDefinition and update the Longhorn Storage Network setting. Once the Longhorn setting is updated, Longhorn will restart all instance-manager-r, instance-manager-e, and backing-image-manager pods to apply the new network configuration, and Harvester will restart the pods. note Harvester will not start VM automatically. Users should check whether the configuration is completed or not in the next section and start VM manually on demand. Verify Configuration is Completed​ Step 1​ Check if Harvester Storage Network setting's status is True and the type is configured. kubectl get settings.harvesterhci.io storage-network -o yaml Completed Setting Example: apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: annotations: storage-network.settings.harvesterhci.io/hash: da39a3ee5e6b4b0d3255bfef95601890afd80709 storage-network.settings.harvesterhci.io/net-attach-def: &quot;&quot; storage-network.settings.harvesterhci.io/old-net-attach-def: &quot;&quot; creationTimestamp: &quot;2022-10-13T06:36:39Z&quot; generation: 51 name: storage-network resourceVersion: &quot;154638&quot; uid: 2233ad63-ee52-45f6-a79c-147e48fc88db status: conditions: - lastUpdateTime: &quot;2022-10-13T13:05:17Z&quot; reason: Completed status: &quot;True&quot; type: configured Step 2​ Verify the readiness of all Longhorn instance-manager-e, instance-manager-r, and backing-image-manager pods, and confirm that their networks are correctly configured. Execute the following command to inspect a pod's details: kubectl -n longhorn-system describe pod &lt;pod-name&gt; If you encounter an event resembling the following one, the Storage Network might have run out of its available IPs: Events: Type Reason Age From Message ---- ------ ---- ---- ------- .... Warning FailedCreatePodSandBox 2m58s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox &quot;04e9bc160c4f1da612e2bb52dadc86702817ac557e641a3b07b7c4a340c9fc48&quot;: plugin type=&quot;multus&quot; name=&quot;multus-cni-network&quot; failed (add): [longhorn-system/ba cking-image-ds-default-image-lxq7r/7d6995ee-60a6-4f67-b9ea-246a73a4df54:storagenetwork-sdfg8]: error adding container to network &quot;storagenetwork-sdfg8&quot;: erro r at storage engine: Could not allocate IP in range: ip: 172.16.0.1 / - 172.16.0.6 / range: net.IPNet{IP:net.IP{0xac, 0x10, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0xff, 0xf8}} .... Please reconfigure the Storage Network with a sufficient IP range. note If the Storage Network has run out of IPs, you might encounter the same error when you upload/download images. Please delete the related images and reconfigure the Storage Network with a sufficient IP range. Step 3​ Check the k8s.v1.cni.cncf.io/network-status annotations and ensure that an interface named lhnet1 exists, with an IP address within the designated IP range. Users could use the following command to show all Longhorn Instance Manager to verify. kubectl get pods -n longhorn-system -l longhorn.io/component=instance-manager -o yaml Correct Network Example: apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/containerID: 2518b0696f6635896645b5546417447843e14208525d3c19d7ec6d7296cc13cd cni.projectcalico.org/podIP: 10.52.2.122/32 cni.projectcalico.org/podIPs: 10.52.2.122/32 k8s.v1.cni.cncf.io/network-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] k8s.v1.cni.cncf.io/networks: '[{&quot;namespace&quot;: &quot;harvester-system&quot;, &quot;name&quot;: &quot;storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;}]' k8s.v1.cni.cncf.io/networks-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] kubernetes.io/psp: global-unrestricted-psp longhorn.io/last-applied-tolerations: '[{&quot;key&quot;:&quot;kubevirt.io/drain&quot;,&quot;operator&quot;:&quot;Exists&quot;,&quot;effect&quot;:&quot;NoSchedule&quot;}]' Omitted... Start VM Manually​ After verifying the configuration, users could start VM manually on demand.","keywords":"","version":"v1.2"},{"title":"PXE Boot Installation","type":0,"sectionRef":"#","url":"/v1.2/install/pxe-boot-install","content":"PXE Boot Installation Starting from version 0.2.0, Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples. Prerequisite​ info Nodes need to have at least 8 GB of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers​ An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10, and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/. Preparing Boot Files​ Download the required files from the Harvester releases page. The ISO: harvester-&lt;version&gt;-amd64.isoThe kernel: harvester-&lt;version&gt;-vmlinuz-amd64The initrd: harvester-&lt;version&gt;-initrd-amd64The rootfs squashfs image: harvester-&lt;version&gt;-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts​ When performing an automatic installation, there are two modes: CREATE: we are installing a node to construct an initial Harvester cluster.JOIN: we are installing a node to join an existing Harvester cluster. CREATE Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml scheme_version: 1 token: token # Replace with a desired token os: hostname: node1 # Set a hostname. This can be omitted if DHCP server offers hostnames ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install: mode: create management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console vip: 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use vip_mode: static # Or dhcp, check configuration file for more information # vip_hw_addr: 52:54:00:ec:0e:0b # Leave empty when vip_mode is static For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create. note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp). See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter. JOIN Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml scheme_version: 1 server_url: https://10.100.0.99:443 # Should be the VIP set up in &quot;CREATE&quot; config token: token os: hostname: node2 ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password dns_nameservers: - 1.1.1.1 - 8.8.8.8 install: mode: join management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join. DHCP Server Configuration​ note In the PXE installation scenario, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. In the ISO installation scenario, when the management network interface is in DHCP mode, you are also required to add the routers option (option routers) when configuring the DHCP server. For example: Harvester Host:~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see ISC DHCPv4 Option Configuration. The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16; subnet 10.100.0.0 netmask 255.255.255.0 { option routers 10.100.0.10; option domain-name-servers 192.168.2.1; range 10.100.0.100 10.100.0.253; } group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } group { # join group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-join-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-join&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node2 { hardware ethernet 52:54:00:69:d5:92; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration​ For more information about Harvester configuration, please refer to the Harvester configuration page. By default, the first node will be the management node of the cluster. When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config. In this case, at least three different zones are required. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support​ UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program​ Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration​ If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } elsif substring (option vendor-class-identifier, 0, 10) = &quot;HTTPClient&quot; { # UEFI HTTP Boot option vendor-class-identifier &quot;HTTPClient&quot;; filename &quot;http://10.100.0.10/harvester/ipxe.efi&quot;; } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi. The iPXE Script for UEFI Boot​ It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-&lt;version&gt;-vmlinuz initrd=harvester-&lt;version&gt;-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot The parameter initrd=harvester-&lt;version&gt;-initrd is required. Useful Kernel Parameters​ Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7). ip=dhcp​ If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces. rd.net.dhcp.retry=&lt;cnt&gt;​ Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=&lt;cnt&gt;to retry DHCP request for &lt;cnt&gt; times.","keywords":"Harvester harvester Rancher rancher Install Harvester Installing Harvester Harvester Installation PXE Boot Install","version":"v1.2"},{"title":"Management Address","type":0,"sectionRef":"#","url":"/v1.2/install/management-address","content":"Management Address Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address​ To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath='{.metadata.annotations}' Example of output: {&quot;kube-vip.io/hwaddr&quot;:&quot;02:00:00:09:7f:3f&quot;,&quot;kube-vip.io/requestedIP&quot;:&quot;10.84.102.31&quot;} Usages​ The management address: Allows the access to the Harvester API/UI via HTTPS protocol.Allows other nodes to join the cluster.","keywords":"VIP","version":"v1.2"},{"title":"Hardware and Network Requirements","type":0,"sectionRef":"#","url":"/v1.2/install/requirements","content":"Hardware and Network Requirements As an HCI solution on bare metal servers, there are minimum node hardware and network requirements for installing and running Harvester. A three-node cluster is required to fully realize the multi-node features of Harvester. The first node that is added to the cluster is by default the management node. When the cluster has three or more nodes, the two nodes added after the first are automatically promoted to management nodes to form a high availability (HA) cluster. Certain versions of Harvester support the deployment of single-node clusters. Such clusters do not support high availability, multiple replicas, and live migration. Hardware Requirements​ Harvester nodes have the following hardware requirements and recommendations for installation and testing. Type\tRequirements and RecommendationsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum for testing; 16-core or above required for production Memory\t32 GB minimum for testing; 64 GB or above required for production Disk Capacity\t250 GB minimum for testing (180 GB minimum when using multiple disks); 500 GB or above required for production Disk Performance\t5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for etcd Network Card\t1 Gbps Ethernet minimum for testing; 10Gbps Ethernet required for production Network Switch\tTrunking of ports required for VLAN support important Use server-class hardware to achieve the best results. Laptops and nested virtualization are not supported.Each node must have a unique product_uuid (fetched from /sys/class/dmi/id/product_uuid) to prevent errors from occurring during VM live migration and other operations. For more information, see Issue #4025. Network Requirements​ Harvester nodes have the following network requirements for installation. Port Requirements for Harvester Nodes​ Harvester nodes require the following port connections or inbound rules. Typically, all outbound traffic is allowed. Protocol\tPort\tSource\tDescriptionTCP\t2379\tHarvester management nodes\tEtcd client port TCP\t2381\tHarvester management nodes\tEtcd health checks TCP\t2380\tHarvester management nodes\tEtcd peer port TCP\t10010\tHarvester management and compute nodes\tContainerd TCP\t6443\tHarvester management nodes\tKubernetes API TCP\t9345\tHarvester management nodes\tKubernetes API TCP\t10252\tHarvester management nodes\tKube-controller-manager health checks TCP\t10257\tHarvester management nodes\tKube-controller-manager secure port TCP\t10251\tHarvester management nodes\tKube-scheduler health checks TCP\t10259\tHarvester management nodes\tKube-scheduler secure port TCP\t10250\tHarvester management and compute nodes\tKubelet TCP\t10256\tHarvester management and compute nodes\tKube-proxy health checks TCP\t10258\tHarvester management nodes\tCloud-controller-manager TCP\t9091\tHarvester management and compute nodes\tCanal calico-node felix TCP\t9099\tHarvester management and compute nodes\tCanal CNI health checks UDP\t8472\tHarvester management and compute nodes\tCanal CNI with VxLAN TCP\t2112\tHarvester management nodes\tKube-vip TCP\t6444\tHarvester management and compute nodes\tRKE2 agent TCP\t6060\tHarvester management and compute nodes\tNode-disk-manager TCP\t10246/10247/10248/10249\tHarvester management and compute nodes\tNginx worker process TCP\t8181\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t8444\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t10245\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t80\tHarvester management and compute nodes\tNginx TCP\t9796\tHarvester management and compute nodes\tNode-exporter TCP\t30000-32767\tHarvester management and compute nodes\tNodePort port range TCP\t22\tHarvester management and compute nodes\tsshd UDP\t68\tHarvester management and compute nodes\tWicked TCP\t3260\tHarvester management and compute nodes\tiscsid Port Requirements for Integrating Harvester with Rancher​ If you want to integrate Harvester with Rancher, you need to make sure that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. When provisioning VMs with Kubernetes clusters from Rancher into Harvester, you need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise, the cluster won't be manageable by Rancher. For more information, refer to Rancher Architecture. Port Requirements for K3s or RKE/RKE2 Clusters​ For the port requirements for guest clusters deployed inside Harvester VMs, refer to the following links: K3s NetworkingRKE PortsRKE2 Networking","keywords":"Installation Requirements","version":"v1.2"},{"title":"Update Harvester Configuration After Installation","type":0,"sectionRef":"#","url":"/v1.2/install/update-harvester-configuration","content":"Update Harvester Configuration After Installation Harvester's OS has an immutable design, which means most files in the OS revert to their pre-configured state after a reboot. The Harvester OS loads the pre-configured values of system components from configuration files during the boot time. This page describes how to edit some of the most-requested Harvester configurations. To update a configuration, you must first update the runtime value in the system and then update configuration files to make the changes persistent between reboots. note If you upgrade from a version before v1.1.2, the cloud-init file in examples will be /oem/99_custom.yaml. Please substitute the value if needed. DNS servers​ Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Edit /etc/sysconfig/network/config and update the following line. Use a space to separate DNS server addresses if there are multiple servers. NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot; Update and reload the configuration with the following command: netconfig update Confirm the file /etc/resolv.conf contains the correct DNS servers with the cat command: cat /etc/resolv.conf Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the value under the yaml path stages.initramfs[0].commands. The commands array must contain a line to manipulate the NETCONFIG_DNS_STATIC_SERVERS config. Add the line if the line doesn't exist. The following example adds a line to change the NETCONFIG_DNS_STATIC_SERVERS config: stages: initramfs: - commands: - sed -i 's/^NETCONFIG_DNS_STATIC_SERVERS.*/NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot;/' /etc/sysconfig/network/config Replace the DNS server addresses and save the file. Harvester sets up new servers after rebooting. NTP servers​ We introduce the new mechanism for the NTP configuration in Harvester v1.2.0. For more information about NTP settings in Harvester v1.2.0 and later versions, see the NTP servers. SSH keys of user rancher​ Runtime change​ Log in to a Harvester node as user rancher. See how to log into a Harvester node for more details.Edit /home/rancher/.ssh/authorized_keys to add or remove keys. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.network[0].authorized_keys.rancher. Add or remove keys in the rancher array: stages: network: - ... authorized_keys: rancher: - key1 - key2 Password of user rancher​ Runtime change​ Log in to a Harvester node as user rancher. See how to log into a Harvester node for more details.To reset the password for the user rancher, run the command passwd. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].users.rancher.passwd. Refer to the configuration os.password for details on how to specify the password in an encrypted form. Bonding slaves​ You can update the slave interfaces of Harvester's management bonding interface mgmt-bo. Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Identify the interface names with the following command: ip a Edit /etc/sysconfig/network/ifcfg-mgmt-bo and update the lines associated with bonding slaves and bonding mode: BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' Restart the network with the wicked ifreload command: wicked ifreload mgmt-bo caution A mistake in the configuration may disrupt the SSH session. Configuration persistence​ Beginning with v1.1.2, the persistent name of the cloud-init file is /oem/90_custom.yaml. Harvester now uses a newer version of Elemental, which creates the file during installation. When upgrading from an earlier version to v1.1.2 or later, Harvester retains the old file name (/oem/99_custom.yaml) to avoid confusion. You can manually rename the file to /oem/90_custom.yaml if necessary. Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].files. More specifically, update the content of the /etc/sysconfig/network/ifcfg-mgmt-bo file and edit the BONDING_SLAVE_X and BONDING_MODULE_OPTS entries accordingly: stages: initramfs: - ... files: - path: /etc/sysconfig/network/ifcfg-mgmt-bo permissions: 384 owner: 0 group: 0 content: |+ STARTMODE='onboot' BONDING_MASTER='yes' BOOTPROTO='none' POST_UP_SCRIPT=&quot;wicked:setup_bond.sh&quot; BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' DHCLIENT_SET_DEFAULT_ROUTE='no' encoding: &quot;&quot; ownerstring: &quot;&quot; - path: /etc/sysconfig/network/ifcfg-ens6 permissions: 384 owner: 0 group: 0 content: | STARTMODE='hotplug' BOOTPROTO='none' encoding: &quot;&quot; ownerstring: &quot;&quot; note If you didn't select an interface during installation, you must add an entry to initialize the interface. Please check the /etc/sysconfig/network/ifcfg-ens6 file creation in the above example. The file name should be /etc/sysconfig/network/ifcfg-&lt;interface-name&gt;.","keywords":"Harvester configuration Configuration","version":"v1.2"},{"title":"Harvester Configuration","type":0,"sectionRef":"#","url":"/v1.2/install/harvester-configuration","content":"Harvester Configuration Configuration Example​ Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: scheme_version: 1 server_url: https://cluster-VIP:443 token: TOKEN_VALUE os: ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files: - encoding: &quot;&quot; content: test content owner: root path: /etc/test.txt permissions: '0755' hostname: myhost modules: - kvm - nvme sysctls: kernel.printk: &quot;4 4 1 7&quot; kernel.kptr_restrict: &quot;1&quot; dns_nameservers: - 8.8.8.8 - 1.1.1.1 ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password: rancher environment: http_proxy: http://myserver https_proxy: http://myserver labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7C&quot; method: dhcp force_efi: true device: /dev/sda data_disk: /dev/sdb silent: true iso_url: http://myserver/test.iso poweroff: true no_format: true debug: true tty: ttyS0 vip: 10.10.0.19 vip_hw_addr: 52:54:00:ec:0e:0b vip_mode: dhcp force_mbr: false addons: harvester_vm_import_controller: enabled: false values_content: &quot;&quot; harvester_pcidevices_controller: enabled: false values_content: &quot;&quot; rancher_monitoring: enabled: true values_content: &quot;&quot; rancher_logging: enabled: false values_content: &quot;&quot; harvester_seeder: enabled: false values_content: &quot;&quot; system_settings: auto-disk-provision-paths: &quot;&quot; Configuration Reference​ Below is a reference of all configuration keys. caution Security Risks: The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. note Configuration Priority: When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname. scheme_version​ Definition​ The version of scheme reserved for future configuration migration. This configuration is mandatory for migrating the configuration to a new scheme version. It tells Harvester the previous version and the need to migrate. note This field didn't take any effect in the current Harvester version. caution Make sure that your custom configuration always has the correct scheme version. server_url​ Definition​ server_url is the URL of the Harvester cluster, which is used for the new node to join the cluster. This configuration is mandatory when the installation is in JOIN mode. The default format of server_url is https://cluster-VIP:443. note To ensure a high availability (HA) Harvester cluster, please use either the Harvester cluster VIP or a domain name in server_url. Example​ server_url: https://cluster-VIP:443 install: mode: join token​ Definition​ The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example​ token: myclustersecret Or a node token token: &quot;K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4&quot; os.ssh_authorized_keys​ Definition​ A list of SSH authorized keys that should be added to the default user, rancher. SSH keys can be obtained from GitHub user accounts by using the formatgithub:${USERNAME}. This is done by downloading the keys from https://github.com/${USERNAME}.keys. Example​ os: ssh_authorized_keys: - &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D&quot; - &quot;github:ibuildthecloud&quot; os.write_files​ A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: &quot;&quot;: content data are written in plain text. In this case, the encoding field can be also omitted.b64, base64: content data are base64-encoded.gz, gzip: content data are gzip-compressed.gz+base64, gzip+base64, gz+b64, gzip+b64: content data are gzip-compressed first and then base64-encoded. Example os: write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner: root:root path: /etc/connman/main.conf permissions: '0644' - content: | # My new /etc/sysconfig/samba file SMDBOPTIONS=&quot;-D&quot; path: /etc/sysconfig/samba - content: !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path: /bin/arch permissions: '0555' - content: | 15 * * * * root ship_logs path: /etc/crontab os.persistent_state_paths​ Definition​ The os.persistent_state_paths option allows you to configure custom paths where modifications made to files will persist across reboots. Any changes to files in these paths will not be lost after a reboot. Example​ Refer to the following example config for installing rook-ceph in Harvester: os: persistent_state_paths: - /var/lib/rook - /var/lib/ceph modules: - rbd - nbd os.after_install_chroot_commands​ Definition​ You can add additional software packages with after_install_chroot_commands. The after-install-chroot stage, provided by elemental-toolkit, allows you to execute commands not restricted by file system write issues, ensuring the persistence of user-defined commands even after a system reboot. Example​ Refer to the following example config for installing an RPM package in Harvester: os: after_install_chroot_commands: - rpm -ivh &lt;the url of rpm package&gt; DNS resolution is unavailable in the after-install-chroot stage, and the nameserver might not be available. If you need to access a domain name to install a package using an URL, create a temporary /etc/resolv.conf file first. For example: os: after_install_chroot_commands: - &quot;rm -f /etc/resolv.conf &amp;&amp; echo 'nameserver 8.8.8.8' | sudo tee /etc/resolv.conf&quot; - &quot;mkdir /usr/local/bin&quot; - &quot;curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 &amp;&amp; chmod 700 get_helm.sh &amp;&amp; ./get_helm.sh&quot; - &quot;rm -f /etc/resolv.conf &amp;&amp; ln -s /var/run/netconfig/resolv.conf /etc/resolv.conf&quot; note Upgrading Harvester causes the changes to the OS in the after-install-chroot stage to be lost. You must also configure the after-upgrade-chroot to make your changes persistent across an upgrade. Refer to Runtime persistent changes before upgrading Harvester. os.hostname​ Definition​ Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value. Example​ os: hostname: myhostname os.modules​ Definition​ A list of kernel modules to be loaded on start. Example​ os: modules: - kvm - nvme os.sysctls​ Definition​ Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf. Values must be specified as strings. Example​ os: sysctls: kernel.printk: 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict: &quot;1&quot; # force the YAML parser to read as a string os.dns_nameservers​ Definition​ Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example​ os: dns_nameservers: - 8.8.8.8 - 1.1.1.1 os.ntp_servers​ Definition​ Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines. Example​ os: ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org os.password​ Definition​ The password for the default user, rancher. By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from/etc/shadow. You can also encrypt a password using OpenSSL. For the encryption algorithms supported by Harvester, please refer to the table below. Algorithm\tCommand\tSupportSHA-512\topenssl passwd -6\tYes SHA-256\topenssl passwd -5\tYes MD5\topenssl passwd -1\tYes MD5, Apache variant\topenssl passwd -apr1\tYes AIX-MD5\topenssl passwd -aixmd5\tNo Example​ Encrypted: os: password: &quot;$6$kZYUnRaTxNdg4W8H$WSEJydGWsNpaRbbbRdTDLJ2hDLbkizxSFGW2RtexlqG6njEATaGQG9ssztjaKDCsaNUPBZ1E1YdsvSLMAi/IO/&quot; Or clear text: os: password: supersecure os.environment​ Definition​ Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example​ os: environment: http_proxy: http://myserver https_proxy: http://myserver note This example sets the HTTP(S) proxy for foundational OS components. To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy. os.labels​ Definition​ Labels to be added to this Node. Example​ os: labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue os.sshd.sftp​ since v1.2.2 Definition​ Subsystem used to configure the OpenSSH Daemon (sshd). Harvester currently only supports sftp. Example​ os: sshd: sftp: true # The SFTP subsystem is enabled. install.mode​ Definition​ Harvester installation mode: create: Creating a new Harvester installation.join: Join an existing Harvester installation. Need to specify server_url. Example​ install: mode: create install.management_interface​ Definition​ Configure network interfaces for the host machine. Valid configuration fields are: method: Method to assign an IP to this network. The following are supported: static: Manually assign an IP and gateway.dhcp: Request an IP from the DHCP server. ip: Static IP for this network. Required if static method is chosen.subnet_mask: Subnet mask for this network. Required if static method is chosen.gateway: Gateway for this network. Required if static method is chosen.interfaces: An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name: The name of the slave interface for the bonded network.interfaces.hwAddr: The hardware MAC address of the interface. bond_options: Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlbmiimon: 100 mtu: The MTU for the interface.vlan_id: The VLAN ID for the interface. note Harvester uses the systemd net naming scheme. Please make sure the interface name is present on the target machine before installation. Example​ install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7D&quot; # The hwAddr is optional method: dhcp bond_options: mode: balance-tlb miimon: 100 mtu: 1492 vlan_id: 101 install.force_efi​ Force EFI installation even when EFI is not detected. Default: false. install.device​ The device to install the OS. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. install.silent​ Reserved. install.iso_url​ ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff​ Shutdown the machine after installation instead of rebooting install.no_format​ Do not partition and format, assume layout exists already. install.debug​ Run the installation with additional logging and debugging enabled for the installed system. install.persistent_partition_size​ Definition​ Configure the size of partition COS_PERSISTENT in Gi or Mi. This partition is used to store data like system packages and container images. The default and minimum value is 150Gi. Example​ install: persistent_partition_size: 150Gi install.tty​ Definition​ The tty device used for the console. Example​ install: tty: ttyS0,115200n8 install.vip​ install.vip_mode​ install.vip_hw_addr​ Definition​ install.vip: The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://&lt;VIP&gt;.install.vip_mode dhcp: Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided.static: Harvester uses a static VIP. install.vip_hw_addr: The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp. See Management Address for more information. Example​ Configure a static VIP. install: vip: 192.168.0.100 vip_mode: static Configure a DHCP VIP. install: vip: 10.10.0.19 vip_mode: dhcp vip_hw_addr: 52:54:00:ec:0e:0b install.force_mbr​ Definition​ By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. note Harvester creates an additional partition for storing VM data ifinstall.data_disk is configured to use the same storage device as the one set for install.device. When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example​ install: force_mbr: true install.data_disk​ Available as of v1.0.1 Definition​ Sets the default storage device to store the VM data. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. Default: Same storage device as the one set for install.device Example​ install: data_disk: /dev/sdb install.addons​ Available as of v1.2.0 Definition​ Sets the default enabled/disabled status of Harvester addons. Default: The addons are disabled. Example​ install: addons: rancher_monitoring: enabled: true rancher_logging: enabled: false Harvester v1.2.0 ships with five addons: vm-import-controller (chartName: harvester-vm-import-controller)pcidevices-controller (chartName: harvester-pcidevices-controller)rancher-monitoringrancher-loggingharvester-seeder (experimental) install.harvester.storage_class.replica_count​ Available as of v1.1.2 Definition​ Sets the replica count of Harvester's default storage class harvester-longhorn. Default: 3 Supported values: 1, 2, 3. All other values are considered 3. In edge scenarios where users may deploy single-node Harvester clusters, they can set this value to 1. In most scenarios, it is recommended to keep the default value 3 for storage high availability. Please refer to longhorn-replica-count for more details. Example​ install: harvester: storage_class: replica_count: 1 install.harvester.longhorn.default_settings.guaranteedEngineManagerCPU​ Available as of v1.2.0 Definition​ Sets the default percentage of the total allocatable CPU on each node will be reserved for each Longhorn engine manager Pod. Default: 12 Supported values: 0-12. All other values are considered 12. This integer value indicates what percentage of the total allocatable CPU on each node will be reserved for each engine manager Pod. In edge scenarios where users may deploy single-node Harvester clusters, they can set this parameter to a value smaller than 12. In most scenarios, it is recommended to keep the default value for system high availability. Before setting the value, please refer to longhorn-guaranteed-engine-manager-cpu for more details. Example​ install: harvester: longhorn: default_settings: guaranteedEngineManagerCPU: 6 install.harvester.longhorn.default_settings.guaranteedReplicaManagerCPU​ Available as of v1.2.0 Definition​ Sets the default percentage of the total allocatable CPU on each node will be reserved for each Longhorn replica manager Pod. Default: 12 Supported values: 0-12. All other values are considered 12. This integer value indicates what percentage of the total allocatable CPU on each node will be reserved for each replica manager Pod. In edge scenarios where users may deploy single-node Harvester clusters, can set this parameter to a value smaller than 12. In most scenarios, it is recommended to keep the default value for system high availability. Before setting the value, please refer to longhorn-guaranteed-replica-manager-cpu for more details. Example​ install: harvester: longhorn: default_settings: guaranteedReplicaManagerCPU: 6 system_settings​ Definition​ You can overwrite the default Harvester system settings by configuring system_settings. See the Settings page for additional information and the list of all the options. note Overwriting system settings only works when Harvester is installed in &quot;create&quot; mode. If you install Harvester in &quot;join&quot; mode, this setting is ignored. Installing in &quot;join&quot; mode will adopt the system settings from the existing Harvester system. Example​ The example below overwrites containerd-registry, http-proxy and ui-source settings. The values must be a string. system_settings: containerd-registry: '{&quot;Mirrors&quot;: {&quot;docker.io&quot;: {&quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;]}}, &quot;Configs&quot;: {&quot;myregistry.local:5000&quot;: {&quot;Auth&quot;: {&quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot;}, &quot;TLS&quot;: {&quot;InsecureSkipVerify&quot;: false}}}}' http-proxy: '{&quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc&quot;}' ui-source: auto ","keywords":"Harvester harvester Rancher rancher Harvester Configuration","version":"v1.2"},{"title":"ISO Installation","type":0,"sectionRef":"#","url":"/v1.2/install/index","content":"ISO Installation Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image. To get the ISO image, download 💿 harvester-v1.x.x-amd64.iso from the Harvester releases page. During the installation, you can either choose to create a new Harvester cluster or join the node to an existing Harvester cluster. The following video shows a quick overview of an ISO installation. Installation Steps​ Mount the Harvester ISO file and boot the server by selecting the Harvester Installer option. Use the arrow keys to choose an installation mode. By default, the first node will be the management node of the cluster. Create a new Harvester cluster: creates an entirely new Harvester cluster. Join an existing Harvester cluster: joins an existing Harvester cluster. You need the VIP and cluster token of the cluster you want to join. Install Harvester binaries only: If you choose this option, additional setup is required after the first bootup. info When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config by providing a URL of Harvester configuration on the customize the host step. In this case, at least three different zones are required. Choose the installation disk you want to install the Harvester cluster on and the data disk you want to store VM data on. By default, Harvester uses GUID Partition Table (GPT) partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select Master boot record (MBR). Installation disk: The disk to install the Harvester cluster on. Data disk: The disk to store VM data on. Choosing a separate disk to store VM data is recommended.Persistent size: If you only have one disk or use the same disk for both OS and VM data, you need to configure persistent partition size to store system packages and container images. The default and minimum persistent partition size is 150 GiB. You can specify a size like 200Gi or 153600Mi. Configure the HostName of the node. Configure network interface(s) for the management network. By default, Harvester creates a bonded NIC named mgmt-bo, and the IP address can be configured via DHCP or statically assigned. note It is not possible to change the node IP throughout the lifecycle of a Harvester cluster. If using DHCP, you must ensure the DHCP server always offers the same IP for the same node. If the node IP is changed, the related node cannot join the cluster and might even break the cluster. In addition, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. For example: Linux~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see DHCP Server Configuration. (Optional) Configure the DNS Servers. Use commas as a delimiter to add more DNS servers. Leave it blank to use the default DNS server. Configure the virtual IP (VIP) by selecting a VIP Mode. This VIP is used to access the cluster or for other nodes to join the cluster. note If using DHCP to configure the IP address, you need to configure a static MAC-to-IP address mapping on your DHCP server to have a persistent virtual IP (VIP), and the VIP must be unique. Configure the Cluster token. This token is used for adding other nodes to the cluster. Configure and confirm a Password to access the node. The default SSH user is rancher. Configure NTP servers to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org. Use commas as a delimiter to add more NTP servers. (Optional) If you need to use an HTTP proxy to access the outside world, enter the Proxy address. Otherwise, leave this blank. (Optional) You can choose to import SSH keys by providing HTTP URL. For example, your GitHub public keys https://github.com/&lt;username&gt;.keys can be used. (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Review and confirm your installation options. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, your node restarts. After the restart, the Harvester console displays the management URL and status. The default URL of the web interface is https://your-virtual-ip. You can use F12 to switch from the Harvester console to the Shell and type exit to go back to the Harvester console. note Choosing Install Harvester binaries only on the first page requires additional setup after the first bootup. You will be prompted to set the password for the default admin user when logging in for the first time. Known Issue​ Installer may crash when using an older graphics card/monitor​ In some cases, if you are using an older graphics card/monitor, you may encounter a panic: invalid dimensions error during ISO installation. We are working on this known issue and planning a fix for a future release. You can try to use another GRUB entry to force it to use the resolution of 1024x768 when booting up. If you are using a version earlier than v1.1.1, please try the following workaround: Boot up with the ISO, and press E to edit the first menu entry: Append vga=792 to the line started with $linux: Press Ctrl+X or F10 to boot up. Fail to join nodes using FQDN to a cluster which has custom SSL certificate configured​ You may encounter that newly joined nodes stay in the Not Ready state indefinitely. This is likely the outcome if you already have a set of custom SSL certificates configured on the to-be-joined Harvester cluster and provide an FQDN instead of a VIP address for the management address during the Harvester installation. You can check the SSL certificates on the Harvester dashboard's setting page or using the command line tool kubectl get settings.harvesterhci.io ssl-certificates to see if there is any custom SSL certificate configured (by default, it is empty). The second thing to look at is the joining nodes. Try to get access to the nodes via consoles or SSH sessions and then check the log of rancherd: $ journalctl -u rancherd.service Oct 02 08:04:43 node-0 systemd[1]: Starting Rancher Bootstrap... Oct 02 08:04:43 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:43Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/50-defaults.yaml]&quot; Oct 02 08:04:43 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:43Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/91-harvester-bootstrap-repo.yaml]&quot; Oct 02 08:04:43 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:43Z&quot; level=info msg=&quot;Loading config file [/etc/rancher/rancherd/config.yaml]&quot; Oct 02 08:04:43 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:43Z&quot; level=info msg=&quot;Bootstrapping Rancher (v2.7.5/v1.25.9+rke2r1)&quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;Writing plan file to /var/lib/rancher/rancherd/plan/plan.json&quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;Applying plan with checksum &quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231002-080444-applied.plan/_0&quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;Running command: /usr/bin/env [sh /var/lib/rancher/rancherd/install.sh]&quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent configuration directory /etc/rancher/agent&quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent var directory /var/lib/rancher/agent&quot; Oct 02 08:04:44 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:44Z&quot; level=info msg=&quot;[stderr]: [WARN] /usr/local is read-only or a mount point; installing to /opt/rancher-system-agent&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stdout]: [INFO] Determined CA is necessary to connect to Rancher&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded CA certificate&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stdout]: [INFO] Value from https://192.168.48.240/cacerts is an x509 certificate&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: curl: (60) SSL: no alternative certificate subject name matches target host name '192.168.48.240'&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: More details here: https://curl.se/docs/sslcerts.html&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: &quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: curl failed to verify the legitimacy of the server and therefore could not&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: establish a secure connection to it. To learn more about this situation and&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: how to fix it, please visit the web page mentioned above.&quot; Oct 02 08:04:45 node-0 rancherd[2017]: time=&quot;2023-10-02T08:04:45Z&quot; level=info msg=&quot;[stderr]: [ERROR] 000 received while testing Rancher connection. Sleeping for 5 seconds and trying again&quot; This is because rancherd will try to download the CA using the provided FQDN in the insecure mode from the embedded Rancher Manager on the Harvester cluster when bootstrapping and then use that CA to verify the received certificates for the following communications. However, the bootstrapping script /var/lib/rancher/rancherd/install.sh, which is also downloaded from the embedded Rancher Manager, has the VIP address configured: #!/usr/bin/env sh CATTLE_AGENT_BINARY_BASE_URL=&quot;https://192.168.48.240/assets&quot; CATTLE_SERVER=https://192.168.48.240 CATTLE_CA_CHECKSUM=&quot;be59358f796b09615b3f980cfe28ff96cae42a141289900bae494d869f363a67&quot; ... The nodes will query the embedded Rancher Manager via the VIP address instead of the FQDN provided during the Harvester installation. If the custom SSL certificate you configured doesn't contain a valid IP SAN extension, rancherd will fail at the exact point shown above. To work around this, you need to configure the cluster with a valid IP SAN extension, i.e., include the VIP address in the IP SAN extension when generating the CSR or signing the certificate. After applying the new certificate on the cluster, rancherd can then proceed and finish its job. However, soon rancher-system-agent will complain that it cannot verify the certificate received from the embedded Rancher Manager: $ journalctl -u rancher-system-agent.service Oct 02 10:18:44 node-0 systemd[1]: rancher-system-agent.service: Scheduled restart job, restart counter is at 91. Oct 02 10:18:44 node-0 systemd[1]: Stopped Rancher System Agent. Oct 02 10:18:44 node-0 systemd[1]: Started Rancher System Agent. Oct 02 10:18:44 node-0 rancher-system-agent[9620]: time=&quot;2023-10-02T10:18:44Z&quot; level=info msg=&quot;Rancher System Agent version v0.3.3 (9e827a5) is starting&quot; Oct 02 10:18:44 node-0 rancher-system-agent[9620]: time=&quot;2023-10-02T10:18:44Z&quot; level=info msg=&quot;Using directory /var/lib/rancher/agent/work for work&quot; Oct 02 10:18:44 node-0 rancher-system-agent[9620]: time=&quot;2023-10-02T10:18:44Z&quot; level=info msg=&quot;Starting remote watch of plans&quot; Oct 02 10:18:44 node-0 rancher-system-agent[9620]: time=&quot;2023-10-02T10:18:44Z&quot; level=info msg=&quot;Initial connection to Kubernetes cluster failed with error Get \\&quot;https://192.168.48.240/version\\&quot;: x509: certificate signed by unknown authority, removing CA data and trying again&quot; Oct 02 10:18:44 node-0 rancher-system-agent[9620]: time=&quot;2023-10-02T10:18:44Z&quot; level=fatal msg=&quot;error while connecting to Kubernetes cluster with nullified CA data: Get \\&quot;https://192.168.48.240/version\\&quot;: x509: certificate signed by unknown authority&quot; Oct 02 10:18:44 node-0 systemd[1]: rancher-system-agent.service: Main process exited, code=exited, status=1/FAILURE Oct 02 10:18:44 node-0 systemd[1]: rancher-system-agent.service: Failed with result 'exit-code'. If you see a similar log output, you need to manually add the CA to the trust list on each joining node with the following commands: # prepare the CA as embedded-rancher-ca.pem on the nodes $ sudo cp embedded-rancher-ca.pem /etc/pki/trust/anchors/ $ sudo update-ca-certificates After adding the CA to the trust list, the nodes can join to the cluster successfully.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.2"},{"title":"USB Installation","type":0,"sectionRef":"#","url":"/v1.2/install/usb-install","content":"USB Installation Create a bootable USB flash drive​ There are a couple of ways to create a USB installation flash drive. caution Known Issue: For the v1.2.0 ISO image, there is a known issue where the interactive ISO installation will get stuck using the USB method. To resolve this, you can use the patched ISO. This patched version only corrects the partition label, and there are no other changes. You can also use the related sha512 file to verify the ISO. Refer to the Harvester interactive ISO hangs with the USB installation method for details and a workaround. No matter which tool you use, creating a bootable device erases your USB device data. Please back up all data on your USB device before making a bootable device. Rufus​ Rufus allows you to create an ISO image on your USB flash drive on a Windows computer. Open Rufus and insert a clean USB stick into your computer. Rufus automatically detects your USB. Select the USB device you want to use from the Device drop-down menu. For Boot Selection, choose Select and find the Harvester installation ISO image you want to burn onto the USB. info If using older versions of Rufus, both DD mode and ISO mode works. DD mode works just like the dd command in Linux, and you can't browse partitions after you create a bootable device. ISO mode creates partitions on your device automatically and copies files to these partitions, and you can browse these partitions even after you create a bootable device. balenaEtcher​ balenaEtcher supports writing an image to a USB flash drive on most Linux distros, macOS, and Windows. It has a GUI and is easy to use. Select the Harvester installation ISO. Select the target USB device to create a USB installation flash drive. dd command​ You can use the 'dd' command on Linux or other platforms to create a USB installation flash drive. Ensure you choose the correct device; the following command erases data on the selected device. # sudo dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb_device&gt; bs=64k Known issues​ A GRUB _ text is displayed, but nothing happens when booting from a USB installation flash drive​ If you use the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. For example, select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. The representation varies from system to system. Graphics issue​ Firmwares of some graphic cards are not shipped in v0.3.0. You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Harvester installer is not displayed​ If a USB flash driver boots, but you can't see the harvester installer, try one of the following workarounds: Plug the USB flash drive into a USB 2.0 slot.For version v0.3.0 or above, remove the console=ttyS0 parameter when booting. Press e to edit the GRUB menu entry and remove the console=ttyS0 parameter. Harvester interactive ISO hangs with the USB installation method​ During installation from a USB flash drive with v1.2.0 ISO image (created by tools like balenaEtcher, dd, etc.), the installation process may get stuck on the initial image loading process because a required label is missing on the boot partition. Therefore, the installation cannot mount the data partition correctly, causing some checks in dracut to be blocked. If you encounter this issue, you'll observe the following similar output, and the process will hang for at least 50 minutes (the default timeout value from dracut). Workaround​ To address this problem, you can manually modify the root partition as follows: # Replace the `CDLABEL=COS_LIVE` with your USB data partition. Usually, your USB data partition is the first partition with the device name `sdx` that hangs on your screen. # Original $linux ($root)/boot/kernel cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable net.ifnames=1 # Modified $linux ($root)/boot/kernel cdroot root=live:/dev/sda1 rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable net.ifnames=1 The modified parameter should look like the following: After making this adjustment, press Ctrl + x to initiate booting. You should now enter the installer as usual. Related issue: [BUG] v1.2.0 Interactive ISO Fails to Install On Some Bare-Metal Devices","keywords":"","version":"v1.2"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.2/monitoring/harvester-monitoring","content":"Monitoring Available as of v1.2.0 The monitoring feature is now implemented with an addon and is disabled by default in new installations. Users can enable/disable rancher-monitoring addon from the Harvester WebUI after installation. Users can also enable/disable the rancher-monitoring addon in their Harvester installation by customizing the harvester-configuration file. For Harvester clusters upgraded from version v1.1.x, the monitoring feature is converted to an addon automatically and kept enabled as before. Dashboard Metrics​ Harvester has provided a built-in monitoring integration using Prometheus. Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. note Only admin users are able to view the cluster dashboard metrics. Additionally, Grafana is provided by rancher-monitoring, so the default admin password is: prom-operator Reference: values.yaml VM Detail Metrics​ For VMs, you can view VM metrics by clicking on the VM details page &gt; VM Metrics. note The current Memory Usage is calculated based on (1 - free/total) * 100%, not (used/total) * 100%. For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100%, roughly 40%. How to Configure Monitoring Settings​ Available as of v1.0.2 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: PrometheusPrometheus Node Exporter From UI​ On the Advanced page, you can view and change the resource settings as follows: Go to the Advanced &gt; Addons page and select the rancher-monitoring page.From the Prometheus tab, change the resource requests and limits.Select Save when finished configuring the settings for the rancher-monitoring addon. The Monitoring deployments restart within a few seconds. Please be aware that the reboot can take time to reload previous data. note The UI configuration is only visible when the rancher-monitoring addon is enabled. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. caution When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM(out of memory). In that case, you should increase the value of limits.memory. From CLI​ You can use the following kubectl command to change resource configurations for the rancher-monitoring addon: kubectl edit addons.harvesterhci.io -n cattle-monitoring-system rancher-monitoring. The resource path and default values are as follows: apiVersion: harvesterhci.io/v1beta1 kind: Addon metadata: name: rancher-monitoring namespace: cattle-monitoring-system spec: valuesContent: | prometheus: prometheusSpec: resources: limits: cpu: 1000m memory: 2500Mi requests: cpu: 850m memory: 1750Mi note You can still make configuration adjustments when the addon is disabled. However, these changes only take effect when you re-enable the addon. Alertmanager​ Harvester uses Alertmanager to collect and manage all the alerts that happened/happening in the cluster. Alertmanager Config​ Enable/Disable Alertmanager​ Alertmanager is enabled by default. You may disable it from the following config path. Change Resource Setting​ You can also change the resource settings of Alertmanager as shown in the picture above. Configure AlertmanagerConfig from WebUI​ To send the alerts to third-party servers, you need to config AlertmanagerConfig. On the WebUI, navigate to Monitoring &amp; Logging -&gt; Monitoring -&gt; Alertmanager Configs. On the Alertmanager Config: Create page, click Namespace to select the target namespace from the drop-down list and set the Name. After this, click Create in the lower right corner. Click the Alertmanager Configs you just created to continue the configuration. Click Add Receiver. Set the Name for the receiver. After this, select the receiver type, for example, Webhook, and click Add Webhook. Fill in the required parameters and click Create. Configure AlertmanagerConfig from CLI​ You can also add AlertmanagerConfig from the CLI. Exampe: a Webhook receiver in the default namespace. cat &lt;&lt; EOF &gt; a-single-receiver.yaml apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: amc-example # namespace: your value labels: alertmanagerConfig: example spec: route: continue: true groupBy: - cluster - alertname receiver: &quot;amc-webhook-receiver&quot; receivers: - name: &quot;amc-webhook-receiver&quot; webhookConfigs: - sendResolved: true url: &quot;http://192.168.122.159:8090/&quot; EOF # kubectl apply -f a-single-receiver.yaml alertmanagerconfig.monitoring.coreos.com/amc-example created # kubectl get alertmanagerconfig -A NAMESPACE NAME AGE default amc-example 27s Example of an Alert Received by Webhook​ Alerts sent to the webhook server will be in the following format: { 'receiver': 'longhorn-system-amc-example-amc-webhook-receiver', 'status': 'firing', 'alerts': [], 'groupLabels': {}, 'commonLabels': {'alertname': 'LonghornVolumeStatusWarning', 'container': 'longhorn-manager', 'endpoint': 'manager', 'instance': '10.52.0.83:9500', 'issue': 'Longhorn volume is Degraded.', 'job': 'longhorn-backend', 'namespace': 'longhorn-system', 'node': 'harv2', 'pod': 'longhorn-manager-r5bgm', 'prometheus': 'cattle-monitoring-system/rancher-monitoring-prometheus', 'service': 'longhorn-backend', 'severity': 'warning'}, 'commonAnnotations': {'description': 'Longhorn volume is Degraded for more than 5 minutes.', 'runbook_url': 'https://longhorn.io/docs/1.3.0/monitoring/metrics/', 'summary': 'Longhorn volume is Degraded'}, 'externalURL': 'https://192.168.122.200/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy', 'version': '4', 'groupKey': '{}/{namespace=&quot;longhorn-system&quot;}:{}', 'truncatedAlerts': 0 } note Different receivers may present the alerts in different formats. For details, please refer to the related documents. Known Limitation​ The AlertmanagerConfig is enforced by the namespace. Gloabl-level AlertmanagerConfig without a namespace is not supported. We have already created a GithHb issue to track upstream changes. Once the feature is available, Harvester will adopt it. View and Manage Alerts​ From Alertmanager Dashboard​ You can visit the original dashboard of Alertmanager from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy/#/alerts The overall view of the Alertmanager dashboard is as follows. You can view the details of an alert: From Prometheus Dashboard​ You can visit the original dashboard of Prometheus from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-prometheus:9090/proxy/ The Alerts menu in the top navigation bar shows all defined rules in Prometheus. You can use the filters Inactive, Pending, and Firing to quickly find the information that you need. Troubleshooting​ For Monitoring support and troubleshooting, please refer to the troubleshooting page .","keywords":"","version":"v1.2"},{"title":"Harvester Network Deep Dive","type":0,"sectionRef":"#","url":"/v1.2/networking/deep-dive","content":"Harvester Network Deep Dive The network topology below reveals how we implement the Harvester network. The diagram contains the built-in cluster network mgmt and a custom cluster network called oob. As shown above, the Harvester network primarily focuses on OSI model layer 2. We leverage Linux network devices and protocols to construct traffic paths for the communication between VM to VM, VM to host, and VM to external network devices. The Harvester network is composed of three layers, including: KubeVirt networking layer Harvester networking layer external networking layer KubeVirt Networking​ The general purpose of KubeVirt is to run VM inside the Kubernetes pod. The KubeVirt network builds the network path between the pod and VM. Please refer to the KubeVirt official document for more details. Harvester Networking​ Harvester networking is designed to build the network path between pods and the host network. It implements a management network, VLAN networks and untagged networks. We can refer to the last two networks as bridge networks, because bridge plays a vital role in their implementation. Bridge Network​ We leverage multus CNI and bridge CNI to implement the bridge network. Multus CNI is a Container Network Interface (CNI) plugin for Kubernetes that can attach multiple network interfaces to a pod. Its capability allows a VM to have one NIC for the management network and multiple NICs for the bridge network. Using the bridge CNI, the VM pod will be plugged into the L2 bridge specified in the Network Attachment Definition config. # Example 1 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;vlan100&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;mgmt-br&quot;, &quot;promiscMode&quot;: true, &quot;vlan&quot;: 100, } # Example 2 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;untagged-network&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;oob-br&quot;, &quot;promiscMode&quot;: true, &quot;ipam&quot;: {} } Example 1 is a typical VLAN configuration with VLAN ID 100, while Example 2 is an untagged network configuration with no VLAN ID. The VM pod configured using Example 1 will be plugged into the bridge mgmt-br, while the VM pod using Example 2 will be plugged into the bridge oob-br. To achieve high availability and fault tolerance, a bond device where the real NICs are bound is created to serve as the uplink of the bridge. By default, this bond device will allow the target tagged traffic/packets to pass through. harvester-0:/home/rancher # bridge -c vlan show dev oob-bo port vlan ids oob-bo 1 PVID Egress Untagged 100 200 The example above shows that the bond oob-bo allows packages with tag 1, 100 or 200. Management Network​ The management network is based on Canal. It is worth mentioning that the Canal interface where the Harvester configures the node IP is the bridge mgmt-br or a VLAN sub-interface of mgmt-br. This design has two benefits: The built-in mgmt cluster network supports both the management network and bridge network.With the VLAN network interface, we can assign a VLAN ID to the management network. As components of the mgmt cluster network, it's not allowed to delete or modify the bridge mgmt-br, the bond mgmt-bo and the VLAN device. External Networking​ External network devices typically refer to switches and DHCP servers. With a cluster network, we can group host NICs and connect them to different switches for traffic isolation. Below are some usage instructions. To allow tagged packets to pass, you need to set the port type of the external switch or other devices (such as a DHCP server) to trunk or hybrid mode and allow the specified VLAN tag. You need to configure link aggregation on the switch based on the bond mode of the peer host. Link aggregation can work in manual mode or LACP mode. The following lists the correspondence between bond mode and link aggregation mode. Bond Mode\tLink Aggregation Modemode 0(balance-rr)\tmanual mode 1(active-backup)\tnone mdoe 2(balance-oxr)\tmanual mode 3(broadcast)\tmanual mode 4(802.3ad)\tLACP mode 5(balance-tlb)\tnone mode 6(balance-alb)\tnone If you want VMs in a VLAN to be able to obtain IP addresses through the DHCP protocol, configure an IP pool for that VLAN in the DHCP server.","keywords":"Harvester Networking Topology","version":"v1.2"},{"title":"VM Network","type":0,"sectionRef":"#","url":"/v1.2/networking/harvester-network","content":"VM Network Harvester provides three types of networks for virtual machines (VMs), including: Management NetworkVLAN NetworkUntagged Network The management network is usually used for VMs whose traffic only flows inside the cluster. If your VMs need to connect to the external network, use the VLAN network or untagged network. Available as of v1.0.1 Harvester also introduced storage networking to separate the storage traffic from other cluster-wide workloads. Please refer to the storage network document for more details. Management Network​ Harvester uses Canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, you can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. How to use management network​ Since the management network is built-in and doesn't require extra operations, you can add it directly when configuring the VM network. important Network interfaces of VMs connected to the management network have an MTU value of 1450. This is because a VXLAN overlay network typically has a slightly higher per-packet overhead. If any of your workloads involve transmission of network traffic, you must specify the appropriate MTU value for the affected VM network interfaces and bridges. VLAN Network​ The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. Create a VM Network​ Go to Networks &gt; VM Networks. Select Create. Configure the following settings: Namespace Name Description (optional) On the Basics tab, configure the following settings: Type: Select L2VlanNetwork.Vlan ID Cluster Network On the Route tab, select an option and then specify the related IPv4 addresses. Auto(DHCP): The Harvester network controller retrieves the CIDR and gateway addresses from the DHCP server. You can specify the DHCP server address. Manual: Specify the CIDR and gateway addresses. important Harvester uses the information to verify that all nodes can access the VM network you are creating. If that is the case, the Network connectivity column on the VM Networks screen indicates that the network is active. Otherwise, the screen indicates that an error has occurred. Create a VM with VLAN Network​ You can now create a new VM using the VLAN network configured above: Click the Create button on the Virtual Machines page.Specify the required parameters and click the Networks tab.Either configure the default network to be a VLAN network or select an additional network to add. Untagged Network​ As is known, the traffic under a VLAN network has a VLAN ID tag and we can use the VLAN network with PVID (default 1) to communicate with any normal untagged traffic. However, some network devices may not expect to receive an explicitly tagged VLAN ID that matches the native VLAN on the switch the uplink belongs to. That's the reason why we provide the untagged network. How to use untagged network​ The usage of untagged network is similar to the VLAN network. To create a new untagged network, go to the Networks &gt; Networks page and click the Create button. You have to specify the name, select the type Untagged Network and choose the cluster network. note Starting from Harvester v1.1.2, Harvester supports updating and deleting VM networks. Make sure to stop all affected VMs before updating or deleting VM networks.","keywords":"Harvester Network","version":"v1.2"},{"title":"Cluster Network","type":0,"sectionRef":"#","url":"/v1.2/networking/index","content":"Cluster Network Concepts​ Cluster Network​ Available as of v1.1.0 In Harvester v1.1.0, we introduced a new concept called cluster network for traffic isolation. The following diagram describes a typical network architecture that separates data-center (DC) traffic from out-of-band (OOB) traffic. We abstract the sum of devices, links, and configurations on a traffic-isolated forwarding path on Harvester as a cluster network. In the above case, there will be two cluster networks corresponding to two traffic-isolated forwarding paths. Network Configuration​ Specifications including network devices of the Harvester hosts can be different. To be compatible with such a heterogeneous cluster, we designed the network configuration. Network configuration only works under a certain cluster network. Each network configuration corresponds to a set of hosts with uniform network specifications. Therefore, multiple network configurations are required for a cluster network on non-uniform hosts. VM Network​ A VM network is an interface in a virtual machine that connects to the host network. As with a network configuration, every network except the built-in management network must be under a cluster network. Harvester supports adding multiple networks to one VM. If a network's cluster network is not enabled on some hosts, the VM that owns this network will not be scheduled to those hosts. Please refer to network part for more details about networks. Relationship Between Cluster Network, Network Config, VM Network​ The following diagram shows the relationship between a cluster network, a network config, and a VM network. All Network Configs and VM Networks are grouped under a cluster network. A label can be assigned to each host to categorize hosts based on their network specifications. A network config can be added for each group of hosts using a node selector. For example, in the diagram above, the hosts in ClusterNetwork-A are divided into three groups as follows: The first group includes host0, which corresponds to network-config-A.The second group includes host1 and host2, which correspond to network-config-B.The third group includes the remaining hosts (host3, host4, and host5), which do not have any related network config and therefore do not belong to ClusterNetwork-A. The cluster network is only effective on hosts that are covered by the network configuration. A VM using a VM network under a specific cluster network can only be scheduled on a host where the cluster network is active. In the diagram above, we can see that: ClusterNetwork-A is active on host0, host1, and host2. VM0 uses VM-network-A, so it can be scheduled on any of these hosts.VM1 uses both VM-network-B and VM-network-C, so it can only be scheduled on host2 where both ClusterNetwork-A and ClusterNetwork-B are active.VM0, VM1, and VM2 cannot run on host3 where the two cluster networks are inactive. Overall, this diagram provides a clear visualization of the relationship between cluster networks, network configurations, and VM networks, as well as how they impact VM scheduling on hosts. Cluster Network Details​ Built-in Cluster Network​ Harvester provides a built-in cluster network called mgmt. It's different from the custom cluster network. The mgmt cluster network: Cannot be deleted.Does not need any network configuration.Is enabled on all hosts and cannot be disabled.Shares the same traffic egress with the management network. If there is no need for traffic separation, you can put all your network under the mgmt cluster network. Custom Cluster Network​ You are allowed to add the custom cluster network, which will not be available until it's enabled on some hosts by adding a network configuration. How to create a new cluster network​ To create a cluster network, go to the Networks &gt; ClusterNetworks/Configs page and click the Create button. You only need to specify the name. Click the Create Network Config button on the right of the cluster network to create a new network configuration. In the Node Selector tab, specify the name and choose one of the three methods to select nodes where the network configuration will apply. If you want to cover the unselected nodes, you can create another network configuration. Click the Uplink tab to add the NICs, and configure the bond options and link attributes. The bond mode defaults to active-backup. note The NICs drop-down list shows all the common NICs on all the selected nodes. The drop-down list will change as you select different nodes.The text enp7s3 (1/3 Down) in the NICs drop-down list indicates that the enp7s3 NIC is down in one of the three selected nodes. In this case, you need to find the NIC, set it up, and refresh this page. After this, it should be selectable. note Starting with Harvester v1.1.2, Harvester supports updating network configs. Make sure to stop all affected VMs before updating network configs.","keywords":"Harvester Networking ClusterNetwork NetworkConfig Network","version":"v1.2"},{"title":"IP Pool","type":0,"sectionRef":"#","url":"/v1.2/networking/ippool","content":"IP Pool Available as of v1.2.0 Harvester IP Pool is a built-in IP address management (IPAM) solution exclusively available to Harvester load balancers (LBs). Features​ Multiple IP ranges: Each IP pool can contain multiple IP ranges or CIDRs.Allocation history: The IP pool keeps track of the allocation history of every IP address and prioritizes assigning previously allocated addresses by load balancer name. status: allocatedHistory: 192.168.178.8: default/rke2-default-lb-pool-2fab9ac0 Scope: IP pools can be confined to a particular network, project, namespace, or guest cluster. How to create​ To create a new IP pool: Go to the Networks &gt; IP Pools page and select Create.Specify the Name of the IP pool.Go to the Range tab to specify the IP ranges for the IP pool. You can add multiple IP ranges.Go to the Selector tab to specify the Scope and Priority of the IP pool. Selection policy​ Each IP pool will have a specific range, and you can specify the corresponding requirements in the LB annotations. IP pools that meet the specified requirements will automatically assign IP addresses to LBs. LBs utilize the following annotations to express requirements (all annotations are optional): loadbalancer.harvesterhci.io/network specifies the VM network the guest cluster nodes use.loadbalancer.harvesterhci.io/project and loadbalancer.harvesterhci.io/namespace identify the project and namespace of the VMs that comprise the guest cluster.loadbalancer.harvesterhci.io/cluster denotes the name of the guest cluster. The IP pool has a selector, including network and scope, to match the requirements of the LB. Network is a hard condition. The optional IP pool must match the value of the LB annotation loadbalancer.harvesterhci.io/network.Every IP pool, except the global IP pool, has a unique scope different from others if its priority is 0. The project, namespace, or cluster name of LBs should be in the scope of the IP pool if they want to get an IP from this pool. spec.selector.priority specifies the priority of the IP Pool. The larger the number, the higher the priority. If the priority is not 0, the value should differ. The priority helps you to migrate the old IP pool to the new one.If the IP Pool has a scope that matches all projects, namespaces, and guest clusters, it's called a global IP pool, and only one global IP pool is allowed. If there is no IP pool matching the requirements of the LB, the IPAM will allocate an IP address from the global IP pool if it exists. Examples​ Example 1: You wish to set up an IP pool within the range 192.168.100.0/24 for the default namespace. In this scenario, all load balancers within the default namespace will receive an IP address from this designated IP pool: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: default-ip-pool spec: ranges: - subnet: 192.168.100.0/24 selector: scope: namespace: default Example 2: You have a guest cluster rke2 deployed within the network default/vlan1, and its project/namespace name is product/default. If you want to configure an exclusive IP pool range 192.168.10.10-192.168.10.20 for it. Refer to the following YAML config: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: rke2-ip-pool spec: ranges: - subnet: 192.168.10.0/24 rangeStart: 192.168.10.10 rangeEnd: 192.168.10.20 selector: network: default/vlan1 scope: - project: product namespace: default guestCluster: rke2 Example 3: If you have specified the IP pool default-ip-pool for the default namespace, you want to migrate the IP pool default-ip-pool to a different IP pool default-ip-pool-2 with range 192.168.200.0/24. It's not allowed to specify over one IP pool for the same scope, but you can give the IP pool default-ip-pool-2 a higher priority than default-ip-pool. Refer to the following YAML config: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: default-ip-pool-2 spec: ranges: - subnet: 192.168.200.0/24 selector: priority: 1 # The priority is higher than default-ip-pool scope: namespace: default Example 4: You want to configure a global IP pool with a CIDR range of 192.168.20.0/24: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: global-ip-pool spec: ranges: - subnet: 192.168.20.0/24 selector: scope: - project: &quot;*&quot; namespace: &quot;*&quot; guestCluster: &quot;*&quot; Allocation policy​ The IP pool prioritizes the allocation of previously assigned IP addresses based on their allocation history.IP addresses are assigned in ascending order. note Starting with Harvester v1.2.0, the vip-pools setting is deprecated. Following the upgrade, this setting will be automatically migrated to the Harvester IP pools.","keywords":"IP Pool","version":"v1.2"},{"title":"Load Balancer","type":0,"sectionRef":"#","url":"/v1.2/networking/loadbalancer","content":"Load Balancer Available as of v1.2.0 The Harvester load balancer (LB) is a built-in Layer 4 load balancer that distributes incoming traffic across workloads deployed on Harvester virtual machines (VMs) or guest Kubernetes clusters. VM load balancer​ Features​ Harvester VM load balancer supports the following features: Address assignment: Get the LB IP address from a DHCP server or a pre-defined IP pool.Protocol support: Supports both TCP and UDP protocols for load balancing.Multiple listeners: Create multiple listeners to handle incoming traffic on different ports or with other protocols.Label selector: The LB uses label selectors to match the backend servers. Therefore, you must configure the corresponding labels for the backend VMs you want to add to the LB.Health check: Only send traffic to healthy backend instances. Limitations​ Harvester VM load balancer has the following limitations: Namespace restriction: This restriction facilitates permission management and ensures the LB only uses VMs in the same namespace as the backend servers.IPv4-only: The LB is only compatible with IPv4 addresses for VMs.Guest agent installation: Installing the guest agent on each backend VM is required to obtain IP addresses. Connectivity Requirement: Network connectivity must be established between backend VMs and Harvester hosts. When a VM has multiple IP addresses, the LB will select the first one as the backend address.Access Restriction: The VM LB address is exposed only within the same network as the Harvester hosts. To access the LB from outside the network, you must provide a route from outside to the LB address. note Harvester VM load balancer doesn't support Windows VMs because the guest agent is not available for Windows VMs. How to create​ To create a new Harvester VM load balancer: Go to the Networks &gt; Load Balancer page and select Create.Select the Namespace and specify the Name.Go to the Basic tab to choose the IPAM mode, which can be DHCP or IP Pool. If you select IP Pool, prepare an IP pool first, specify the IP pool name, or choose auto. If you choose auto, the LB automatically selects an IP pool according to the IP pool selection policy.Go to the Listeners tab to add listeners. You must specify the Port, Protocol, and Backend Port for each listener.Go to the Backend Server Selector tab to add label selectors. To add the VM to the LB, go to the Virtual Machine &gt; Instance Labels tab to add the corresponding labels to the VM.Go to the Health Check tab to enable health check and specify the parameters, including the Port, Success Threshold, Failure Threshold, Interval, and Timeout if the backend service supports health check. Refer to Health Checks for more details. Health Checks​ The Harvester load balancer supports TCP health checks. You can specify the parameters in the Harvester UI if you've enabled the Health Check option. Name\tValue Type\tRequired\tDefault\tDescriptionHealth Check Port\tint\ttrue\tN/A\tSpecifies the port. The prober will access the address composed of the backend server IP and the port. Health Check Success Threshold\tint\tfalse\t1\tSpecifies the health check success threshold. Disabled by default. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. Health Check Failure Threshold\tint\tfalse\t3\tSpecifies the health check failure threshold. Disabled by default. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. Health Check Period\tint\tfalse\t5\tSpecifies the health check period in seconds. Disabled by default. Health Check Timeout\tint\tfalse\t3\tSpecifies the timeout of every health check in seconds. Disabled by default. Guest Kubernetes cluster load balancer​ In conjunction with Harvester Cloud Provider, the Harvester load balancer provides load balancing for LB services in the guest cluster.When you create, update, or delete an LB service on a guest cluster with Harvester Cloud Provider, the Harvester Cloud Provider will create a Harvester LB automatically. For more details, refer to Harvester Cloud Provider.","keywords":"Load Balancer","version":"v1.2"},{"title":"Harvester Cloud Provider","type":0,"sectionRef":"#","url":"/v1.2/rancher/cloud-provider","content":"Harvester Cloud Provider RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and Harvester cluster storage passthrough support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2 cluster.How to use the Harvester load balancer. Backward Compatibility Notice​ note Please note a known backward compatibility issue if you're using the Harvester cloud provider version v0.2.2 or higher. If your Harvester version is below v1.2.0 and you intend to use newer RKE2 versions (i.e., &gt;= v1.26.6+rke2r1, v1.25.11+rke2r1, v1.24.15+rke2r1), it is essential to upgrade your Harvester cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester cloud provider. For a detailed support matrix, please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section of the official website. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hope to remove this limitation in a future release of Harvester. Deploying to the RKE1 Cluster with Harvester Node Driver​ When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Deploying to the RKE2 Cluster with Harvester Node Driver​ When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Deploying to the RKE2 custom cluster (experimental)​ Use generate_addon.sh to generate a cloud-config and place it into the directory /etc/kubernetes/cloud-config on every custom node. curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | bash -s &lt;serviceaccount name&gt; &lt;namespace&gt; note The generate_addon.sh script depends on kubectl and jq to operate the Harvester cluster. The script needs access to the Harvester Cluster kubeconfig to work. You can find the kubeconfig file from one of the Harvester management nodes in the /etc/rancher/rke2/rke2.yaml path. The namespace needs to be the namespace in which the guest cluster will be created. Configure the Cloud Provider to Harvester and select Create to spin up the cluster. Deploying to the K3s cluster with Harvester node driver (experimental)​ When spinning up a K3s cluster using the Harvester node driver, you can perform the following steps to deploy the harvester cloud provider: Use generate_addon.sh to generate cloud config. curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | bash -s &lt;serviceaccount name&gt; &lt;namespace&gt; The output will look as follows: ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: &lt;CACERT&gt; server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: &lt;TOKEN&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: &lt;CONTENT&gt; owner: root:root path: /etc/kubernetes/cloud-config permissions: '0644' Copy and paste the cloud-init user data content to Machine Pools &gt;Show Advanced &gt; User Data. Add the following HelmChart yaml of harvester-cloud-provider to Cluster Configuration &gt; Add-On Config &gt; Additional Manifest. apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.2.2 helmVersion: v3 Disable the in-tree cloud provider in the following ways: Click the Edit as YAML button. Disable servicelb and set disable-cloud-controller: true to disable the default K3s cloud controller. machineGlobalConfig: disable: - servicelb disable-cloud-controller: true Add cloud-provider=external to use the Harvester cloud provider. machineSelectorConfig: - config: kubelet-arg: - cloud-provider=external protect-kernel-defaults: false With these settings in place a K3s cluster should provision successfully while using the external cloud provider. Upgrade Cloud Provider​ Upgrade RKE2​ The cloud provider can be upgraded by upgrading the RKE2 version. You can upgrade the RKE2 cluster via the Rancher UI as follows: Click ☰ &gt; Cluster Management.Find the guest cluster that you want to upgrade and select ⋮ &gt; Edit Config.Select Kubernetes Version.Click Save. Upgrade RKE/K3s​ RKE/K3s upgrade cloud provider via the Rancher UI, as follows: Click ☰ &gt; RKE/K3s Cluster &gt; Apps &gt; Installed Apps.Find the cloud provider chart and select ⋮ &gt; Edit/Upgrade.Select Version. Click Next &gt; Update. Load Balancer Support​ Once you've deployed the Harvester cloud provider, you can leverage the Kubernetes LoadBalancer service to expose a microservice within the guest cluster to the external world. Creating a Kubernetes LoadBalancer service assigns a dedicated Harvester load balancer to the service, and you can make adjustments through the Add-on Config within the Rancher UI. IPAM​ Harvester's built-in load balancer offers both DHCP and Pool modes, and you can configure it by adding the annotation cloudprovider.harvesterhci.io/ipam: $mode to its corresponding service. Starting from Harvester cloud provider &gt;= v0.2.0, it also introduces a unique Share IP mode. A service shares its load balancer IP with other services in this mode. DCHP: A DHCP server is required. The Harvester load balancer will request an IP address from the DHCP server. Pool: An IP pool must be configured first. The Harvester load balancer controller will allocate an IP for the load balancer service following the IP pool selection policy. Share IP: When creating a new load balancer service, you can re-utilize an existing load balancer service IP. The new service is referred to as a secondary service, while the currently chosen service is the primary one. To specify the primary service in the secondary service, you can add the annotation cloudprovider.harvesterhci.io/primary-service: $primary-service-name. However, there are two known limitations: Services that share the same IP address can't use the same port.Secondary services cannot share their IP with additional services. note Modifying the IPAM mode isn't allowed. You must create a new service if you intend to change the IPAM mode. Health checks​ Beginning with Harvester cloud provider v0.2.0, additional health checks of the LoadBalancer service within the guest Kubernetes cluster are no longer necessary. Instead, you can configure liveness and readiness probes for your workloads. Consequently, any unavailable pods will be automatically removed from the load balancer endpoints to achieve the same desired outcome.","keywords":"Harvester harvester RKE rke RKE2 rke2 Harvester Cloud Provider","version":"v1.2"},{"title":"Logging","type":0,"sectionRef":"#","url":"/v1.2/logging/harvester-logging","content":"Logging Available as of v1.2.0 It is important to know what is happening/has happened in the Harvester Cluster. Harvester collects the cluster running log, kubernetes audit and event log right after the cluster is powered on, which is helpful for monitoring, logging, auditing and troubleshooting. Harvester supports sending those logs to various types of log servers. note The size of logging data is related to the cluster scale, workload and other factors. Harvester does not use persistent storage to store log data inside the cluster. Users need to set up a log server to receive logs accordingly. The logging feature is now implemented with an addon and is disabled by default in new installations. Users can enable/disable the rancher-logging addon from the Harvester UI after installation. Users can also enable/disable the rancher-logging addon in their Harvester installation by customizing the harvester-configuration file. For Harvester clusters upgraded from version v1.1.x, the logging feature is converted to an addon automatically and kept enabled as before. High-level Architecture​ The Banzai Cloud Logging operator now powers both Harvester and Rancher as an in-house logging solution. In Harvester's practice, the Logging, Audit and Event shares one architecture, the Logging is the infrastructure, while the Audit and Event are on top of it. Logging​ The Harvester logging infrastructure allows you to aggregate Harvester logs into an external service such as Graylog, Elasticsearch, Splunk, Grafana Loki and others. Collected Logs​ See below for a list logs that are collected: Logs from all cluster PodsKernel logs from each nodeLogs from select systemd services from each node rke2-serverrke2-agentrancherdrancher-system-agentwickediscsid note Users are able to configure and modify where the aggregated logs are sent, as well as some basic filtering. It is not supported to change which logs are collected. Configuring Log Resources​ Underneath Banzai Cloud's logging operator are fluentd and fluent-bit, which handle the log routing and collecting respectively. If desired, you can modify how many resources are dedicated to those components. From UI​ Go to the Advanced &gt; Addons page and select the rancher-logging addon.From the Fluentbit tab, change the resource requests and limits.From the Fluentd tab, change the resource requests and limits.Select Save when finished configuring the settings for the rancher-logging addon. note The UI configuration is only visible when the rancher-logging addon is enabled. From CLI​ You can use the following kubectl command to change resource configurations for the rancher-logging addon: kubectl edit addons.harvesterhci.io -n cattle-logging-system rancher-logging. The resource path and default values are as follows. apiVersion: harvesterhci.io/v1beta1 kind: Addon metadata: name: rancher-logging namespace: cattle-logging-system spec: valuesContent: | fluentbit: resources: limits: cpu: 200m memory: 200Mi requests: cpu: 50m memory: 50Mi fluentd: resources: limits: cpu: 1000m memory: 800Mi requests: cpu: 100m memory: 200Mi note You can still make configuration adjustments when the addon is disabled. However, these changes only take effect when you re-enable the addon. Configuring Log Destinations​ Logging is backed by the Banzai Cloud Logging Operator, and so is controlled by Flows/ClusterFlows and Outputs/ClusterOutputs. You can route and filter logs as you like by applying these CRDs to the Harvester cluster. When applying new Ouptuts and Flows to the cluster, it can take some time for the logging operator to effectively apply them. So please allow a few minutes for the logs to start flowing. Clustered vs Namespaced​ One important thing to understand when routing logs is the difference between ClusterFlow vs Flow and ClusterOutput vs Output. The main difference between the clustered and non-clustered version of each is that the non-clustered versions are namespaced. The biggest implication of this is that Flows can only access Outputs that are within the same namespace, but can still access any ClusterOutput. For more information, see the documentation: Flows/ClusterFlowsOutputs/ClusterOutputs From UI​ note UI images are for Output and Flow whose configuration process is almost identical to their clustered counterparts. Any differences will be noted in the steps below. Creating Outputs​ Choose the option to create a new Output or ClusterOutput.If creating an Output, select the desired namespace.Add a name for the resources.Select the logging type.Select the logging output type. Configure the output buffer if necessary. Add any labels or annotations. Once done, click Create on the lower right. note Depending on the output selected (Splunk, Elasticsearch, etc), there will be additional fields to specify in the form. Output​ The fields present in the Output form will change depending on the Output chosen, in order to expose the fields present for each output plugin. Output Buffer​ The Output Buffer editor allows you to describe how you want the output buffer to behave. You can find the documentation for the buffer fields here. Labels &amp; Annotations​ You can append labels and annotations to the created resource. Creating Flows​ Choose the option to create a new Flow or ClusterFlow.If creating a Flow, select the desired namespace.Add a name for the resource.Select any nodes whose logs to include or exclude. Select target Outputs and ClusterOutputs. Add any filters if desired. Once done, click Create on the lower left. Matches​ Matches allow you to filter which logs you want to include in the Flow. The form only allows you to include or exclude node logs, but if needed, you can add other match rules supported by the resource by selecting Edit as YAML. For more information about the match directive, see Routing your logs with match directive. Outputs​ Outputs allow you to select one or more OutputRefs to send the aggregated logs to. When creating or editing a Flow / ClusterFlow, it is required that the user selects at least one Output. note There must be at least one existing ClusterOutput or Output that can be attached to the flow, or you will not be able to create / edit the flow. Filters​ Filters allow you to transform, process, and mutate the logs. In the text edit, you will find descriptions of the supported filters, but for more information, you can visit the list of supported filters. From CLI​ To configure log routes via the command line, you only need to define the YAML files for the relevant resources: # elasticsearch-logging.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: elasticsearch-example namespace: fleet-local labels: example-label: elasticsearch-example annotations: example-annotation: elasticsearch-example spec: elasticsearch: host: &lt;url-to-elasticsearch-server&gt; port: 9200 --- apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: elasticsearch-example namespace: fleet-local spec: match: - select: {} globalOutputRefs: - elasticsearch-example And then apply them: kubectl apply -f elasticsearch-logging.yaml Referencing Secrets​ There are 3 ways Banzai Cloud allows specifying secret values via yaml values. The simplest is to use the value key, which is a simple string value for the desired secret. This method should only be used for testing and never in production: aws_key_id: value: &quot;secretvalue&quot; The next is to use valueFrom, which allows referencing a specific value from a secret by a name and key pair: aws_key_id: valueFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; Some plugins require a file to read from rather than simply receiving a value from the secret (this is often the case for CA cert files). In these cases, you need to use mountFrom, which will mount the secret as a file to the underlying fluentd deployment and point the plugin to the file. The valueFrom and mountFrom object look the same: tls_cert_path: mountFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; For more information, you can find the related documentation here. Example Outputs​ Elasticsearch​ For the simplest deployment, you can deploy Elasticsearch on your local system using docker: docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled=false -e node.name=es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23 Make sure that you have set vm.max_map_count to be &gt;= 262144 or the docker command above will fail. Once the Elasticsearch server is up, you can create the yaml file for the ClusterOutput and ClusterFlow: cat &lt;&lt; EOF &gt; elasticsearch-example.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: elasticsearch-example namespace: cattle-logging-system spec: elasticsearch: host: 192.168.0.119 port: 9200 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: elasticsearch-example namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - elasticsearch-example EOF And apply the file: kubectl apply -f elasticsearch-example.yaml After allowing some time for the logging operator to apply the resources, you can test that the logs are flowing: $ curl localhost:9200/fluentd/_search { &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 11603, &quot;max_score&quot;: 1, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;fluentd&quot;, &quot;_type&quot;: &quot;fluentd&quot;, &quot;_id&quot;: &quot;yWHr0oMBXcBggZRJgagY&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;stream&quot;: &quot;stderr&quot;, &quot;logtag&quot;: &quot;F&quot;, &quot;message&quot;: &quot;I1013 02:29:43.020384 1 csi_handler.go:248] Attaching \\&quot;csi-974b4a6d2598d8a7a37b06d06557c428628875e077dabf8f32a6f3aa2750961d\\&quot;&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;csi-attacher-5d4cc8cfc8-hd4nb&quot;, &quot;namespace_name&quot;: &quot;longhorn-system&quot;, &quot;pod_id&quot;: &quot;c63c2014-9556-40ce-a8e1-22c55de12e70&quot;, &quot;labels&quot;: { &quot;app&quot;: &quot;csi-attacher&quot;, &quot;pod-template-hash&quot;: &quot;5d4cc8cfc8&quot; }, &quot;annotations&quot;: { &quot;cni.projectcalico.org/containerID&quot;: &quot;857df09c8ede7b8dee786a8c8788e8465cca58f0b4d973c448ed25bef62660cf&quot;, &quot;cni.projectcalico.org/podIP&quot;: &quot;10.52.0.15/32&quot;, &quot;cni.projectcalico.org/podIPs&quot;: &quot;10.52.0.15/32&quot;, &quot;k8s.v1.cni.cncf.io/network-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;k8s.v1.cni.cncf.io/networks-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;kubernetes.io/psp&quot;: &quot;global-unrestricted-psp&quot; }, &quot;host&quot;: &quot;harvester-node-0&quot;, &quot;container_name&quot;: &quot;csi-attacher&quot;, &quot;docker_id&quot;: &quot;f10e4449492d4191376d3e84e39742bf077ff696acbb1e5f87c9cfbab434edae&quot;, &quot;container_hash&quot;: &quot;sha256:03e115718d258479ce19feeb9635215f98e5ad1475667b4395b79e68caf129a6&quot;, &quot;container_image&quot;: &quot;docker.io/longhornio/csi-attacher:v3.4.0&quot; } } }, ... ] } } Graylog​ You can follow the instructions here to deploy and view cluster logs via Graylog: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;all-logs-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;example-gelf-hs&quot; --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;example-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: gelf: host: &quot;192.168.122.159&quot; port: 12202 protocol: &quot;udp&quot; Splunk​ You can follow the instructions here to deploy and view cluster logs via Splunk. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: splunkHec: hec_host: 192.168.122.101 hec_port: 8088 insecure_ssl: true index: harvester-log-index hec_token: valueFrom: secretKeyRef: key: HECTOKEN name: splunk-hec-token2 buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: globalOutputRefs: - harvester-logging-splunk Loki​ You can follow the instructions in the logging HEP on deploying and viewing cluster logs via Grafana Loki. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-loki namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - harvester-loki --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-loki namespace: cattle-logging-system spec: loki: url: http://loki-stack.cattle-logging-system.svc:3100 extra_labels: logOutput: harvester-loki Audit​ Harvester collects Kubernetes audit and is able to send the audit to various types of log servers. The policy file to guide kube-apiserver is here. Audit Definition​ In kubernetes, the audit data is generated by kube-apiserver according to defined policy. ... Audit policy Audit policy defines rules about what events should be recorded and what data they should include. The audit policy object structure is defined in the audit.k8s.io API group. When an event is processed, it's compared against the list of rules in order. The first matching rule sets the audit level of the event. The defined audit levels are: None - don't log events that match this rule. Metadata - log request metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body. Request - log event metadata and request body but not response body. This does not apply for non-resource requests. RequestResponse - log event metadata, request and response bodies. This does not apply for non-resource requests. Audit Log Format​ Audit Log Format in Kubernetes​ Kubernetes apiserver logs audit with following JSON format into a local file. { &quot;kind&quot;:&quot;Event&quot;, &quot;apiVersion&quot;:&quot;audit.k8s.io/v1&quot;, &quot;level&quot;:&quot;Metadata&quot;, &quot;auditID&quot;:&quot;13d0bf83-7249-417b-b386-d7fc7c024583&quot;, &quot;stage&quot;:&quot;RequestReceived&quot;, &quot;requestURI&quot;:&quot;/apis/flowcontrol.apiserver.k8s.io/v1beta2/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1&quot;, &quot;verb&quot;:&quot;create&quot;, &quot;user&quot;:{&quot;username&quot;:&quot;system:apiserver&quot;,&quot;uid&quot;:&quot;d311c1fe-2d96-4e54-a01b-5203936e1046&quot;,&quot;groups&quot;:[&quot;system:masters&quot;]}, &quot;sourceIPs&quot;:[&quot;::1&quot;], &quot;userAgent&quot;:&quot;kube-apiserver/v1.24.7+rke2r1 (linux/amd64) kubernetes/e6f3597&quot;, &quot;objectRef&quot;:{&quot;resource&quot;:&quot;prioritylevelconfigurations&quot;, &quot;apiGroup&quot;:&quot;flowcontrol.apiserver.k8s.io&quot;, &quot;apiVersion&quot;:&quot;v1beta2&quot;}, &quot;requestReceivedTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot;, &quot;stageTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot; } Audit Log Format before Being Sent to Log Servers​ Harvester keeps the audit log unchanged before sending it to the log server. Audit Log Output/ClusterOutput​ To output audit related log, the Output/ClusterOutput requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit Only from the Type drpo-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: http: endpoint: &quot;http://192.168.122.159:8096/&quot; open_timeout: 3 format: type: &quot;json&quot; buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Audit Log Flow/ClusterFlow​ To route audit related logs, the Flow/ClusterFlow requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit. When you config from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;harvester-audit-webhook&quot; loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Harvester​ Event​ Harvester collects Kubernetes event and is able to send the event to various types of log servers. Event Definition​ Kubernetes events are objects that show you what is happening inside a cluster, such as what decisions were made by the scheduler or why some pods were evicted from the node. All core components and extensions (operators/controllers) may create events through the API Server. Events have no direct relationship with log messages generated by the various components, and are not affected with the log verbosity level. When a component creates an event, it often emits a corresponding log message. Events are garbage collected by the API Server after a short time (typically after an hour), which means that they can be used to understand issues that are happening, but you have to collect them to investigate past events. Events are the first thing to look at for application, as well as infrastructure operations when something is not working as expected. Keeping them for a longer period is essential if the failure is the result of earlier events, or when conducting post-mortem analysis. Event Log Format​ Event Log Format in Kubernetes​ A kubernetes event example: { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;count&quot;: 1, &quot;eventTime&quot;: null, &quot;firstTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;involvedObject&quot;: { &quot;apiVersion&quot;: &quot;kubevirt.io/v1&quot;, &quot;kind&quot;: &quot;VirtualMachineInstance&quot;, &quot;name&quot;: &quot;vm-ide-1&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604601&quot;, &quot;uid&quot;: &quot;1bd4133f-5aa3-4eda-bd26-3193b255b480&quot; }, &quot;kind&quot;: &quot;Event&quot;, &quot;lastTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;message&quot;: &quot;VirtualMachineInstance defined.&quot;, &quot;metadata&quot;: { &quot;creationTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;name&quot;: &quot;vm-ide-1.170e43cbdd833b62&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604626&quot;, &quot;uid&quot;: &quot;0114f4e7-1d4a-4201-b0e5-8cc8ede202f4&quot; }, &quot;reason&quot;: &quot;Created&quot;, &quot;reportingComponent&quot;: &quot;&quot;, &quot;reportingInstance&quot;: &quot;&quot;, &quot;source&quot;: { &quot;component&quot;: &quot;virt-handler&quot;, &quot;host&quot;: &quot;harv1&quot; }, &quot;type&quot;: &quot;Normal&quot; }, Event Log Format before Being Sent to Log Servers​ Each event log has the format of: {&quot;stream&quot;:&quot;&quot;,&quot;logtag&quot;:&quot;F&quot;,&quot;message&quot;:&quot;&quot;,&quot;kubernetes&quot;:{&quot;&quot;}}. The kubernetes event is in the field message. { &quot;stream&quot;:&quot;stdout&quot;, &quot;logtag&quot;:&quot;F&quot;, &quot;message&quot;:&quot;{ \\\\&quot;verb\\\\&quot;:\\\\&quot;ADDED\\\\&quot;, \\\\&quot;event\\\\&quot;:{\\\\&quot;metadata\\\\&quot;:{\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1.170e446c3f890433\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;0b44b6c7-b415-4034-95e5-a476fcec547f\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612482\\\\&quot;,\\\\&quot;creationTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;managedFields\\\\&quot;:[{\\\\&quot;manager\\\\&quot;:\\\\&quot;virt-controller\\\\&quot;,\\\\&quot;operation\\\\&quot;:\\\\&quot;Update\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;v1\\\\&quot;,\\\\&quot;time\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;}]},\\\\&quot;involvedObject\\\\&quot;:{\\\\&quot;kind\\\\&quot;:\\\\&quot;VirtualMachineInstance\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;1bd4133f-5aa3-4eda-bd26-3193b255b480\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;kubevirt.io/v1\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612477\\\\&quot;},\\\\&quot;reason\\\\&quot;:\\\\&quot;SuccessfulDelete\\\\&quot;,\\\\&quot;message\\\\&quot;:\\\\&quot;Deleted PodDisruptionBudget kubevirt-disruption-budget-hmmgd\\\\&quot;,\\\\&quot;source\\\\&quot;:{\\\\&quot;component\\\\&quot;:\\\\&quot;disruptionbudget-controller\\\\&quot;},\\\\&quot;firstTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;lastTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;count\\\\&quot;:1,\\\\&quot;type\\\\&quot;:\\\\&quot;Normal\\\\&quot;,\\\\&quot;eventTime\\\\&quot;:null,\\\\&quot;reportingComponent\\\\&quot;:\\\\&quot;\\\\&quot;,\\\\&quot;reportingInstance\\\\&quot;:\\\\&quot;\\\\&quot;} }&quot;, &quot;kubernetes&quot;:{&quot;pod_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;namespace_name&quot;:&quot;cattle-logging-system&quot;,&quot;pod_id&quot;:&quot;d3453153-58c9-456e-b3c3-d91242580df3&quot;,&quot;labels&quot;:{&quot;app.kubernetes.io/instance&quot;:&quot;harvester-default-event-tailer&quot;,&quot;app.kubernetes.io/name&quot;:&quot;event-tailer&quot;,&quot;controller-revision-hash&quot;:&quot;harvester-default-event-tailer-747b9d4489&quot;,&quot;statefulset.kubernetes.io/pod-name&quot;:&quot;harvester-default-event-tailer-0&quot;},&quot;annotations&quot;:{&quot;cni.projectcalico.org/containerID&quot;:&quot;aa72487922ceb4420ebdefb14a81f0d53029b3aec46ed71a8875ef288cde4103&quot;,&quot;cni.projectcalico.org/podIP&quot;:&quot;10.52.0.178/32&quot;,&quot;cni.projectcalico.org/podIPs&quot;:&quot;10.52.0.178/32&quot;,&quot;k8s.v1.cni.cncf.io/network-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;k8s.v1.cni.cncf.io/networks-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;kubernetes.io/psp&quot;:&quot;global-unrestricted-psp&quot;},&quot;host&quot;:&quot;harv1&quot;,&quot;container_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;docker_id&quot;:&quot;455064de50cc4f66e3dd46c074a1e4e6cfd9139cb74d40f5ba00b4e3e2a7ab2d&quot;,&quot;container_hash&quot;:&quot;docker.io/banzaicloud/eventrouter@sha256:6353d3f961a368d95583758fa05e8f4c0801881c39ed695bd4e8283d373a4262&quot;,&quot;container_image&quot;:&quot;docker.io/banzaicloud/eventrouter:v0.1.0&quot;} } Event Log Output/ClusterOutput​ Events share the Output/ClusterOutput with Logging. Select Logging/Event from the Type drop-down list. Event Log Flow/ClusterFlow​ Compared with the normal Logging Flow/ClusterFlow, the Event related Flow/ClusterFlow, has one more match field with the value of event-tailer. When you configure from the Harvester dashboard, the field is added automatically. Select Event from the Type drop-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-event-webhook namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: - select: labels: app.kubernetes.io/name: event-tailer globalOutputRefs: - harvester-event-webhook ","keywords":"Harvester Logging Audit Event","version":"v1.2"},{"title":"Harvester CSI Driver","type":0,"sectionRef":"#","url":"/v1.2/rancher/csi-driver","content":"Harvester CSI Driver The Harvester Container Storage Interface (CSI) Driver provides a standard CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines that run as guest Kubernetes nodes are in the same namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hopeto remove this limitation in a future release of Harvester. note Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only(ROX) and read-write(RWX) support. Deploying with Harvester RKE1 node driver​ Select the Harvester(Out-of-tree) option. Install Harvester CSI Driver from the Rancher marketplace. Deploying with Harvester RKE2 node driver​ When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed automatically when Harvester cloud provider is selected. Install CSI driver manually in the RKE2 cluster​ If you prefer to install the Harvester CSI driver without enabling the Harvester cloud provider, you can refer to the following steps: Prerequisites of manual install​ Ensure that you have the following prerequisites in place: You have kubectl and jq installed on your system.You have the kubeconfig file for your bare-metal Harvester cluster. You can find the kubeconfig file from one of the Harvester management nodes in the /etc/rancher/rke2/rke2.yaml path. export KUBECONFIG=/path/to/your/harvester-kubeconfig Perform the following steps to deploy the Harvester CSI driver manually: Deploy Harvester CSI driver​ Generate the cloud-config. You can generate the cloud-config file using the generate_addon_csi.sh script. It is available on the harvester/harvester-csi-driver repo. &lt;serviceaccount name&gt; usually corresponds to your guest cluster name, and &lt;namespace&gt; should match the machine pool's namespace. ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; RKE2 The generated output will be similar to the following one: ########## cloud-config ############ apiVersion: v1 clusters: - cluster: &lt;token&gt; server: https://&lt;YOUR HOST HARVESTER VIP&gt;:6443 name: default contexts: - context: cluster: default namespace: default user: rke2-guest-01-default-default name: rke2-guest-01-default-default current-context: rke2-guest-01-default-default kind: Config preferences: {} users: - name: rke2-guest-01-default-default user: token: &lt;token&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: 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 owner: root:root path: /var/lib/rancher/rke2/etc/config-files/cloud-provider-config permissions: '0644' Copy and paste the cloud-init user data content to Machine Pools &gt; Show Advanced &gt; User Data. The cloud-provider-config file will be created after you apply the cloud-init user data above. You can find it on the guest Kubernetes nodes at the path /var/lib/rancher/rke2/etc/config-files/cloud-provider-config. Configure the Cloud Provider either to Default - RKE2 Embedded or External. Select Create to create your RKE2 cluster. Once the RKE2 cluster is ready, install the Harvester CSI Driver chart from the Rancher marketplace. You do not need to change the cloud-config path by default. note If you prefer not to install the Harvester CSI driver using Rancher (Apps &gt; Charts), you can use Helm instead. The Harvester CSI driver is packaged as a Helm chart. For more information, see https://charts.harvesterhci.io. By following the above steps, you should be able to see those CSI driver pods are up and running on the kube-system namespace, and you can verify it by provisioning a new PVC using the default StorageClass harvester on your RKE2 cluster. Deploying with Harvester K3s node driver​ You can follow the Deploy Harvester CSI Driver steps described in the RKE2 section. The only difference is in generating the cloud-init config where you need to specify the provider type as k3s: ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; k3s Customize the Default StorageClass​ The Harvester CSI driver provides the interface for defining the default StorageClass. If the default StorageClass in unspecified, the Harvester CSI driver uses the default StorageClass of the host Harvester cluster. You can use the parameter host-storage-class to customize the default StorageClass. Create a StorageClass for the host Harvester cluster. Example: Deploy the CSI driver with the parameter host-storage-class. Example: Verify that the Harvester CSI driver is ready. On the PersistentVolumeClaims screen, create a PVC. Select Use a Storage Class to provision a new Persistent Volume and specify the StorageClass you created. Example: Once the PVC is created, note the name of the provisioned volume and verify that the status is Bound. Example: On the Volumes screen, verify that the volume was provisioned using the StorageClass that you created. Example: Passthrough Custom StorageClass​ Beginning with Harvester CSI driver v0.1.15, it's possible to create a PersistentVolumeClaim (PVC) using a different Harvester StorageClass on the guest Kubernetes Cluster. note Harvester CSI driver v0.1.15 is supported out of the box starting with the following RKE2 versions. For RKE1, manual installation of the CSI driver chart is required: v1.23.16+rke2r1 and laterv1.24.10+rke2r1 and laterv1.25.6+rke2r1 and laterv1.26.1+rke2r1 and laterv1.27.1+rke2r1 and later Prerequisites​ Add the following prerequisites to your Harvester cluster to ensure the Harvester CSI driver displays error messages correctly. Proper RBAC settings are essential for error message visibility, especially when creating a PVC with a non-existent StorageClass, as shown in the image below: Follow these steps to set up RBAC for error message visibility: Create a new clusterrole named harvesterhci.io:csi-driver using the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: apiserver app.kubernetes.io/name: harvester app.kubernetes.io/part-of: harvester name: harvesterhci.io:csi-driver rules: - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - get - list - watch Create a new clusterrolebinding associated with the clusterrole above with the relevant serviceaccount using the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: &lt;namespace&gt;-&lt;serviceaccount name&gt; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: harvesterhci.io:csi-driver subjects: - kind: ServiceAccount name: &lt;serviceaccount name&gt; namespace: &lt;namespace&gt; Make sure the serviceaccount name and namespace match your cloud provider settings. Perform the following steps to retrieve these details. Find the rolebinding associated with your cloud provider: $ kubectl get rolebinding -A |grep harvesterhci.io:cloudprovider default default-rke2-guest-01 ClusterRole/harvesterhci.io:cloudprovider 7d1h Extract the subjects information from this rolebinding: $ kubectl get rolebinding default-rke2-guest-01 -n default -o yaml |yq -e '.subjects' Identify the ServiceAccount information: - kind: ServiceAccount name: rke2-guest-01 namespace: default Deploying​ Now you can create a new StorageClass that you intend to use in your guest Kubernetes cluster. For administrators, you can create a desired StorageClass (e.g., named replica-2) in your bare-metal Harvester cluster. Then, on the guest Kubernetes cluster, create a new StorageClass associated with the StorageClass named replica-2 from the Harvester Cluster: note When choosing a Provisioner, select Harvester (CSI). The Host StorageClass parameter should match the StorageClass name created on the Harvester Cluster.For guest Kubernetes owners, you may request that the Harvester cluster administrator create a new StorageClass.If you leave the Host StorageClass field empty, the default StorageClass of the Harvester cluster will be used. You can now create a PVC based on this new StorageClass, which utilizes the Host StorageClass to provision volumes on the bare-metal Harvester cluster.","keywords":"Harvester harvester Rancher Integration","version":"v1.2"},{"title":"Rancher Integration","type":0,"sectionRef":"#","url":"/v1.2/rancher/index","content":"Rancher Integration Rancher is an open-source multi-cluster management platform. Starting with Rancher v2.6.1, Rancher has integrated Harvester by default to centrally manage VMs and containers. Users can import and manage multiple Harvester clusters using the Rancher Virtualization Management feature. Leveraging the Rancher's authentication feature and RBAC control for multi-tenancy support. For a comprehensive overview of the support matrix, please refer to the Harvester &amp; Rancher Support Matrix. For the network requirements, please refer to the doc here. Deploying Rancher server​ To use Rancher with Harvester, please install Rancher on a separate server. If you want to try out the integration features, you can create a VM in Harvester and install the Rancher server by following the Helm CLI quick start. For production setup, please use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix MetalOutscale (uses Terraform) If you prefer, the following guide will take you through the same process in individual steps. Use this if you want to run Rancher in a different provider, on prem, or if you want to see how easy it is. Manual Install caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged, and your cluster may not be abled to be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. Virtualization management​ With Rancher's virtualization management feature, you can import and manage your Harvester cluster. By clicking one of the imported clusters, you can easily access and manage a range of Harvester cluster resources, including hosts, VMs, images, volumes, and more. Additionally, the virtualization management feature leverages Rancher's existing capabilities, such as authentication with various auth providers and multi-tenancy support. For in-depth insights, please refer to the virtualization management page. Creating Kubernetes clusters using the Harvester node driver​ You can launch a Kubernetes cluster from Rancher using the Harvester node driver. When Rancher deploys Kubernetes onto these nodes, you can choose between Rancher Kubernetes Engine (RKE) or RKE2 distributions. One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected. Starting from Rancher version v2.6.1, the Harvester node driver is included by default. You can refer to the node-driver page for more details. Harvester baremetal container workload support (experimental)​ Available as of Harvester v1.2.0 + Rancher v2.7.6 Starting with Rancher v2.7.6, Harvester introduces a new feature that enables you to deploy and manage container workloads directly to the underlying Harvester cluster. With this feature, you can seamlessly combine the power of virtual machines with the flexibility of containerization, allowing for a more versatile and efficient infrastructure setup. This guide will walk you through enabling and using this experimental feature, highlighting its capabilities and best practices. To enable this new feature flag, follow these steps: Click the hamburger menu and choose the Global Settings tab.Click Feature Flags and locate the new feature flag harvester-baremetal-container-workload.Click the drop-down menu and select Activate to enable this feature.If the feature state changes to Active, the feature is successfully enabled. Key Features​ Unified Dashboard View:Once you've enabled the feature, you can explore the dashboard view of the Harvester cluster, just like you would with other standard Kubernetes clusters. This unified experience simplifies the management and monitoring of both your virtual machines and container workloads from a single, user-friendly interface. Deploy Custom Workloads:This feature lets you deploy custom container workloads directly to the bare-metal Harvester cluster. While this functionality is experimental, it introduces exciting possibilities for optimizing your infrastructure. However, we recommend deploying container and VM workloads in separate namespaces to ensure clarity and separation. note Critical system components such as monitoring, logging, Rancher, KubeVirt, and Longhorn are all managed by the Harvester cluster itself. You can't upgrade or modify these components. Therefore, exercise caution and avoid making changes to these critical system components.It is essential not to deploy any workloads to the system namespaces cattle-system, harvester-system, or longhorn-system. Keeping your workloads in separate namespaces is crucial to maintaining clarity and preserving the integrity of the system components.For best practices, we recommend deploying container and VM workloads in separate namespaces. note With this feature enabled, your Harvester cluster does not appear on the Continuous Delivery page in the Rancher UI. Please check the issue #4482 for further updates.","keywords":"Harvester harvester Rancher rancher Rancher Integration","version":"v1.2"},{"title":"Creating an K3s Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.2/rancher/node/k3s-cluster","content":"Creating an K3s Kubernetes Cluster You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. note Harvester K3s node driver is in Tech Preview.VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For the port requirements of the guest clusters deployed within Harvester, please refer to the port requirements for guest clusters. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create K3s Kubernetes cluster​ You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules. Using Harvester K3s node driver in air gapped environment​ K3s provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with the required packages (e.g., iptables, qemu-guest-agent).Option 2. Go to Show Advanced &gt; User Data to allow VMs to install the required packages via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.2"},{"title":"Harvester Node Driver","type":0,"sectionRef":"#","url":"/v1.2/rancher/node/node-driver","content":"Harvester Node Driver The Harvester node driver, similar to the Docker Machine driver, is used to provision VMs in the Harvester cluster, and Rancher uses it to launch and manage Kubernetes clusters. One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected. Additionally, the Harvester node driver is integrated with the Harvester cloud provider by default, providing built-in load balancer support as well as storage passthrough from the bare-metal cluster to the guest Kubernetes clusters to gain native storage performance. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. note The Harvester node driver only supports cloud images. This is because ISO images usually require additional setup that interferes with a clean deployment (without requiring user intervention), and they are not typically used in cloud environments. Harvester node driver​ Starting from Rancher v2.6.3, the Harvester node driver is enabled by default. You can go to the Cluster Management &gt; Drivers &gt; Node Drivers page to check the Harvester node driver status. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. note Refer to the Rancher downstream cluster support matrix for its supported RKE2 versions and guest OS versions. Changes made to the node driver configuration is not persisted. Any modifications applied will be reset upon restarting the Rancher container. Starting with Harvester node driver v0.6.3, the automatic injection of the qemu-guest-agent has been removed from the backend. If the image you are using does not contain the qemu-guest-agent package, you can still install it via the userdata config. Otherwise, the cluster will not be provisioned successfully. #cloud-config package_update: true packages: - qemu-guest-agent runcmd: - - systemctl - enable - '--now' - qemu-guest-agent.service RKE1 Kubernetes cluster​ Click to learn how to create RKE1 Kubernetes Clusters. RKE2 Kubernetes cluster​ Click to learn how to create RKE2 Kubernetes Clusters. K3s Kubernetes cluster​ Click to learn how to create k3s Kubernetes Clusters. Topology spread constraints​ Available as of v1.0.3 Within your guest Kubernetes cluster, you can use topology spread constraints to manage how workloads are distributed across nodes, accounting for factors such as failure domains like regions and zones. This helps achieve high availability and efficient resource utilization of the Harvester cluster resources. For RKE2 versions before v1.25.x, the minimum required versions to support the topology label sync feature are as follows: Minimum Required RKE2 Version&gt;= v1.24.3+rke2r1 &gt;= v1.23.9+rke2r1 &gt;= v1.22.12+rke2r1 Furthermore, for custom installation, the Harvester cloud provider version should be &gt;= v0.1.4. Sync topology labels to the guest cluster node​ During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone topology labels are supported. Configure topology labels on the Harvester nodes on the Hosts &gt; Edit Config &gt; Labels page. For example, add the topology labels as follows: topology.kubernetes.io/region: us-east-1 topology.kubernetes.io/zone: us-east-1a Create a downstream RKE2 cluster using the Harvester node driver with Harvester cloud provider enabled. We recommend adding the node affinity rules, which prevents nodes from drifting to other zones after VM rebuilding. After the cluster is ready, confirm that those topology labels are successfully synchronized to the nodes on the guest Kubernetes cluster. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints. note For Harvester cloud provider &gt;= v0.2.0, topology labels on the Harvester node will be automatically resynchronized when a VM (corresponding to the guest node) undergoes migration or update. For Harvester cloud provider &lt; v0.2.0, label synchronization will only occur during the initialization of guest nodes. To prevent nodes from drifting to different regions or zones, we recommend adding node affinity rules during cluster provisioning. This will allow you to schedule VMs in the same zone even after rebuilding.","keywords":"Harvester harvester Rancher rancher Harvester Node Driver","version":"v1.2"},{"title":"Creating an RKE1 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.2/rancher/node/rke1-cluster","content":"Creating an RKE1 Kubernetes Cluster You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. RKE1 and RKE2 have several slight behavioral differences. Refer to the differences between RKE1 and RKE2 to get some high-level insights. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For port requirements of guest clusters deployed within Harvester, please refer to the port requirements for guest clusters. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials, you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster.In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential name.Select &quot;Imported Harvester Cluster&quot;.Click Create. Create node templates​ You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials.Configure Instance Options: Configure the CPU, memory, and diskSelect an OS image that is compatible with the cloud-init config.Select a network that the node driver is able to connect to; currently, only VLAN is supported.Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu. (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs:Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration &gt; Node Templates: Check the Advanced Options tab and click Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Create an RKE1 Kubernetes cluster​ Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE1.Select Harvester node driver.Enter Cluster Name (required).Enter Name Prefix (required).Enter Template (required).Select etcd and Control Plane (required).On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver.Click Create. Using Harvester RKE1 node driver in air-gapped environments​ RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with both qemu-guest-agent and docker.Option 2. Configure the cloud-init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=&quot;http://192.168.0.1:3128&quot; HTTPS_PROXY=&quot;http://192.168.0.1:3128&quot; append: true ","keywords":"","version":"v1.2"},{"title":"Creating an RKE2 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.2/rancher/node/rke2-cluster","content":"Creating an RKE2 Kubernetes Cluster You can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For the port requirements of the guest clusters deployed within Harvester, please refer to the doc here.For RKE2 with Harvester cloud provider support matrix, please refer to the website here. Backward Compatibility Notice​ note Please note a known backward compatibility issue if you're using the Harvester cloud provider version v0.2.2 or higher. If your Harvester version is below v1.2.0 and you intend to use newer RKE2 versions (i.e., &gt;= v1.26.6+rke2r1, v1.25.11+rke2r1, v1.24.15+rke2r1), it is essential to upgrade your Harvester cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester cloud provider. For a detailed support matrix, please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section of the official website. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create RKE2 kubernetes cluster​ Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).(optional) Configure the Show Advanced &gt; User Data to install the required packages of VM. #cloud-config packages: - iptables note Calico and Canal networks require the iptables or xtables-nft package to be installed on the node, for more details, please refer to the RKE2 known issues. Click Create. note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration.Only imported Harvester clusters are supported by the Harvester node driver. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Add workload affinity​ Available as of v1.2.0 + Rancher v2.7.6 The workload affinity rules allow you to constrain which nodes your machines can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels. Workload affinity rules can be added to the machine pools during the cluster creation: Select Show Advanced and choose Add Workload Selector.Select Type, Affinity or Anti-Affinity.Select Priority. Prefered means it's an optional rule, and Required means a mandatory rule.Select the namespaces for the target workloads.Select Add Rule to specify the workload affinity rules.Set Topology Key to specify the label key that divides Harvester hosts into different topologies. See the Kubernetes Pod Affinity and Anti-Affinity Documentation for more details. Update RKE2 Kubernetes cluster​ The fields highlighted below of the RKE2 machine pool represent the Harvester VM configurations. Any modifications to these fields will trigger node reprovisioning. Using Harvester RKE2 node driver in air gapped environment​ RKE2 provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. Calico and Canal require the iptables or xtables-nft package to be installed on the node. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with required packages (e.g., iptables, qemu-guest-agent).Option 2. Go to Show Advanced &gt; User Data to allow VMs to install the required packages via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.2"},{"title":"Resource Quotas","type":0,"sectionRef":"#","url":"/v1.2/rancher/resource-quota","content":"Resource Quotas ResourceQuota is used to limit the usage of resources within a namespace. It helps administrators control and restrict the allocation of cluster resources to ensure fairness and controlled resource distribution among namespaces. In Harvester, ResourceQuota can define usage limits for the following resources: CPU: Limits compute resource usage, including CPU cores and CPU time.Memory: Limits the usage of memory resources in bytes or other recognizable memory units. Set ResourceQuota via Rancher​ In the Rancher UI, administrators can configure resource quotas for namespaces through the following steps: Click the hamburger menu and choose the Virtualization Management tab.Choose one of the clusters and go to Projects/Namespaces &gt; Create Project.Specify the desired project Name. Next, go to the Resource Quotas tab and select the Add Resource option. Within the Resource Type field, select either CPU Limit or Memory Limit and define the Project Limit and Namespace Default Limit values. You can configure the Namespace limits as follows: Find the newly created project, and select Create Namespace.Specify the desired namespace Name, and adjust the limits.Complete the process by selecting Create. Overhead memory of virtual machine​ Upon creating a virtual machine (VM), the VM controller seamlessly incorporates overhead resources into the VM's configuration. These additional resources intend to guarantee the consistent and uninterrupted functioning of the VM. It's important to note that configuring memory limits requires a higher memory reservation due to the inclusion of these overhead resources. For example, consider the creation of a new VM with the following configuration: CPU: 8 coresMemory: 16Gi note The operating system, either Linux or Windows, does not affect overhead calculations. Memory Overhead is calculated in the following sections: Memory PageTables Overhead: This accounts for one bit for every 512b RAM size. For instance, a memory of 16Gi requires an overhead of 32Mi.VM Fixed Overhead: This consists of several components: VirtLauncherMonitorOverhead: 25Mi (the ps RSS for virt-launcher-monitor)VirtLauncherOverhead: 75Mi (the ps RSS for the virt-launcher process)VirtlogdOverhead: 17Mi (the ps RSS for virtlogd)LibvirtdOverhead: 33Mi (the ps RSS for libvirtd)QemuOverhead : 30Mi (the ps RSS for qemu, minus the RAM of its (stressed) guest, minus the virtual page table) 8Mi per CPU (vCPU) Overhead: Additionally, 8Mi of overhead per vCPU is added, along with a fixed 8Mi overhead for IOThread.Extra Added Overhead: This encompasses various factors like video RAM overhead and architecture overhead. Refer to Additional Overhead for further details. This calculation demonstrates that the VM instance necessitates an additional memory overhead of approximately 276Mi. For more information, see Memory Overhead. For more information on how the memory overhead is calculated in Kubevirt, refer to kubevirt/pkg/virt-controller/services/template.go. Automatic adjustment of ResourceQuota during migration​ When the allocated resource quota controlled by the ResourceQuota object reaches its limit, migrating a VM becomes unfeasible. The migration process automatically creates a new pod mirroring the resource requirements of the source VM. If these pod creation prerequisites surpass the defined quota, the migration operation cannot proceed. Available as of v1.2.0 In Harvester, the ResourceQuota values will dynamically expand ahead of migration to accommodate the resource needs of the target virtual machine. After migration, the ResourceQuotas will be reinstated to their prior configurations. Please be aware of the following constrains of the automatic resizing of ResourceQuota: ResourceQuota cannot be changed during VM migration.When raising the ResourceQuota value, if you create, start, or restore other VMs, Harvester will verify if the resources are sufficient based on the original ResourceQuota. If the conditions are not met, the system will alert that the migration process is not feasible.After expanding ResourceQuota, potential resource contention may occur between non-VM pods and VM pods, leading to migration failures. Therefore, deploying custom container workloads and VMs to the same namespace is not recommended.Due to the concurrent limitation of the webhook validator, the VM controller will execute a secondary validation to confirm resource sufficiency. If the resource is insufficient, it will auto config the VM's RunStrategy to Halted, and a new annotation harvesterhci.io/insufficient-resource-quota will be added to the VM object, informing you that the VM was shut down due to insufficient resources.","keywords":"Harvester harvester Rancher rancher Resource Quota","version":"v1.2"},{"title":"Virtualization Management","type":0,"sectionRef":"#","url":"/v1.2/rancher/virtualization-management","content":"Virtualization Management With Rancher's virtualization management capabilities, you can import and manage multiple Harvester clusters. It provides a solution that unifies virtualization and container management from a single pane of glass. Additionally, Harvester leverages Rancher's existing capabilities, such as authentication and RBAC control, to provide full multi-tenancy support. Importing Harvester cluster​ Please refer to the Harvester &amp; Rancher Support Matrix to find a desired Rancher version. You can use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix MetalOutscale (uses Terraform)Manual Install Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server.Specify the Cluster Name and click Create. You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly.Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly.From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy​ In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication, users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions: Define user authorization outside the scope of any particular cluster. Cluster and Project Roles: Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings.A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example​ The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users &amp; Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project.A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab.Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save.Open an incognito browser and log in as project-owner.After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster and project to which you have been assigned.Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed.Create a VM with one of the images that you have uploaded.Log in with another user, e.g., project-readonly, and this user will only have the read permission of the assigned project. note The harvester-public namespace is a predefined namespace accessible to all users assigned to this cluster. Delete Imported Harvester Cluster​ Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management &gt; Harvester Clusters. Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. You will also need to reset the cluster-registration-url setting on the associated Harvester cluster to clean up the Rancher cluster agent. caution Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","keywords":"Harvester Rancher","version":"v1.2"},{"title":"Harvester Terraform Provider","type":0,"sectionRef":"#","url":"/v1.2/terraform/terraform-provider","content":"Harvester Terraform Provider Support Matrix​ Harvester Version\tSupported Terraform Provider Harvester\tSupported Terraformer Harvesterv1.2.0\tv0.6.3\tv1.1.1-harvester v1.1.2\tv0.6.3\tv1.1.1-harvester v1.1.1\tv0.6.3\tv1.1.1-harvester v1.1.0\tv0.6.3\tv1.1.1-harvester Requirements​ Terraform &gt;= 0.13.xGo 1.18 to build the provider plugin Install The Provider​ copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = &quot;harvester/harvester&quot; version = &quot;&lt;replace to the latest release version&gt;&quot; } } } provider &quot;harvester&quot; { # Configuration options } Using the provider​ More details about the provider-specific configurations can be found in the docs. Github Repo: https://github.com/harvester/terraform-provider-harvester","keywords":"","version":"v1.2"},{"title":"Installation","type":0,"sectionRef":"#","url":"/v1.2/troubleshooting/index","content":"Installation The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS)​ Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancherPassword: rancher Meeting hardware requirements​ Check that your hardware meets the minimum requirements to complete installation. Stuck in Loading images. This may take a few minutes...​ Because the system doesn't have a default route, your installer may become &quot;stuck&quot; in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev mgmt-br proto dhcp &lt;-- Does a default route exist? 10.10.0.0/24 dev mgmt-br proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. For more information, see DHCP Server Configuration. Modifying cluster token on agent nodes​ When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg=&quot;Bootstrapping Rancher (v2.7.5/v1.25.9+rke2r1)&quot; msg=&quot;failed to bootstrap system, will retry: generating plan: response 502: 502 Bad Gateway getting cacerts: &lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;\\r\\n&lt;body&gt;\\r\\n&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;\\r\\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\\r\\n&lt;/body&gt;\\r\\n&lt;/html&gt;\\r\\n&quot; Note that the Rancher version and IP address depend on your environment and might differ from the message above. To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml. For example, if the cluster token setup in the server node is ThisIsTheCorrectOne, you will update the token value as follow: token: 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/90_custom.yaml: name: Harvester Configuration stages: ... initramfs: - commands: - rm -f /etc/sysconfig/network/ifroute-mgmt-br files: - path: /etc/rancher/rancherd/config.yaml permissions: 384 owner: 0 group: 0 content: | server: https://$cluster-vip:443 role: agent token: &quot;ThisIsTheCorrectOne&quot; kubernetesVersion: v1.25.9+rke2r1 rancherVersion: v2.7.5 rancherInstallerImage: rancher/system-agent-installer-rancher:v2.7.5 labels: - harvesterhci.io/managed=true extraConfig: disable: - rke2-snapshot-controller - rke2-snapshot-controller-crd - rke2-snapshot-validation-webhook encoding: &quot;&quot; ownerstring: &quot;&quot; note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml. For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting troubleshooting information​ Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file.","keywords":"","version":"v1.2"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.2/troubleshooting/monitoring","content":"Monitoring The following sections contain tips to troubleshoot Harvester Monitoring. Monitoring is unusable​ When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons. Monitoring is unusable due to Pod being stuck in Terminating status​ Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0/3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1/1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod &quot;prometheus-rancher-monitoring-prometheus-0&quot; force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ... Expand PV/Volume Size​ Harvester integrates Longhorn as the default storage provider. Harvester Monitoring uses Persistent Volume (PV) to store running data. When a cluster has been running for a certain time, the Persistent Volume may need to expand its size. Based on the Longhorn Volume expansion guide, Harvester illustrates how to expand the volume size. View Volume​ From Embedded Longhorn WebUI​ Access the embedded Longhorn WebUI according to this document. The Longhorn dashboard default view. Click Volume to list all existing volumes. From CLI​ You can also use kubectl to get all Volumes. # kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 5Gi RWO harvester-longhorn 43h cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 50Gi RWO harvester-longhorn 43h cattle-monitoring-system rancher-monitoring-grafana Bound pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 2Gi RWO harvester-longhorn 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 attached degraded 2147483648 harv31 43h Scale Down a Deployment​ To detach the Volume, you need to scale down the deployment that uses the Volume. The example below is against the PVC claimed by rancher-monitoring-grafana. Find the deployment in the namespace cattle-monitoring-system. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // target deployment rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h Scale down the deployment rancher-monitoring-grafana to 0. # kubectl scale --replicas=0 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment and the volume. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 0/0 0 0 43h // scaled down rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 detached unknown 2147483648 43h // volume is detached Expand Volume​ In the Longhorn WebUI, the related volume becomes Detached. Click the icon in the Operation column, and select Expand Volume. Input a new size, and Longhorn will expand the volume to this size. Scale Up a Deployment​ After the Volume is expanded to target size, you need to scale up the aforementioned deployment to its original replicas. For the above example of rancher-monitoring-grafana, the original replicas is 1. # kubectl scale --replicas=1 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment again. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // scaled up rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h The Volume is attached to the new POD. To now, the Volume is expanded to the new size and the POD is using it smoothly.","keywords":"","version":"v1.2"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/v1.2/troubleshooting/os","content":"Operating System Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the elemental-toolkit. The following sections contain information and tips to help users troubleshoot OS-related issues. How to log in to a Harvester node​ Users can log in to a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~&gt; sudo blkid # Or become root rancher@node1:~&gt; sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only?​ The OS file system, like a container image, is image-based and immutable except in some directories. We recommend using a toolbox container to run programs not packaged in the Harvester OS for debugging purposes. Please see this article to learn how to build and run a toolbox container. The Harvester OS also provides a way to enable the read-write mode temporarily. Please follow the following steps: caution Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0, we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat &gt; /oem/91_hack.yaml &lt;&lt;'EOF' name: &quot;Rootfs Layout Settings for debugrw&quot; stages: rootfs: - if: 'grep -q root=LABEL=COS_STATE /proc/cmdline &amp;&amp; grep -q rd.cos.debugrw /proc/cmdline' name: &quot;Layout configuration for debugrw&quot; environment_file: /run/cos/cos-layout.env environment: RW_PATHS: &quot; &quot; EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters​ note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry &quot;${display_name}&quot; --id cos { # label is kept around for backward compatibility set label=${active_label} set img=/cOS/active.img loopback $loopdev /$img source ($loopdev)/etc/cos/bootargs.cfg linux ($loopdev)$kernel $kernelcmd ${extra_cmdline} ${extra_active_cmdline} nomodeset initrd ($loopdev)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry​ To change the default entry, first check the --id attribute of a menu entry. Grub menu entries are located in the following files: /run/initramfs/cos-state/grub2/grub.cfg: Contains the default, fallback, and recovery entries/run/initramfs/cos-state/grubcustom: Contains the debug entry In the following example, the id of the entry is debug. # cat \\ /run/initramfs/cos-state/grub2/grub.cfg \\ /run/initramfs/cos-state/grubcustom &lt;...&gt; menuentry &quot;${display_name} (debug)&quot; --id debug { search --no-floppy --set=root --label COS_STATE set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd ${extra_cmdline} ${extra_passive_cmdline} ${crash_kernel_params} initrd (loop0)$initramfs } You can configure the default entry by running the following commands: # mount -o remount,rw /run/initramfs/cos-state # grub2-editenv /run/initramfs/cos-state/grub_oem_env set saved_entry=debug If necessary, you can undo the change by running the command grub2-editenv /run/initramfs/cos-state/grub_oem_env unset saved_entry. How to debug a system crash or hang​ Collect crash log​ If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps​ For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/&lt;time&gt; directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","keywords":"","version":"v1.2"},{"title":"VM","type":0,"sectionRef":"#","url":"/v1.2/troubleshooting/vm","content":"VM The following sections contain information useful in troubleshooting issues related to Harvester VM management. VM Start Button is Not Visible​ Issue Description​ On rare occasions, the Start button is unavailable on the Harvester UI for VMs that are Off. Without that button, users are unable to start the VMs. VM General Operations​ On the Harvester UI, the Stop button is visible after a VM is created and started. The Start button is visible after the VM is stopped. When the VM is powered off from inside the VM, both the Start and Restart buttons are visible. General VM Related Objects​ A Running VM​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of all three objects is Running. # kubectl get vm NAME AGE STATUS READY vm8 7m25s Running True # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 78s Running 10.52.0.199 harv41 True # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 1/1 Running 0 80s A VM Stopped Using the Harvester UI​ Only the object vm exists and its status is Stopped. Both vmi and pod disappear. # kubectl get vm NAME AGE STATUS READY vm8 123m Stopped False # kubectl get vmi No resources found in default namespace. # kubectl get pod No resources found in default namespace. # A VM Stopped Using the VM's Poweroff Command​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of vm is Stopped, while the status of pod is Completed. # kubectl get vm NAME AGE STATUS READY vm8 134m Stopped False # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 2m49s Succeeded 10.52.0.199 harv41 False # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 0/1 Completed 0 2m54s Issue Analysis​ When the issue occurs, the objects vm, vmi, and pod exist. The status of the objects is similar to that of A VM Stopped Using the VM's Poweroff Command. Example: The VM ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachine ... status: conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready Similarly, the VMI (virtual machine instance) ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance ... name: ocffm031v000 ... status: activePods: ec36a1eb-84a5-4421-b57b-2c14c1975018: aibfredg02 conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready On the other hand, the pod virt-launcher-ocffm031v000-rrkss is not ready (status: &quot;False&quot;) because the pod has run to completion (reason: &quot;PodCompleted&quot;). The underlying container 0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb is terminated, and the exitCode is 0. - apiVersion: v1 kind: Pod ... name: virt-launcher-ocffm031v000-rrkss ... ownerReferences: - apiVersion: kubevirt.io/v1 ... kind: VirtualMachineInstance name: ocffm031v000 uid: 8d2cf524-7e73-4713-86f7-89e7399f25db uid: ec36a1eb-84a5-4421-b57b-2c14c1975018 ... status: conditions: - lastProbeTime: &quot;2023-07-18T13:48:56Z&quot; lastTransitionTime: &quot;2023-07-18T13:48:56Z&quot; message: the virtual machine is not paused reason: NotPaused status: &quot;True&quot; type: kubevirt.io/virtual-machine-unpaused - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-18T13:48:55Z&quot; reason: PodCompleted status: &quot;True&quot; type: Initialized - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: Ready - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: ContainersReady ... containerStatuses: - containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb image: registry.suse.com/suse/sles/15.4/virt-launcher:0.54.0-150400.3.3.2 imageID: sha256:43bb08efdabb90913534b70ec7868a2126fc128887fb5c3c1b505ee6644453a2 lastState: {} name: compute ready: false restartCount: 0 started: false state: terminated: containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb exitCode: 0 finishedAt: &quot;2023-07-20T08:38:55Z&quot; reason: Completed startedAt: &quot;2023-07-18T13:50:17Z&quot; A critical difference is that the Stop and Start actions appear in the stateChangeRequests property of vm. status: conditions: ... printableStatus: Stopped stateChangeRequests: - action: Stop uid: 8d2cf524-7e73-4713-86f7-89e7399f25db - action: Start Root Cause​ The root cause of this issue is under investigation. It is notable that the source code checks the status of vm and assumes that the object is starting. No Start and Restart operations are added to the object. func (vf *vmformatter) canStart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } .. } func (vf *vmformatter) canRestart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } ... } func (vf *vmformatter) isVMStarting(vm *kubevirtv1.VirtualMachine) bool { for _, req := range vm.Status.StateChangeRequests { if req.Action == kubevirtv1.StartRequest { return true } } return false } Workaround​ To address the issue, you can force delete the pod using the command kubectl delete pod virt-launcher-ocffm031v000-rrkss -n namespace --force. After the pod is successfully deleted, the Start button becomes visible again on the Harvester UI. Related Issue​ https://github.com/harvester/harvester/issues/4659","keywords":"","version":"v1.2"},{"title":"Upgrading Harvester","type":0,"sectionRef":"#","url":"/v1.2/upgrade/index","content":"Upgrading Harvester Upgrade support matrix​ The following table shows the upgrade path of all supported versions. Upgrade from version\tSupported new version(s)v1.1.2/v1.2.0\tv1.2.1 v1.1.1/v1.1.2\tv1.1.3 v1.1.0/v1.1.1\tv1.1.2 Rancher upgrade​ If you are using Rancher to manage your Harvester cluster, we recommend upgrading your Rancher server first. For more information, please refer to the Rancher upgrade guide. For the Harvester &amp; Rancher support matrix, please visit our website here. note Upgrading Rancher will not automatically upgrade your Harvester cluster. You still need to upgrade your Harvester cluster after upgrading Rancher.Upgrading Rancher will not bring your Harvester cluster down. You can still access your Harvester cluster using its virtual IP. Start an upgrade​ caution Before you upgrade your Harvester cluster, we highly recommend: Back up your VMs if needed. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 30 GiB of free system partition space (df -h /usr/local/). If any node in the cluster has less than 30 GiB of free system partition space, the upgrade will be denied. Check free system partition space requirement for more information.Run the pre-check script on a Harvester control-plane node. Please pick a script according to your cluster's version: https://github.com/harvester/upgrade-helpers/tree/main/pre-check. caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Prepare an air-gapped upgrade​ caution Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages. Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso. Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml. Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~&gt; sudo -i rancher@node1:~&gt; kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page. Free system partition space requirement​ Available as of v1.2.0 The minimum free system partition space requirement in Harvester v1.2.0 is 30 GiB, which will be revised in each release. Harvester will check the amount of free system partition space on each node when you select Upgrade. If any node does not meet the requirement, the upgrade will be denied as follows If some nodes do not have enough free system partition space, but you still want to try upgrading, you can customize the upgrade by updating the harvesterhci.io/minFreeDiskSpaceGB annotation of Version object. apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: annotations: harvesterhci.io/minFreeDiskSpaceGB: &quot;30&quot; # the value is pre-defined and may be customized name: 1.2.0 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://192.168.0.181:8000/harvester-master-amd64.iso minUpgradableVersion: 1.1.2 releaseDate: &quot;20230609&quot; caution Setting a smaller value than the pre-defined value may cause the upgrade to fail and is not recommended in a production environment.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v1.2"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/v1.2/upgrade/troubleshooting","content":"Troubleshooting Overview​ Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes. You can click the version in the support matrix table to see if there are any known issues.Dive into the upgrade design proposal. The following section briefly describes phases within an upgrade and possible diagnostic methods. Diagnose the upgrade flow​ A Harvester upgrade process contains several phases. Phase 1: Provision upgrade repository VM.​ The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s Phase 2: Preload container images​ The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase. Phase 3: Upgrade system services​ In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ... Phase 4: Upgrade nodes​ The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node.post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-&lt;hostname&gt;). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... caution Please do not start over an upgrade if the upgrade fails at this phase. Phase 5: Clean-up​ The Harvester controller deletes the upgrade repository VM and all files that are no longer needed. Common operations​ Start over an upgrade​ Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again. Download upgrade logs​ We have designed and implemented a mechanism to automatically collect all the upgrade-related logs and display the upgrade procedure. By default, this is enabled. You can also choose to opt out of such behavior. You can click the Download Log button to download the log archive during an upgrade. Log entries will be collected as files for each upgrade-related Pod, even for intermediate Pods. The support bundle provides a snapshot of the current state of the cluster, including logs and resource manifests, while the upgrade log preserves any logs generated during an upgrade. By combining these two, you can further investigate the issues during upgrades. After the upgrade ended, Harvester stops collecting the upgrade logs to avoid occupying the disk space. In addition, you can click the Dismiss it button to purge the upgrade logs. For more details, please refer to the upgrade log HEP. caution The storage volume for storing upgrade-related logs is 1GB by default. If an upgrade went into issues, the logs may consume all the available space of the volume. To work around such kind of incidents, try the following steps: Detach the log-archive Volume by scaling down the fluentd StatefulSet and downloader Deployment. # Locate the StatefulSet and Deployment $ kubectl -n harvester-system get statefulsets -l harvesterhci.io/upgradeLogComponent=aggregator NAME READY AGE hvst-upgrade-xxxxx-upgradelog-infra-fluentd 1/1 43s $ kubectl -n harvester-system get deployments -l harvesterhci.io/upgradeLogComponent=downloader NAME READY UP-TO-DATE AVAILABLE AGE hvst-upgrade-xxxxx-upgradelog-downloader 1/1 1 1 38s # Scale down the resources to terminate any Pods using the volume $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=0 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=0 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled Expand the volume size via Longhorn dashboard. For more details, please refer to the volume expansion guide. # Here's how to find out the actual name of the target volume $ kubectl -n harvester-system get pvc -l harvesterhci.io/upgradeLogComponent=log-archive -o jsonpath='{.items[].spec.volumeName}' pvc-63355afb-ce61-46c4-8781-377cf962278a Recover the fluentd StatefulSet and downloader Deployment. $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=1 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=1 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled ","keywords":"","version":"v1.2"},{"title":"Upgrade from v1.1.1/v1.1.2 to v1.1.3","type":0,"sectionRef":"#","url":"/v1.2/upgrade/v1-1-1-to-v1-1-3","content":"Upgrade from v1.1.1/v1.1.2 to v1.1.3 General information​ An Upgrade button appears on the Dashboard screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see Start an upgrade. For air-gapped environments, see Prepare an air-gapped upgrade. Known Issues​ 1. The upgrade process is stuck when pre-draining a node. (Case 1)​ Starting from v1.1.0, Harvester waits for all volumes to become healthy before upgrading a node (for clusters with three or more nodes). When this issue occurs, you can check the health of the affected volumes on the embedded Longhorn UI. You can also check the pre-drain job logs. For more troubleshooting information, see Phase 4: Upgrade nodes. 2. The upgrade process is stuck when pre-draining a node. (Case 2)​ An upgrade is stuck, as shown in the screenshot below: Harvester is unable to proceed with the upgrade and the status of two or more nodes is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. The upgrade process is stuck on the first node.​ Harvester attempts to upgrade the first node but is unable to proceed. The upgrade eventually fails because the job is not completed by the expected end time. Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. The status of a Fleet bundle after the upgrade indicates that deployment errors occurred.​ After an upgrade is completed, the status of a bundle managed by Fleet may be ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]. The errors that occurred while deploying the bundle may block the next Harvester upgrade or managedChart update if not addressed. To check the status of bundles, run the following command: kubectl get bundles -A The following output indicates that the issue exists in your cluster. NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. The upgrade process stops after the upgrade repository is created.​ Harvester is unable to retrieve the harvester-release.yaml file and proceed with the upgrade. The following error message is displayed: Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)context deadline exceeded (Client.Timeout exceeded while awaiting headers)` message: This issue was fixed in v1.1.2. For v1.1.0 and v1.1.1 users, however, the workaround is to restart the upgrade process. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. The upgrade is stuck in the &quot;Pre-drained&quot; state.​ This issue could be caused by a misconfigured pod disruption budget (PDB). You can perform the following steps to confirm the cause and use the current workaround. In this example, the affected node is harvester-node-1. Check the name of the instance-manager-e or instance-manager-r pod on the node. $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output shows that the instance-manager-r-d4ed2788 pod is on the node. Check the Rancher logs and verify that the instance-manager-e or instance-manager-r pod cannot be drained. $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check if a PDB is associated with the node. $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager for the associated PDB. $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output does not show the affected node, the issue exists in your cluster. In this example, the output shows harvester-node-2 instead of harvester-node-1. Check the health of all volumes. kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should show that all volumes are marked healthy. If not, consider uncordoning nodes to improve volume health. Remove the misconfigured PDB. kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.2"},{"title":"Harvester","type":0,"sectionRef":"#","url":"/v1.2/troubleshooting/harvester","content":"Harvester Fail to Deploy a Multi-node Cluster Due to Incorrect HTTP Proxy Setting​ ISO Installation Without a Harvester Configuration File​ Configure HTTP Proxy During Harvester Installation​ In some environments, you configure http-proxy of OS Environment during Harvester installation. Configure HTTP Proxy After First Node is Ready​ After the first node is installed successfully, you login into the Harvester GUI to configure http-proxy of Harvester System Settings. Then you continue to add more nodes to the cluster. One Node Becomes Unavailable​ The issue you may encounter: The first node is installed successfully. The second node is installed successfully. The third node is installed successfully. Then the second node changes to Unavialable state and cannot recover automatically. Solution​ When the nodes in the cluster do not use the HTTP Proxy to communicate with each other, after the first node is installed successfully, you need to configure http-proxy.noProxy against the CIDR used by those nodes. For example, your cluster assigns IPs from CIDR 172.26.50.128/27 to nodes via DHCP/static setting, please add this CIDR to noProxy. After setting this, you can continue to add new nodes to the cluster. For more details, please refer to Harvester issue 3091. ISO Installation With a Harvester Configuration File​ When a Harvester configuration file is used in ISO installation, please configure proper http-proxy in Harvester System Settings. PXE Boot Installation​ When PXE Boot Installation is adopted, please configure proper http-proxy in OS Environment and Harvester System Settings. Generate a Support Bundle​ Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Manually Download and Retain a Support Bundle File​ By default, a support bundle file is automatically generated, downloaded, and deleted after you click Create on the Harvester UI. However, you may want to retain a file for various reasons, including the following: You are unable to download the file because of network connectivity errors and other issues. You must use a previously generated file to troubleshoot issues (because generating a support bundle file takes time). You want to view information that only exists in a previously generated file. Even if the file remains in the cluster, the Harvester UI does not provide a download link. Use the following workaround to generate, manually download, and retain a support bundle file: Generate the File and Prevent Automatic Downloading​ On the Harvester UI, click Generate Support Bundle. When the progress indicator reaches 20% to 80%, close the browser tab to prevent automatic downloading of the generated file. Retrieve a list of all support bundles in all namespaces using kubectl. Example: $ kubectl get supportbundle -A NAMESPACE NAME ISSUE_URL DESCRIPTION AGE harvester-system bundle-htl5f sp1 3h43m Retrieve the details of all existing support bundles using the command kubectl get supportbundle -A -o yaml. Example: $ kubectl get supportbundle -A -oyaml apiVersion: v1 items: - apiVersion: harvesterhci.io/v1beta1 kind: SupportBundle metadata: creationTimestamp: &quot;2024-02-02T11:18:09Z&quot; generation: 5 name: bundle-htl5f // resource name namespace: harvester-system resourceVersion: &quot;1218311&quot; uid: a3776373-05fe-4584-8a9a-baac3fa91bbf spec: description: sp1 issueURL: &quot;&quot; status: conditions: - lastUpdateTime: &quot;2024-02-02T11:18:38Z&quot; status: &quot;True&quot; type: Initialized filename: supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-02T11-18-10Z.zip // support bundle file name filesize: 8868712 progress: 100 // 100 means successfully generated state: ready The file is ready for downloading when the value of progress is &quot;100&quot; and the value of state is &quot;ready&quot;. Download the File​ Create a download URL that includes the following information: VIP or DNS nameResource name of the fileParameter ?retain=true: If you do not include this parameter, resources related to the support bundle are automatically deleted after the file is successfully downloaded. Example: https://{vip/dns-name}/v1/harvester/supportbundles/bundle-htl5f/download?retain=true Download the file using either a command-line tool (for example, curl and wget) or a web browser. Example: curl -k https://{vip/dns-name}/v1/harvester/supportbundles/bundle-htl5f/download?retain=true -o sb2.zip Verify that resources related to the support bundle were not deleted. Example: $ kubectl get supportbundle -A NAMESPACE NAME ISSUE_URL DESCRIPTION AGE harvester-system bundle-htl5f sp1 3h43m Delete the Related Resources​ Retained support bundle files consume memory and storage resources. Each file is backed by a supportbundle-manager-bundle* pod in the harvester-system namespace, and the generated ZIP file is stored in the /tmp folder of the pod's memory-based filesystem. Example: $ kubectl get pods -n harvester-system NAME READY STATUS RESTARTS AGE supportbundle-manager-bundle-dtl2k-69dcc69b59-w64vl 1/1 Running 0 8m18s You can delete the related resources using the following method: Manual: Run the command kubectl delete supportbundle -n {namespace} {resource-name}. Deleting a support bundle object automatically deletes the pod that backs it. Example: $ kubectl delete supportbundle -n harvester-system bundle-htl5f supportbundle.harvesterhci.io &quot;bundle-htl5f&quot; deleted $ kubectl get supportbundle -A No resources found Manually Copy the Support Bundle File​ You can run the command kubectl cp to copy the generated file from the backing pod. Example: kubectl cp harvester-system/supportbundle-manager-bundle-dtl2k-69dcc69b59-w64vl:/tmp/support-bundle-kit/supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-02T11-18-10Z.zip bundle.zip Known Limitations​ Replacing the backing pod prevents the support bundle file from being downloaded. The support bundle file is stored in the /tmp folder of the pod's memory-based filesystem so it is removed when the pod is replaced during cluster and node rebooting, Kubernetes pod rescheduling, and other processes. After starting, the new pod regenerates the file but assigns a name that is different from the file name in the support bundle object. Example: A support bundle file is generated and retained. $ kubectl get supportbundle -A -oyaml apiVersion: v1 items: - apiVersion: harvesterhci.io/v1beta1 kind: SupportBundle metadata: creationTimestamp: &quot;2024-02-06T11:01:19Z&quot; generation: 5 name: bundle-yr2vq namespace: harvester-system resourceVersion: &quot;1583252&quot; uid: eb8538cf-886b-4791-a7b0-dbc34dcee524 spec: description: sp2 issueURL: &quot;&quot; status: conditions: - lastUpdateTime: &quot;2024-02-06T11:01:47Z&quot; status: &quot;True&quot; type: Initialized filename: supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-06T11-01-20Z.zip // file is ready to download filesize: 7832010 progress: 100 state: ready kind: List metadata: resourceVersion: &quot;&quot; The backing pod restarts. $ kubectl get pods -n harvester-system supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d -oyaml apiVersion: v1 kind: Pod metadata: ... labels: app: support-bundle-manager pod-template-hash: c5484fbdf rancher/supportbundle: bundle-yr2vq name: supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d namespace: harvester-system containerStatuses: - containerID: containerd://ea82b63875c18a2b5b36afea6a47a99a5efd26464f94d401cde1727d175ef740 ... name: manager ready: true restartCount: 1 started: true state: running: startedAt: &quot;2024-02-06T11:05:33Z&quot; // pod's latest starting timestamp, newer than the timestamp in support bundle's file name The backing pod regenerates the file after it starts. The name of the regenerated file is different from the file name recorded in the support bundle object. $ kubectl exec -i -t -n harvester-system supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d -- ls /tmp/support-bundle-kit -alth total 2.2M drwxr-xr-x 3 root root 4.0K Feb 6 11:05 . -rw-r--r-- 1 root root 2.2M Feb 6 11:05 supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-06T11-05-34Z.zip // different with above file name Attempts to download the regenerated file fail. The following download URL cannot be used to access the regenerated file. https://{vip/dns-name}/v1/harvester/supportbundles/bundle-yr2vq/download?retain=true. Retained support bundle files may affect system and node rebooting, node draining, and system upgrades. Retained support bundle files are backed by pods in the harvester-system namespace. These pods are replaced during system and node rebooting, node draining, and system upgrades, consuming CPU and memory resources. Moreover, the regenerated files are very similar in content to the retained files, which means that storage resources are also unnecessarily consumed. For more information, see Issue 3383. Access Embedded Rancher and Longhorn Dashboards​ Available as of v1.1.0 You can now access the embedded Rancher and Longhorn dashboards directly on the Support page, but you must first go to the Preferences page and check the Enable Extension developer features box under Advanced Features. note We only support using the embedded Rancher and Longhorn dashboards for debugging and validation purposes. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here. I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers​ If you changedSSL/TLS enabled protocols and ciphers settingsand you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl: # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{&quot;protocols&quot;:&quot;TLS99&quot;,&quot;ciphers&quot;:&quot;WRONG_CIPHER&quot;}' # &lt;- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly. Network interfaces are not showing up​ You may need help finding the correct interface with a 10G uplink since the interface is not showing up. The uplink doesn't show up when the ixgbe module fails to load because an unsupported SFP+ module type is detected. How to identify the issue with the unsupported SFP?​ Execute the command lspci | grep -i net to see the number of NIC ports connected to the motherboard. By running the command ip a, you can gather information about the detected interfaces. If the number of detected interfaces is less than the number of identified NIC ports, then it's likely that the problem arises from using an unsupported SFP+ module. Testing​ You can perform a simple test to verify whether the unsupported SFP+ is the cause. Follow these steps on a running node: Create the file /etc/modprobe.d/ixgbe.conf manually with the content: options ixgbe allow_unsupported_sfp=1 Then run following command: rmmod ixgbe &amp;&amp; modprobe ixgbe If the above steps are successful and the missing interface shows, we can confirm that the issue is an unsupported SFP+. However, the above test is not permanent and will be flushed out once rebooted. Solution​ Due to support issues, Intel restricts the types of SFPs used on their NICs. To make the above changes persistent, adding the following content to a config.yaml during installation is recommended. os: write_files: - content: | options ixgbe allow_unsupported_sfp=1 path: /etc/modprobe.d/ixgbe.conf - content: | name: &quot;reload ixgbe module&quot; stages: boot: - commands: - rmmod ixgbe &amp;&amp; modprobe ixgbe path: /oem/99_ixgbe.yaml ","keywords":"","version":"v1.2"},{"title":"Upgrade from v1.1.0/v1.1.1 to v1.1.2","type":0,"sectionRef":"#","url":"/v1.2/upgrade/v1-1-to-v1-1-2","content":"Upgrade from v1.1.0/v1.1.1 to v1.1.2 danger Please do not upgrade a running cluster to v1.1.2 if your machine has an Intel E810 NIC card. We saw some reports that the NIC card has a problem when added to a bonding device. Please check this issue for more info: https://github.com/harvester/harvester/issues/3860. General information​ Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known Issues​ 1. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 2. An upgrade is stuck when pre-draining a node (case 2)​ An upgrade is stuck, as shown in the screenshot below: And you can also observe that multiple nodes' status is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. After an upgrade, a fleet bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]​ There is a chance fleet-managed bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] after an upgrade. To check if this happened, run the following command: kubectl get bundles -A If you see the following output, it's possible that your cluster has hit the issue: NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. An upgrade stops because it can't retrieve the harvester-release.yaml file​ An upgrade is stopped with the Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) message: We have fixed this issue in v1.1.2. But for v1.1.0 and v1.1.1 users, the workaround is to start over an upgrade. Please refer to Start over an upgrade. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: This could be caused by a misconfigured PDB. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.2"},{"title":"Upgrade from v1.1.2/v1.2.0 to v1.2.1","type":0,"sectionRef":"#","url":"/v1.2/upgrade/v1-2-0-to-v1-2-1","content":"Upgrade from v1.1.2/v1.2.0 to v1.2.1 Important changes to this version​ Harvester v1.2.1 fixes upgrade known issues found in v1.2.0, we suggest upgrading v1.1.2 and v1.2.0 clusters to v1.2.1. The known issues found in v1.2.0 are: An Upgrade is stuck in the Post-draining stateAn upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent For clusters already upgraded to v1.2.0, please refer to the release note for new changes. General information​ tip Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this URL for the script. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ Please check v1.2.0 known issues.","keywords":"","version":"v1.2"},{"title":"Upload Images","type":0,"sectionRef":"#","url":"/v1.2/upload-image","content":"Upload Images Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL​ To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File​ Currently, qcow2, raw, and ISO images are supported. note Please do not refresh the page until the file upload is finished. HTTP 413 Error in Rancher Multi-Cluster Management​ You can upload images from the Multi-Cluster Management screen on the Rancher UI. When the status of an image is Uploading but the progress indicator displays 0% for an extended period, check the HTTP response status code. 413 indicates that the size of the request body exceeds the limit. The maximum request body size should be specific to the cluster that is hosting Rancher (for example, RKE2 clusters have a default limit of 1 MB but no such limit exists in K3s clusters). The current workaround is to upload images from the Harvester UI. If you choose to upload images from the Rancher UI, you may need to configure related settings on the ingress server (for example, proxy-body-size in NGINX). If Rancher is deployed on an RKE2 cluster, perform the following steps: Edit the Rancher ingress. $ kubectl -n cattle-system edit ingress rancher Specify a value for nginx.ingress.kubernetes.io/proxy-body-size. Example: Delete the stuck image, and then restart the upload process. Create Images via Volumes​ On the Volumes page, click Export Image. Enter the image name and select a StorageClass to create an image. Image StorageClass​ When creating an image, you can select a StorageClass and use its pre-defined parameters like replicas, node selectors and disk selectors . note The image will not use the StorageClass selected here directly. It's just a StorageClass template. Instead, it will create a special StorageClass under the hood with a prefix name of longhorn-. This is automatically done by the Harvester backend, but it will inherit the parameters from the StorageClass you have selected. Image Labels​ You can add labels to the image, which will help identify the OS type more accurately. Also, you can add any custom labels for filtering if needed. If your image name or URL contains any valid information, the UI will automatically recognize the OS type and image category for you. If not, you can also manually specify those corresponding labels on the UI. Known Issues​ Attempts to download images while storage network settings are being configured will fail. Use the following workaround to download images without triggering an HTTP 502 error: Obtain the name and namespace of the image. $ kubectl get virtualmachineimages.harvesterhci.io -A -o json | jq -r '.items[] | select(.spec.displayName == &quot;&lt;image name from Harvester GUI&gt;&quot;) | .metadata.namespace + &quot;/&quot; + .metadata.name' Example: $ kubectl get virtualmachineimages.harvesterhci.io -A -o json | jq -r '.items[] | select(.spec.displayName == &quot;jammy-server-cloudimg-amd64.img&quot;) | .metadata.namespace + &quot;/&quot; + .metadata.name' default/image-h6dwf Obtain the name of the related backing image. $ kubectl get backingimage -A -o json | jq -r '.items[] | select(.metadata.annotations[&quot;harvesterhci.io/imageId&quot;] == &quot;&lt;image namespace&gt;/&lt;image name&gt;&quot;) | .metadata.name' Example: $ kubectl get backingimage -A -o json | jq -r '.items[] | select(.metadata.annotations[&quot;harvesterhci.io/imageId&quot;] == &quot;default/image-h6dwf&quot;) | .metadata.name' default-image-h6dwf Obtain the image file path from the backing image manager. $ kubectl get backingimagemanagers.longhorn.io -A -o json | jq -r '.items[] | select(.spec.backingImages.&quot;&lt;backing image name&gt;&quot; != null) | .spec' Example: $ kubectl get backingimagemanagers.longhorn.io -A -o json | jq -r '.items[] | select(.spec.backingImages.&quot;default-image-h6dwf&quot; != null) | .spec' { &quot;backingImages&quot;: { &quot;default-image-dp85d&quot;: &quot;df08a47d&quot;, &quot;default-image-h6dwf&quot;: &quot;dda82f44&quot; }, &quot;diskPath&quot;: &quot;/var/lib/harvester/defaultdisk&quot;, &quot;diskUUID&quot;: &quot;ac97e1b6-7a2e-4125-9589-2247ea8fa93f&quot;, &quot;image&quot;: &quot;longhornio/backing-image-manager:v1.6.0&quot;, &quot;nodeID&quot;: &quot;img-encrypter&quot; } Connect to the corresponding node and then check if the file path matches. $ ls &lt;diskPath&gt;/backing-images/&lt;backing image name&gt;-&lt;backimg image UUID&gt; Example: $ ls /var/lib/harvester/defaultdisk/backing-images/default-image-h6dwf-dda82f44 backing backing.cfg Download the image. $ scp &lt;diskPath&gt;/backing-images/&lt;backing image name&gt;-&lt;backimg image UUID&gt;/backing &lt;destination host&gt; Example: $ scp /var/lib/harvester/defaultdisk/backing-images/default-image-h6dwf-dda82f44/backing rancher@host:~/iso Related issue: [BUG] Download backing image failed with HTTP 502 error if Storage Network configured","keywords":"Harvester harvester Rancher rancher Import Images","version":"v1.2"},{"title":"Access to the Virtual Machine","type":0,"sectionRef":"#","url":"/v1.2/vm/access-to-the-vm","content":"Access to the Virtual Machine Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI​ VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, e.g., the Ubuntu-Minimal-Cloud image, the VM can only be accessed with the serial console. SSH Access​ Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection, which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection, which allows keys or basic auth to be updated dynamically at runtime. Static SSH Key Injection via cloud-init​ You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place. Example of SSH key cloud-init configuration:​ #cloud-config ssh_authorized_keys: - &gt;- ssh-rsa #replace with your public key Dynamic SSH Key Injection via Qemu guest agent​ Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the qemuGuestAgent propagation method. note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click ⋮ button.Click the Edit Config button and go to the Access Credentials tab.Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE).Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI. Access with the SSH Client​ Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@&lt;ip-address-or-hostname&gt; ","keywords":"Harvester harvester Rancher rancher Access to the VM","version":"v1.2"},{"title":"VM Backup, Snapshot & Restore","type":0,"sectionRef":"#","url":"/v1.2/vm/backup-restore","content":"VM Backup, Snapshot &amp; Restore VM Backup &amp; Restore​ Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. note A backup target must be set up. For more information, see Configure Backup Target. If the backup target has not been set, you’ll be prompted with a message to do so. Configure Backup Target​ A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings &gt; backup-target. Parameter\tType\tDescriptionType\tstring\tChoose S3 or NFS Endpoint\tstring\tA hostname or an IP address. It can be left empty for AWS S3. BucketName\tstring\tName of the bucket BucketRegion\tstring\tRegion of the bucket AccessKeyID\tstring\tA user-id that uniquely identifies your account SecretAccessKey\tstring\tThe password to your account Certificate\tstring\tPaste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle\tbool\tUse VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup​ Once the backup target is set, go to the Virtual Machines page.Click Take Backup of the VM actions to create a new VM backup.Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Backup &amp; Snapshot &gt; VM Backups page to view all VM backups. The State will be set to Ready once the backup is complete. Users can either restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup​ To restore a new VM from a backup, follow these steps: Go to the VM Backups page.Specify the new VM name and click Create.A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a backup​ You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the VM Backups page.Click Replace Existing.You can view the restore process from the Virtual Machines page. Restore a new VM on another Harvester cluster​ Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata &amp; content backup feature. prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster​ Check the existing image name (normally starts with image-) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: &quot;&quot; pvcNamespace: &quot;&quot; sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster​ Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster.Go to the VM Backups page.Select the synced VM backup metadata and choose to restore a new VM with a specified VM name.A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page. VM Snapshot &amp; Restore​ Available as of v1.1.0 VM snapshots are created from the Virtual Machines page. The VM snapshot volumes will be stored in the cluster, and they can be used to either restore a new VM or replace an existing VM. Create a VM snapshot​ Go to the Virtual Machines page.Click Take VM Snapshot of the VM actions to create a new VM snapshot.Set a custom snapshot name and click Create to create a new VM snapshot. Result: The snapshot is created. You can also go to the Backup &amp; Snapshot &gt; VM Snapshots page to view all VM snapshots. The State will be set to Ready once the snapshot is complete. Users can either restore a new VM or replace an existing VM using this snapshot. Restore a new VM using a snapshot​ To restore a new VM from a snapshot, follow these steps: Go to the VM Snapshots page.Specify the new VM name and click Create.A new VM will be restored using the snapshot volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a snapshot​ You can replace an existing VM using the snapshot. note You can only choose to retain the previous volumes. Go to the VM Snapshots page.Click Replace Existing.You can view the restore process from the Virtual Machines page. Known issues​ VM Backup Metadata File Naming Conflicts​ Whenever you create a VM backup, Harvester generates a metadata file in the backup target. The metadata file, which is found in &lt;storage-path&gt;/harvester/vmbackups/&lt;vmbackup-namespace&gt;-&lt;vmbackup-name&gt;.cfg, contains VM backup data in JSON format. The naming convention for these metadata files can introduce conflicts. Specifically, files generated for VM backups that were created in different namespaces can have the exact same file name. Example: | VM backup name | Namespace | Metadata file name | | --- | --- | --- | | c | a-b | a-b-c.cfg | | b-c | a | a-b-c.cfg | Harvester v1.3.0 fixes this issue by changing the metadata file path to &lt;storage-path&gt;/harvester/vmbackups/&lt;vmbackup-namespace&gt;/&lt;vmbackup-name&gt;.cfg. If you are using an earlier version, however, ensure that VM backup names do not cause the described file naming conflicts.","keywords":"Harvester harvester Rancher rancher VM Backup  Snapshot & Restore","version":"v1.2"},{"title":"Clone VM","type":0,"sectionRef":"#","url":"/v1.2/vm/clone-vm","content":"Clone VM Available as of v1.1.0 VM can be cloned with/without data. This function doesn't need to take a VM snapshot or set up a backup target first. Clone VM with volume data​ On the Virtual Machines page, click Clone of the VM actions.Set a new VM name and click Create to create a new VM. Clone VM without volume data​ Cloning a VM without volume data creates a new VM with the same configuration as the source VM. On the Virtual Machines page, click Clone of the VM actions.Unclick the clone volume data checkbox.Set a new VM name and click Create to create a new VM.","keywords":"Harvester harvester Rancher rancher Clone VM","version":"v1.2"},{"title":"Create a Windows Virtual Machine","type":0,"sectionRef":"#","url":"/v1.2/vm/create-windows-vm","content":"Create a Windows Virtual Machine Create one or more virtual machines from the Virtual Machines page. note For creating Linux virtual machines, please refer to this page. How to Create a Windows VM​ Header Section​ Create a single VM instance or multiple VM instances.Set the VM name.(Optional) Provide a description for the VM.(Optional) Select the VM template windows-iso-image-base-template. This template will add a volume with the virtio drivers for Windows. Basics Tab​ Configure the number of CPU cores assigned to the VM.Configure the amount of Memory assigned to the VM. note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. caution The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab​ The first volume is an Image Volume with the following values: Name: The value cdrom-disk is set by default. You can keep it or change it.Type: Select cd-rom.Image: Select the Windows image to be installed. See Upload Images for the full description on how to create new images.Size: The value 20 is set by default. You can change it if your image has a bigger size.Bus: The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name: The value rootdisk is set by default. You can keep it or change it.Type: Select disk.StorageClass: You can use the default StorageClass harvester-longhorn or specify a custom one.Size: The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value.Bus: The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI. The third volume is a Container with the following values: Name: The value virtio-container-disk is set by default. You can keep it or change it.Type: Select cd-rom.Docker Image: The value registry.suse.com/suse/vmdp/vmdp:2.5.4.2 is set by default. We recommend not changing this value. Bus: The value SATA is set by default. We recommend not changing this value. You can add additional disks using the buttons Add Volume, Add Existing Volume, Add VM Image, or Add Container. Networks Tab​ The Management Network is added by default with the following values: Name: The value default is set by default. You can keep it or change it.Model: The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown.Network: The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks.Type: The value masquerade is set by default. You can keep it or change it to the other available option, bridge. You can add additional networks by clicking Add Network. caution Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration. Node Scheduling Tab​ Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab​ OS Type: The value Windows is set by default. It's recommended you don't change it.Machine Type: The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value.(Optional) Hostname: Set the VM hostname.(Optional) Cloud Config: Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs.(Optional) Enable TPM, Booting in EFI mode, Secure Boot: Both the TPM 2.0 device and UEFI firmware with Secure Boot are hard requirements for Windows 11. note Currently, only non-persistent vTPMs are supported, and their state is erased after each VM shutdown. Therefore, Bitlocker should not be enabled. Footer Section​ Once all the settings are in place, click on Create to create the VM. note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML. And if you want to cancel all changes made, click Cancel. Installation of Windows​ Select the VM you just created, and click Start to boot up the VM. Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template, the instruction is as follows: Click on Load driver, and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside.Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows, and click Next to load the driver.Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version\tSupported\tDriver pathWindows 7\tNo\tN/A Windows Server 2008\tNo\tN/A Windows Server 2008r2\tNo\tN/A Windows 8 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly. note For full instructions on how to install the VMDP guest driver and tools see the documentation at https://documentation.suse.com/sle-vmdp/2.5/html/vmdp/index.html Known Issues​ Windows ISO unable to boot when using EFI mode​ When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO. VM crashes when reserved memory not enough​ There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. If 256MiB doesn't work, try 512MiB. BSoD (Blue Screen of Death) at first boot time of Windows​ There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name: Harvester Configuration stages: initramfs: - commands: # ... files: - path: /etc/modprobe.d/kvm.conf permissions: 384 owner: 0 group: 0 content: | options kvm ignore_msrs=1 encoding: &quot;&quot; ownerstring: &quot;&quot; # ... note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","keywords":"Harvester harvester Rancher rancher Windows windows Virtual Machine virtual machine Create a Windows VM","version":"v1.2"},{"title":"Upgrade from v1.1.2 to v1.2.0 (not recommended)","type":0,"sectionRef":"#","url":"/v1.2/upgrade/v1-1-2-to-v1-2-0","content":"Upgrade from v1.1.2 to v1.2.0 (not recommended) caution Due to the known issues found v1.2.0: An Upgrade is stuck in the Post-draining stateAn upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent We don't recommend upgrading to v1.2.0. Please upgrade your v1.1.x cluster to v1.2.1. General information​ tip Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this URL for the script. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. An upgrade can't start and reports &quot;validator.harvesterhci.io&quot; denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready​ If a cluster is configured with a storage network, an upgrade can't start with the following message. Related issue: [Doc] upgrade stuck while upgrading system service with alertmanager and prometheus Workaround: https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192 2. An upgrade is stuck in Creating Upgrade Repository​ During an upgrade, Creating Upgrade Repository is stuck in the Pending state: Please perform the following steps to check if the cluster runs into the issue: Check the upgrade repository pod: If the virt-launcher-upgrade-repo-hvst-&lt;upgrade-name&gt; pod stays in ContainerCreating, your cluster might have run into this issue. In this case, proceed with step 2. Check the upgrade repository volume in the Longhorn GUI. Go to Longhorn GUI. Navigate to the Volume page. Check the upgrade repository VM volume. It should be attached to a pod called virt-launcher-upgrade-repo-hvst-&lt;upgrade-name&gt;. If one of the volume's replicas stays in Stopped (gray color), the cluster is running into the issue. Related issue: [BUG] upgrade stuck on create upgrade VM Workaround: Delete the Stopped replica from Longhorn GUI. Or,Start over the upgrade. 3. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 5. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: In this stage, Kubernetes is supposed to drain the workload on the node, but some reasons might cause the process to stall. 5.1 The node contains a Longhorn instance-manager-r pod that serves single-replica volume(s)​ Longhorn doesn't allow draining a node if the node contains the last surviving replica of a volume. To check if a node is running into this situation, follow these steps: List single-replica volumes with the command: kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + &quot;/&quot; + .metadata.name' For example: $ kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + &quot;/&quot; + .metadata.name' longhorn-system/pvc-d1f19bab-200e-483b-b348-c87cfbba85ab Check if the replica resides on the stuck node: List the NodeID of the volume's replica with the command: kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == &quot;&lt;volume&gt;&quot;) | .spec.nodeID' For example: $ kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == &quot;pvc-d1f19bab-200e-483b-b348-c87cfbba85ab&quot;) | .spec.nodeID' node1 If the result shows that the replica resides on the node where the upgrade is stuck (in this example, node1), your cluster is hitting this issue. There are a couple of ways to address this situation. Choose the most appropriate method for your VM: Shut down the VM that uses the single-replica volume to detach the volume, allowing the upgrade to continue.Adjust the volumes's replicas to more than one. Go to Longhorn GUI.Go to the Volume page.Locate the problematic volume and click the icon on the right side, then select Update Replicas Count:Increase the Number of Replicas and select OK. 5.2 Misconfigured Longhorn instance-manager-r Pod Disruption Budgets (PDB)​ A misconfigured PDB could cause this issue. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4 5.3 The instance-manager-e pod could not be drained​ During an upgrade, you might encounter an issue where you can't drain the instance-manager-e pod. When this situation occurs, you will see error messages in the Rancher logs like the ones shown below: $ kubectl logs deployment/rancher -n cattle-system | grep &quot;evicting pod&quot; evicting pod longhorn-system/instance-manager-r-a06a43f3437ab4f643eea7053b915a80 evicting pod longhorn-system/instance-manager-e-452e87d2 error when evicting pods/&quot;instance-manager-r-a06a43f3437ab4f643eea7053b915a80&quot; -n &quot;Longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. error when evicting pods/&quot;instance-manager-e-452e87d2&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check the instance-manager-e to see if any engine instances remain. $ kubectl get instancemanager instance-manager-e-452e87d2 -n longhorn-system -o yaml | yq -e &quot;.status.instances&quot; pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57: spec: name: pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57 status: endpoint: &quot;&quot; errorMsg: &quot;&quot; listen: &quot;&quot; portEnd: 10001 portStart: 10001 resourceVersion: 0 state: running type: &quot;&quot; In this example, the instance-manager-e-452e87d2 still has an engine instance, so you can't drain the pod. You need to check the engine numbers to see if any engine number is redundant. Each PVC should only have one engine. # kubectl get engines -n longhorn-system -l longhornvolume=pvc-7b120d60-1577-4716-be5a-62348271025a NAME STATE NODE INSTANCEMANAGER IMAGE AGE pvc-76120d60-1577-4716-be5a-62348271025a-e-08220662 running harvester-qv4hd instance-manager-e-625d715e2f2e7065d64339f9b31407c2 longhornio/longhorn-engine:v1.4.3 2d12h pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 running harvester-lhlkv instance-manager-e-452e87d2 longhornio/longhorn-engine:v1.4.3 4d10h The example above shows that two engines exist for the same PVC, which is a known issue in Longhorn #6642. To resolve this, delete the redundant engine to allow the upgrade to continue. To determine which engine is the correct one, use the following command: $ kubectl get volumes pvc-7b120d60-1577-4716-be5a-62348271025a -n longhorn-system NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-7b120d60-1577-4716-be5a-62348271025a attached healthy 42949672960 harvester-q4vhd 4d10h In this example, the volume pvc-7b120d60-1577-4716-be5a-62348271025a is active on the node harvester-q4vhd, indicating that the engine not running on this node is redundant. To make the engine inactive and trigger its automatic deletion by Longhorn, run the following command: $ kubectl patch engine pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 -n longhorn-system --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/active&quot;, &quot;value&quot;: false}]' engine.longhorn.io/pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 patched After a few seconds, you can verify the engine's status: $ kubectl get engine -n longhorn-system|grep pvc-7b120d60-1577-4716-be5a-62348271025a pvc-7b120d60-1577-4716-be5a-62348271025a-e-08220b62 running harvester-q4vhd instance-manager-e-625d715e2f2e7065d64339f9631407c2 longhornio/longhorn-engine:v1.4.3 2d13h The instance-manager-e pod should now drain successfully, allowing the upgrade to proceed. Related issue: [BUG] Upgrade (v1.1.2 -&gt; v1.2.0-rc6) stuck in pre-drained 6. An upgrade is stuck in the Upgrading System Service state​ If you notice the upgrade is stuck in the Upgrading System Service state for a long period of time, you might need to investigate if the upgrade is stuck in the apply-manifests phase. POD prometheus-rancher-monitoring-prometheus-0 is to be deleted​ Check the log of the apply-manifests pod to see if the following messages repeat. $ kubectl -n harvester-system logs hvst-upgrade-md6wr-apply-manifests-wqslg --tail=10 Tue Sep 5 10:20:39 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:45 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:50 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:55 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:21:00 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Check if the prometheus-rancher-monitoring-prometheus-0 pod is stuck with the status Terminating. $ kubectl -n cattle-monitoring-system get pods NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Terminating 0 19d Find the UID of the terminating pod with the following command: $ kubectl -n cattle-monitoring-system get pod prometheus-rancher-monitoring-prometheus-0 -o jsonpath='{.metadata.uid}' 33f43165-6faa-4648-927d-69097901471c Get access to any node of the cluster via the console or SSH. Search for the related log messages in /var/lib/rancher/rke2/agent/logs/kubelet.log using the pod's UID. E0905 10:26:18.769199 17399 reconciler.go:208] &quot;operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : UnmountVolume.NewUnmounter failed for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory&quot; err=&quot;UnmountVolume.NewUnmounter failed for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory&quot; If kubelet continues to complain about the volume failing to unmount, apply the following workaround to allow the upgrade to proceed. Forcibly remove the pod stuck with the status Terminating with the following command: kubectl delete pod prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system --force Related issue [BUG] The rancher-monitoring Pod stuck at terminating status when upgrading from v1.1.2 to v1.2.0-rc6 Multiple PODs in cattle-monitoring-system namespace are to be deleted​ Check the log of the apply-manifests pod to see if the following messages repeat. there are still 10 pods in cattle-monitoring-system to be deleted Fri Dec 8 19:06:56 UTC 2023 there are still 10 pods in cattle-monitoring-system to be deleted Fri Dec 8 19:07:01 UTC 2023 When it continues to show 10 (or other number) pods, it encounters below issue. The monitoring feature is deployed from the rancher-monitoring ManagedChart, in Harvester v1.2.0,v1.2.1, this ManagedChart is converted to Harvester Addon feature when upgrading. The ManagedChart rancher-monitoring is deleted, normally, all the generated resources including deployment, daemonset etc. will be deleted automatically. But in this case, those resources are not deleted. The above log reflects the result. Following instructions will guide to delete them manually. Locate the affected resources in the cattle-monitoring-system namespace. Root level resources in cattle-monitoring-system Customized CRD: Prometheus Object: rancher-monitoring-prometheus Sub-object: statefulset.apps/prometheus-rancher-monitoring-prometheus Customized CRD: Alertmanager object: rancher-monitoring-alertmanager Sub-object: statefulset.apps/alertmanager-rancher-monitoring-alertmanager Deployment: rancher-monitoring-grafana rancher-monitoring-kube-state-metrics rancher-monitoring-operator rancher-monitoring-prometheus-adapter Daemonset: rancher-monitoring-prometheus-node-exporter Delete the affected resources. Use below commands to delete them, meanwhile check the log of the `apply-manifests` until it does not report `there are still x pods in cattle-monitoring-system to be deleted`. kubectl delete prometheus rancher-monitoring-prometheus -n cattle-monitoring-system kubectl delete alertmanager rancher-monitoring-alertmanager -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-grafana -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-kube-state-metrics -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-operator -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-prometheus-adapter -n cattle-monitoring-system kubectl delete daemonset rancher-monitoring-prometheus-node-exporter -n cattle-monitoring-system note You may need to run some of the commands more than once to completely delete the resources. Related issue [BUG] upgrade hung on apply-manifests 7. Upgrade stuck in the Upgrading System Service state​ If an upgrade is stuck in an Upgrading System Service state for an extended period, some system services' certificates may have expired. To investigate and resolve this issue, follow these steps: Find the apply-manifest job's name with the command: kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest Example output: NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s Check the job's log with the command: kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system If the following messages appear in the log, continue to the next step: Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Check CAPI cluster's state with the command: kubectl get clusters.provisioning.cattle.io local -n fleet-local -o yaml If you see a condition similar to the one below, it's likely that the cluster has encountered the issue: - lastUpdateTime: &quot;2023-01-17T16:26:48Z&quot; message: 'configuring bootstrap node(s) custom-24cb32ce8387: waiting for probes: kube-controller-manager, kube-scheduler' reason: Waiting status: Unknown type: Updated Find the machine's hostname with the following command, and follow the workaround to see if service certificates expire on a node: kubectl get machines.cluster.x-k8s.io -n fleet-local &lt;machine_name&gt; -o yaml | yq .status.nodeRef.name Replace &lt;machine_name&gt; with the machine's name from the output in the previous step. note If multiple nodes joined the cluster around the same time, you should perform the workaround on all those nodes. Related issue: [DOC/ENHANCEMENT] need to add cert-rotate feature, otherwise upgrade may stuck on Waiting for CAPI cluster fleet-local/local to be provisioned Workaround: https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311 8. The registry.suse.com/harvester-beta/vmdp:latest image is not available in air-gapped environment​ Harvester does not package the registry.suse.com/harvester-beta/vmdp:latest image in the ISO file as of v1.1.0. For Windows VMs before v1.1.0, they used this image as a container disk. However, kubelet may remove old images to free up bytes. Windows VMs can't access an air-gapped environment when this image is removed. You can fix this issue by changing the image to registry.suse.com/suse/vmdp/vmdp:2.5.4.2 and restarting the Windows VMs. Related issue: [BUG] VMDP Image wrong after upgrade to Harvester 1.2.0 9. An Upgrade is stuck in the Post-draining state​ note This known issue is fixed in v1.2.1. The node might be stuck in the OS upgrade process if you encounter the Post-draining state, as shown below. Harvester uses elemental upgrade to help us upgrade the OS. Check the elemental upgrade logs to see if there are any errors. You can check the elemental upgrade logs with the following commands: # View the post-drain job, which should be named `hvst-upgrade-xxx-post-drain-xxx` $ kubectl get pod --selector=harvesterhci.io/upgradeJobType=post-drain -n harvester-system # Check the logs with the following command $ kubectl logs -n harvester-system pods/hvst-upgrade-xxx-post-drain-xxx Suppose you see the following error in the logs. An incomplete state.yaml causes this issue. Flag --directory has been deprecated, 'directory' is deprecated please use 'system' instead INFO[2023-09-13T12:02:42Z] Starting elemental version 0.3.1 INFO[2023-09-13T12:02:42Z] reading configuration form '/tmp/tmp.N6rn4F6mKM' ERRO[2023-09-13T12:02:42Z] Invalid upgrade command setup undefined state partition elemental upgrade failed with return code: 33 + ret=33 + '[' 33 '!=' 0 ']' + echo 'elemental upgrade failed with return code: 33' + cat /host/usr/local/upgrade_tmp/elemental-upgrade-20230913120242.log In this case, Harvester upgrades the elemental-cli to the latest version. It will try to find the state partition from the state.yaml. If the state.yaml is incomplete, there is a chance it will fail to find the state partition. The incomplete state.yaml will look like the following. # Autogenerated file by elemental client, do not edit date: &quot;2023-09-13T08:31:42Z&quot; state: # we are missing `label` here. active: source: dir:///tmp/tmp.01deNrXNEC label: COS_ACTIVE fs: ext2 passive: null Remove this incomplete state.yaml file to work around this issue. (The post-draining will retry every 10 minutes). Remount the state partition to RW. $ mount -o remount,rw /run/initramfs/cos-state Remove the state.yaml. $ rm -f /run/initramfs/cos-state/state.yaml Remount the state partition to RO. $ mount -o remount,ro /run/initramfs/cos-state After performing the steps above, you should pass post-draining with the next retry. Related issues: [BUG] Upgrade stuck with first node in Post-draining stateA potential bug in NewElementalPartitionsFromList which caused upgrade error code 33 Workaround: https://github.com/harvester/harvester/issues/4526#issuecomment-1732853216 10. An upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent​ note This known issue is fixed in v1.2.1. If an upgrade is stuck in an Upgrading System Service state for an extended period, follow these steps to investigate this issue: Find the pods related to the upgrade: kubectl get pods -A | grep upgrade Example output: # kubectl get pods -A | grep upgrade cattle-system system-upgrade-controller-5685d568ff-tkvxb 1/1 Running 0 85m harvester-system hvst-upgrade-vq4hl-apply-manifests-65vv8 1/1 Running 0 87m // waiting for managedchart to be ready .. The pod hvst-upgrade-vq4hl-apply-manifests-65vv8 has the following loop log: Current version: 102.0.0+up40.1.2, Current state: WaitApplied, Current generation: 23 Sleep for 5 seconds to retry Check the status for all bundles. Note thata couple of bundles are OutOfSync: # kubectl get bundle -A NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS ... fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-logging-crd 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-monitoring 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-monitoring-crd 0/1 WaitApplied(1) [Cluster fleet-local/local] The pod fleet-agent-* has following error log: fleet-agent pod log: time=&quot;2023-09-19T12:18:10Z&quot; level=error msg=&quot;Failed to register agent: looking up secret cattle-fleet-local-system/fleet-agent-bootstrap: Post \\&quot;https://192.168.122.199/apis/fleet.cattle.io/ v1alpha1/namespaces/fleet-local/clusterregistrations\\&quot;: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.122.199 because it doesn't contain any IP SANs&quot; Check the ssl-certificates settings in Harvester: From the command line: # kubectl get settings.harvesterhci.io ssl-certificates NAME VALUE ssl-certificates {&quot;publicCertificate&quot;:&quot;-----BEGIN CERTIFICATE-----\\nMIIFNDCCAxygAwIBAgIUS7DoHthR/IR30+H/P0pv6HlfOZUwDQYJKoZIhvcNAQEL\\nBQAwFjEUMBIGA1UEAwwLZXhhbXBsZS5j....&quot;} From the Harvester Web UI: Check the server-url setting, it is the value of VIP: # kubectl get settings.management.cattle.io -n cattle-system server-url NAME VALUE server-url https://192.168.122.199 The root cause: User sets the self-signed ssl-certificates with FQDN in the Harvester settings, but the server-url points to the VIP, the fleet-agent pod fails to register. For example: create self-signed certificate for (*).example.com openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \\ -keyout example.key -out example.crt -subj &quot;/CN=example.com&quot; \\ -addext &quot;subjectAltName=DNS:example.com,DNS:*.example.com&quot; The general outputs are: example.crt, example.key The workaround: Update server-url with the value of https://harv31.example.com # kubectl edit settings.management.cattle.io -n cattle-system server-url setting.management.cattle.io/server-url edited ... # kubectl get settings.management.cattle.io -n cattle-system server-url NAME VALUE server-url https://harv31.example.com After the workaround is applied, the fleet-agent pod is replaced by Rancher automatically and registers successfully, the upgrade continues. Related issue: [BUG] Upgrade to Harvester 1.2.0 fails in fleet-agent due to customer provided SSL certificate without IP SAN Workaround: https://github.com/harvester/harvester/issues/4519#issuecomment-1727132383","keywords":"","version":"v1.2"},{"title":"Edit a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.2/vm/edit-vm","content":"Edit a Virtual Machine How to Edit a VM​ After creating a virtual machine, you can edit your virtual machine by clicking the ⋮ button and selecting the Edit Config button. note In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect. Basics​ On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS. Networks​ You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server. For more details about the network implementation, please refer to the Networking page. Volumes​ You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, edit the size of the expanded volume. After restarting the VM and waiting for the resize to complete, your disk will automatically finish expanding. Access Credentials​ Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has QEMU guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Edit a VM","version":"v1.2"},{"title":"Hot-Plug Volumes","type":0,"sectionRef":"#","url":"/v1.2/vm/hotplug-volume","content":"Hot-Plug Volumes Harvester supports adding hot-plug volumes to a running VM. info Currently, KubeVirt only supports disk bus scsi for hot-plug volumes. For more information, see this issue. Adding Hot-Plug Volumes to a Running VM​ The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select ⋮ &gt; Add Volume. Enter the Name and select the Volume. Click Apply.","keywords":"Harvester Hot-plug Volume","version":"v1.2"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.2/vm/index","content":"Create a Virtual Machine How to Create a VM​ You can create one or more virtual machines from the Virtual Machines page. note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances.Select the namespace of your VMs, only the harvester-public namespace is visible to all users.The VM Name is a required field.(Optional) VM template is optional, you can choose iso-image, raw-image or windows-iso-image template to speed up your VM instance creation.Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision).Select SSH keys or upload new keys.Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured.You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced &gt; Networks first. (Optional) Set node affinity rules on the Node Scheduling tab.(Optional) Set workload affinity rules on the VM Scheduling tab.Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable. Volumes​ You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type\tdescriptiondisk\tA disk disk will expose the volume as an ordinary disk to the VM. cd-rom\tA cd-rom disk will expose the volume as a cd-rom drive to the VM. It is read-only by default. A volume's StorageClass can be specified when adding a new empty volume; for other volumes (such as VM images), the StorageClass is defined during image creation. Adding a container disk​ A container disk is an ephemeral storage volume that can be assigned to any number of VMs and provides the ability to store and distribute VM disks in the container image registry. A container disk is: An ideal tool if you want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Ephemeral volumes are designed for VMs that need more storage but don't care whether that data is stored persistently across VM restarts or only expect some read-only input data to be present in files, like configuration data or secret keys.Not a good solution for any workload that requires persistent root disks across VM restarts. A container disk is added when creating a VM by providing a Docker image. When creating a VM, follow these steps: Go to the Volumes tab.Select Add Container.Enter a Name for the container disk.Choose a disk Type.Add a Docker Image. A disk image, with the format qcow2 or raw, must be placed into the /disk directory.Raw and qcow2 formats are supported, but qcow2 is recommended in order to reduce the container image's size. If you use an unsupported image format, the VM will get stuck in a Running state.A container disk also allows you to store disk images in the /disk directory. An example of creating such a container image can be found here. Choose a Bus type. Networks​ You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type\tdescriptionbridge\tConnect using a Linux bridge masquerade\tConnect using iptables rules to NAT the traffic Management Network​ A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes. Secondary Network​ It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks. In bridge VLAN, virtual machines are connected to the host network through a linux bridge. The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Node Scheduling​ Node Scheduling allows you to constrain which nodes your VMs can be scheduled on based on node labels. See the Kubernetes Node Affinity Documentation for more details. VM Scheduling​ VM Scheduling allows you to constrain which nodes your VMs can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels. For instance, you can combine Required with Affinity to instruct the scheduler to place VMs from two services in the same zone, enhancing communication efficiency. Likewise, the use of Preferred with Anti-Affinity can help distribute VMs of a particular service across multiple zones for increased availability. See the Kubernetes Pod Affinity and Anti-Affinity Documentation for more details. Advanced Options​ Run Strategy​ Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true. RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false. Cloud Configuration​ Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Cloud-init​ Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password: password chpasswd: { expire: False } ssh_pwauth: True Example of network-data configuration using DHCP: network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp You can also use the Advanced &gt; Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM. Installing the QEMU guest agent​ The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service. TPM Device​ Available as of v1.2.0 Trusted Platform Module (TPM) is a cryptoprocessor that secures hardware using cryptographic keys. According to Windows 11 Requirements, the TPM 2.0 device is a hard requirement of Windows 11. In the Harvester UI, you can add an emulated TPM 2.0 device to a VM by checking the Enable TPM box in the Advanced Options tab. note Currently, only non-persistent vTPMs are supported, and their state is erased after each VM shutdown. Therefore, Bitlocker should not be enabled. One-time Boot For ISO Installation​ When creating a VM to boot from cd-rom, you can use the bootOrder option so that the OS can boot from cd-rom during image installation, and boot from the disk when the installation is complete without unmounting the cd-rom. The following example describes how to install an ISO image using openSUSE Leap 15.4: Click Images in the left sidebar and download the openSUSE Leap 15.4 ISO image.Click Virtual Machines in the left sidebar, then create a VM. You need to fill up those VM basic configurations.Click the Volumes tab, In the Image field, select the image downloaded in step 1 and ensure Type is cd-romClick Add Volume and select an existing StorageClass.Drag Volume to the top of Image Volume as follows. In this way, the bootOrder of Volume will become 1. Click Create.Open the VM web-vnc you just created and follow the instructions given by the installer.After the installation is complete, reboot the VM as instructed by the operating system (you can remove the installation media after booting the system).After the VM reboots, it will automatically boot from the disk volume and start the operating system.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Create a VM","version":"v1.2"},{"title":"Live Migration","type":0,"sectionRef":"#","url":"/v1.2/vm/live-migration","content":"Live Migration Live migration means moving a virtual machine to a different host without downtime. note Live migration is not allowed when the virtual machine is using a management network of bridge interface type.Live migration is not allowed when the virtual machine has any volume of the CD-ROM type. Such volumes should be ejected before live migration.Live migration is not allowed when the virtual machine has any volume of the Container Disk type. Such volumes should be removed before live migration.Live migration is not allowed when the virtual machine has any PCIDevice passthrough enabled. Such devices need to be removed before live migration. Starting a Migration​ Go to the Virtual Machines page.Find the virtual machine that you want to migrate and select ⋮ &gt; Migrate.Choose the node to which you want to migrate the virtual machine. Click Apply. When you have node scheduling rules configured for a VM, you must ensure that the target nodes you are migrating to meet the VM's runtime requirements. The list of nodes you get to search and select from will be generated based on: VM scheduling rules.Possibly node rules from the network configuration. Aborting a Migration​ Go to the Virtual Machines page.Find the virtual machine in migrating status that you want to abort. Select ⋮ &gt; Abort Migration. Migration Timeouts​ Completion Timeout​ The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout​ Live migration will also be aborted when copying memory doesn't make any progress in 150s.","keywords":"Harvester harvester Rancher rancher Live Migration","version":"v1.2"},{"title":"Resource Overcommit","type":0,"sectionRef":"#","url":"/v1.2/vm/resource-overcommit","content":"Resource Overcommit Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config, this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600%RAM allocation ratio: 150%Storage allocation ratio: 200% note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated. Configure the global setting overcommit-config​ Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced &gt; Settings page. Find the overcommit-config setting. Configure the desired CPU, Memory, and Storage ratio. Configure overcommit for a single virtual machine​ In situations where you require specific configurations for individual virtual machines without affecting the global settings, you can easily achieve this by modifying the spec.template.spec.domain.resources.limits.&lt;memory|cpu&gt; value on the corresponding virtual machine spec directly. Reserve more memory for the system overhead​ By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ... Why my virtual machines are scheduled unevenly?​ The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling.","keywords":"Harvester Overcommit Overprovision ballooning","version":"v1.2"},{"title":"Clone a Volume","type":0,"sectionRef":"#","url":"/v1.2/volume/clone-volume","content":"Clone a Volume How to Clone a Volume​ After creating a volume, you can clone the volume by following the steps below: Click the ⋮ button and select the Clone option. Select clone volume data. Configure the Name of the new volume and click Create. (Optional) A cloned volume can be added to a VM using Add Existing Volume.","keywords":"Volume","version":"v1.2"},{"title":"Edit a Volume","type":0,"sectionRef":"#","url":"/v1.2/volume/edit-volume","content":"Edit a Volume After creating a volume, you can edit your volume by clicking the ⋮ button and selecting the Edit Config option. Expand a Volume​ You can expand a volume by increasing the value of the Size parameter directly. To prevent the expansion from interference by unexpected data R/W, Harvester supports offline expansion only. You must shut down the VM or detach the volume first if it is attached to a VM, and the detached volume will automatically attach to a random node with maintenance mode to expand automatically. Cancel a Failed Volume Expansion​ If you specify a size larger than Longhorn's capacity during the expansion, the status of the volume expansion will be stuck in Resizing. You can cancel the failed volume expansion by clicking the ⋮ button and selecting the Cancel Expand option. Change the StorageClass of an Existing Volume​ The StorageClass of an existing volume cannot be changed. However, you can change the StorageClass while restoring a new volume from the snapshot by following the steps below: Take a volume snapshot.Select StorageClass when restoring the volume using snapshot.","keywords":"Volume","version":"v1.2"},{"title":"Export a Volume to Image","type":0,"sectionRef":"#","url":"/v1.2/volume/export-volume","content":"Export a Volume to Image You can select and export an existing volume to an image by following the steps below: Click the ⋮ button and select the Export Image option. Select the Namespace of the new image. Configure the Name of the new image. Select an existing StorageClass. (Optional) You can download the exported image from the Images page by clicking the ⋮ button and selecting the Download option.","keywords":"Volume","version":"v1.2"},{"title":"Create a Volume","type":0,"sectionRef":"#","url":"/v1.2/volume/index","content":"Create a Volume Create an Empty Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose New in Source.Select an existing StorageClass.Configure the Size of the volume. Create an Image Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose VM Image in Source.Select an existing Image.Configure the Size of the volume.","keywords":"Volume","version":"v1.2"},{"title":"Volume Snapshots","type":0,"sectionRef":"#","url":"/v1.2/volume/volume-snapshots","content":"Volume Snapshots A volume snapshot represents a snapshot of a volume on a storage system. After creating a volume, you can create a volume snapshot and restore a volume to the snapshot's state. With volume snapshots, you can easily copy or restore a volume's configuration. Create Volume Snapshots​ You can create a volume snapshot from an existing volume by following these steps: Go to the Volumes page. Choose the volume that you want to take a snapshot of and select ⋮ &gt; Take Snapshot. Enter a Name for the snapshot. Select Create to finish creating a new volume snapshot. Check the status of this operation and view all volume snapshots by going to the Volumes page and selecting the Snapshots tab. When the Ready To Use becomes √, the volume snapshot is ready to use. note A recurring snapshot is currently not supported and is tracked via harvester/harvester#572. Restore a new volume from a volume snapshot​ You can restore a new volume from an existing volume snapshot by following these steps: Go to the Backup &amp; Snapshot &gt; Volume Snapshots page or select a Volume from the Volumes page and go to the Snapshots tab. Select ⋮ &gt; Restore. Specify the Name of the new volume. If the source volume is not an image volume, you can select a different StorageClass. You can not change the StorageClass if the source volume is an image volume. Select Create to finish restoring a new volume.","keywords":"Volume Snapshot Volume Snapshots","version":"v1.2"},{"title":"Harvester Overview","type":0,"sectionRef":"#","url":"/v1.3/","content":"Harvester Overview Harvester is a modern, open, interoperable, hyperconverged infrastructure (HCI) solution built on Kubernetes. It is an open-source alternative designed for operators seeking a cloud-native HCI solution. Harvester runs on bare metal servers and provides integrated virtualization and distributed storage capabilities. In addition to traditional virtual machines (VMs), Harvester supports containerized environments automatically through integration with Rancher. It offers a solution that unifies legacy virtualized infrastructure while enabling the adoption of containers from core to edge locations. Harvester Architecture​ The Harvester architecture consists of cutting-edge open-source technologies: Linux OS. Elemental for SLE-Micro 5.3 is at the core of Harvester and is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Built on top of Kubernetes. Kubernetes has become the predominant infrastructure language across all form factors, and Harvester is an HCI solution with Kubernetes under the hood.Virtualization management with KubeVirt. KubeVirt provides virtualization management using KVM on top of Kubernetes.Storage management with Longhorn. Longhorn provides distributed block storage and tiering.Observability with Grafana and Prometheus. Grafana and Prometheus provide robust monitoring and logging. Harvester Features​ Harvester is an enterprise-ready, easy-to-use infrastructure platform that leverages local, direct attached storage instead of complex external SANs. It utilizes Kubernetes API as a unified automation language across container and VM workloads. Some key features of Harvester include: Easy to get started. Since Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image or automatically install it using iPXE scripts.VM lifecycle management. Easily create, edit, clone, and delete VMs, including SSH-Key injection, cloud-init, and graphic and serial port console.VM live migration. Move a VM to a different host or node with zero downtime.VM backup, snapshot, and restore. Back up your VMs from NFS, S3 servers, or NAS devices. Use your backup to restore a failed VM or create a new VM on a different cluster.Storage management. Harvester supports distributed block storage and tiering. Volumes represent storage; you can easily create, edit, clone, or export a volume.Network management. Supports using a virtual IP (VIP) and multiple Network Interface Cards (NICs). If your VMs need to connect to the external network, create a VLAN or untagged network.Integration with Rancher. Access Harvester directly within Rancher through Rancher’s Virtualization Management page and manage your VM workloads alongside your Kubernetes clusters. Harvester Dashboard​ Harvester provides a powerful and easy-to-use web-based dashboard for visualizing and managing your infrastructure. Once you install Harvester, you can access the IP address for the Harvester Dashboard from the node's terminal.","keywords":"Harvester harvester Rancher rancher Harvester Intro","version":"v1.3 (latest)"},{"title":"Addons","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons","content":"Addons Harvester makes optional functionality available as Addons. One of the key reasons for the same is to ensure that Harvester installation footprint can be kept low while allowing users to enable/disable functionality based on their use case or requirements. Some level of customization is allowed for each addon, which depends on the underlying addon. Available as of v1.1.0 Harvester v1.3.0 ships with six Addons: pcidevices-controllervm-import-controllerrancher-monitoringrancher-loggingharvester-seedernvidia-driver-toolkit note harvester-seeder is released as an experimental feature in Harvester v1.2.0 and has an Experimental label added to the Name. You can enable a Disabled by choosing an addon and selecting ⋮ &gt; Enable from the Basic tab. When the addon is enabled successfully, the State will be DeploySuccessful. You can disable an Enabled by choosing an addon and selecting ⋮ &gt; Disable or from the Basic tab. When the addon is disabled successfully, the State will be Disabled. note When an addon is disabled, the configuration data is stored to reuse when the addon is enabled again.","keywords":"","version":"v1.3 (latest)"},{"title":"Nvidia Driver Toolkit","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons/nvidiadrivertoolkit","content":"Nvidia Driver Toolkit Available as of v1.3.0 nvidia-driver-toolkit is an add-on that allows you to deploy out-of-band NVIDIA GRID KVM drivers to your existing Harvester clusters. note The toolkit only includes the correct Harvester OS image, build utilities, and kernel headers that allow NVIDIA drivers to be compiled and loaded from the container. You must download the NVIDIA KVM drivers using a valid NVIDIA subscription. For guidance on identifying the correct driver for your NVIDIA GPU, see the NVIDIA documentation. The Harvester ISO does not include the nvidia-driver-toolkit container image. Because of its size, the image is pulled from Docker Hub by default. If you have an air-gapped environment, you can download and push the image to your private registry. The Image Repository and Image Tag fields on the nvidia-driver-toolkit screen provide information about the image that you must download. note Each new Harvester version will be released with the correct nvidia-driver-toolkit image to ensure that all dependencies required to install the NVIDIA vGPU KVM drivers are available in the image. To enable the addon, users need to perform the following: Provide the Driver Location: which is an http location where nvidia vgpu kvm driver file is located (as shown in the example)update the Image Repository and Image Tag if needed Once the addon is enabled, a nvidia-driver-toolkit daemonset is deployed to the cluster. On pod startup, the entrypoint script will download the nvidia driver from the speificied Driver Location, install the driver and load the kernel drivers. The PCIDevices addon can now leverage this addon to manage the lifecycle of the vGPU devices on nodes containing supported GPU devices.","keywords":"","version":"v1.3 (latest)"},{"title":"Rancher Manager (Experimental)","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons/rancher-vcluster","content":"Rancher Manager (Experimental) Available as of v1.2.0 The rancher-vcluster addon allows you to run Rancher Manager as a workload on the underlying Harvester cluster and is implemented using vcluster. The addon runs a nested K3s cluster in the rancher-vcluster namespace and deploys Rancher to this cluster. During the installation, the ingress for Rancher is synced to the Harvester cluster, allowing end users to access Rancher. Installing rancher-vcluster​ The rancher-vcluster addon is not packaged with Harvester, but you can find it in the experimental-addons repo. Assuming you are using the Harvester kubeconfig, you can run the following commands to install the addon: kubectl apply -f https://raw.githubusercontent.com/harvester/experimental-addons/main/rancher-vcluster/rancher-vcluster.yaml Configuring rancher-vcluster​ After installing the addon, you need to configure it from the Harvester UI as follows: Select Advanced &gt; Addons.Find the rancher-vcluster addon and select ⋮ &gt; Edit Config. In the Hostname field, enter a valid DNS record pointing to the Harvester VIP. This is essential as the vcluster ingress is synced to the parent Harvester cluster. A valid hostname is used to filter ingress traffic to the vcluster workload.In the Bootstrap Password field, enter the bootstrap password for the new Rancher deployed on the vcluster. Once the addon is deployed, Rancher can take a few minutes to become available. You can then access Rancher via the hostname DNS record that you provided. See Rancher Integration for more information. Disabling rancher-vcluster The rancher-vcluster addon is deployed on a vcluster Statefulset that uses a Longhorn PVC. When rancher-vcluster is disabled, the PVC data-rancher-vcluster-0 will remain in the rancher-vcluster namespace. If you enable the addon again, the PVC is re-used, and Rancher will have the old state available again. If you want to wipe the data, ensure that the PVC is deleted.","keywords":"","version":"v1.3 (latest)"},{"title":"Seeder","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons/seeder","content":"Seeder Available as of v1.2.0 The harvester-seeder addon lets you perform out-of-band operations on underlying nodes. This addon can also discover hardware and hardware events for bare-metal nodes that support redfish-based access and then associate the hardware with the corresponding Harvester nodes. You must enable the harvester-seeder addon from the Addons page to get started. Once the addon is enabled, find the desired host and select Edit Config and go to the Out-Of-Band Access tab. seeder leverages ipmi to manage the underlying node hardware. Hardware discovery and event detection require redfish support. Power operations​ Once you've defined the out-of-band config for a node, you can put the node into Maintenance mode, which allows you to shut down or reboot the node as needed. If a node is shut down, you can also select Power On to power it on again: Hardware event aggregation​ If you've enabled Event in Out-of-Band Access, seeder will leverage redfish to query the underlying hardware for information about component failures and fan temperatures. This information is associated with Harvester nodes and can be used as Kubernetes events. info Sometimes, the Out-Of-Band Access section may be stuck with the message Waiting for &quot;inventories.metal.harvesterhci.io&quot; to be ready. In this case, you need to refresh the page. For more information, see this issue.","keywords":"","version":"v1.3 (latest)"},{"title":"Third-Party Storage Support","type":0,"sectionRef":"#","url":"/v1.3/advanced/csidriver","content":"Third-Party Storage Support Available as of v1.2.0 Harvester now offers the capability to install a Container Storage Interface (CSI) in your Harvester cluster. This allows you to leverage external storage for the Virtual Machine's non-system data disk, allowing you to use different drivers tailored for specific needs, whether for performance optimization or seamless integration with your existing in-house storage solutions. note The Virtual Machine (VM) image provisioner in Harvester still relies on Longhorn. Before version 1.2.0, Harvester exclusively supported Longhorn for storing VM data and did not offer support for external storage as a destination for VM data. Prerequisites​ For the Harvester functions to work well, the third-party CSI driver needs to have the following capabilities: Support expansionSupport snapshotSupport cloneSupport block deviceSupport Read-Write-Many (RWX), for Live Migration Create Harvester cluster​ Harvester's operating system follows an immutable design, meaning that most OS files revert to their pre-configured state after a reboot. Therefore, you might need to perform additional configurations before installing the Harvester cluster for third-party CSI drivers. Some CSI drivers require additional persistent paths on the host. You can add these paths to os.persistent_state_paths. Some CSI drivers require additional software packages on the host. You can install these packages with os.after_install_chroot_commands. note Upgrading Harvester causes the changes to the OS in the after-install-chroot stage to be lost. You must also configure the after-upgrade-chroot to make your changes persistent across an upgrade. Refer to Runtime persistent changes before upgrading Harvester. Install the CSI driver​ After installing the Harvester cluster is complete, refer to How can I access the kubeconfig file of the Harvester cluster? to get the kubeconfig of the cluster. With the kubeconfig of the Harvester cluster, you can install the third-party CSI drivers into the cluster by following the installation instructions for each CSI driver. You must also refer to the CSI driver documentation to create the StorageClass and VolumeSnapshotClass in the Harvester cluster. Configure Harvester Cluster​ Before you can make use of Harvester's Backup &amp; Snapshot features, you need to set up some essential configurations through the Harvester csi-driver-config setting. Follow these steps to make these configurations: Login to the Harvester UI, then navigate to Advanced &gt; Settings.Find and select csi-driver-config, and then select ⋮ &gt; Edit Setting to access the configuration options.Set the Provisioner to the third-party CSI driver in the settings.Next, Configure the Volume Snapshot Class Name. This setting points to the name of the VolumeSnapshotClass used for creating volume snapshots or VM snapshots.Similarly, Configure the Backup Volume Snapshot Class Name. This corresponds to the name of the VolumeSnapshotClass responsible for creating VM backups. Use the CSI driver​ After successfully configuring these settings, you can utilize the third-party StorageClass. You can apply the third-party StorageClass when creating an empty volume or adding a new block volume to a VM, enhancing your Harvester cluster's storage capabilities. With these configurations in place, your Harvester cluster is ready to make the most of the third-party storage integration. References​ Use Rook Ceph External Storage with HarvesterUsing NetApp Storage on Harvester","keywords":"","version":"v1.3 (latest)"},{"title":"Custom SUSE VM Images","type":0,"sectionRef":"#","url":"/v1.3/advanced/customsuseimages","content":"Custom SUSE VM Images SUSE provides SUSE Linux Enterprise (SLE) and openSUSE Leap virtual machine (VM) images suitable for use in Harvester. These images are built on the openSUSE Build Service (OBS) using the Kiwi image building tool, and can be used immediately after downloading. For most cases, you can use the Minimal VM Cloud qcow2 images because these include the cloud-init tool necessary for automatic VM configuration. Other image variants require you to log onto the VM console and then perform initial configuration. info The Minimal VM Cloud images were named Minimal VM OpenStack Cloud in releases earlier than SLES 15 SP5 and openSUSE 15.5. Using the openSUSE Build Service (OBS)​ You can create custom images based on what SUSE provides using OBS image templates, which are pre-configured Kiwi image configurations. For example, if you want use other packages with SLE 15 SP5, you can create an image using the SLE 15 SP5 Minimal template. OBS provides an interface for adding packages and automatically builds the image, which you can download and then upload to Harvester. For more information, see the OBS User Guide. 1. Create a custom image based on an existing template.​ Go to https://build.opensuse.org/image_templates. You must sign in to your openSUSE account to access the resources. Select the template that you want to use. Specify a name for the image, and then select Create appliance. OBS automatically builds the image. By default, the interface shows the Overview tab, which contains information such as the number of included packages and the build status. 2. Select image profiles and add packages.​ Go to the Software tab. Select the image profiles that you want OBS to build. info For most cases, you can use the Minimal VM Cloud qcow2 images because these include the cloud-init tool necessary for automatic VM configuration. Other image variants require you to log onto the VM console and then perform initial configuration. (Optional) Add and remove packages. 3. (Optional) Switch to View Package mode.​ View Package mode provides more granular control over configuration. To switch, click the View Package icon in the navigation bar. The Source Files section of the Overview tab shows all the files that comprise your Kiwi template. You can edit any of the files by selecting the corresponding file name. 4. (Optional) Edit the configuration file Minimal.kiwi.​ Select the file name to open the text editor. The &lt;packages type=&quot;image&quot;&gt; section lists the packages to be installed. You can specify additional packages for each image profile. By default, the Cloud image profile (&lt;package type=&quot;image&quot; profiles=&quot;Cloud&quot;&gt;) installs the kernel-default-base package. In the following example, that package is replaced with kernel-default, which includes modules necessary for iSCSI support. 5. Wait for OBS to finish building the image.​ Once the process is completed, the Build Results section on the Overview tab shows the status succeeded. The Build Results section also contains a download link for the new image. 6. Enable publishing to share the image.​ To allow the public to download your custom image, go to the Repositories tab of your OBS project and enable the Publish flag. Your image is published to https://download.opensuse.org/ (under repositories/home:/YOUR_USER_NAME:/branches:/SUSE:/Templates:/Images:/). Using the Kiwi Command-line Tool​ As an alternative to the openSUSE Build Service, you can create images locally using the Kiwi command-line tool. For more information about the tool, see Building Linux System Appliances with KIWI Next Generation (KIWI NG). To create custom images, you must first download the file Minimal.kiwi, and the scripts config.sh and editbootinstall_rpi.sh from the corresponding project on OBS. OS\topenSUSE Build Service ProjectSLE 15 SP5\thttps://build.opensuse.org/package/show/SUSE:SLE-15-SP5:GA/kiwi-templates-Minimal openSUSE Leap 15.5\thttps://build.opensuse.org/package/show/openSUSE:Leap:15.5/kiwi-templates-Minimal SLE 15 SP4\thttps://build.opensuse.org/package/show/SUSE:SLE-15-SP4:GA/kiwi-templates-Minimal openSUSE Leap 15.4\thttps://build.opensuse.org/package/show/openSUSE:Leap:15.4/kiwi-templates-Minimal","keywords":"Custom Images","version":"v1.3 (latest)"},{"title":"Managed DHCP","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons/managed-dhcp","content":"Managed DHCP Available as of v1.3.0 Beginning with v1.3.0, you can configure IP pool information and serve IP addresses to VMs running on Harvester clusters using the embedded Managed DHCP feature. This feature, which is an alternative to the standalone DHCP server, leverages the vm-dhcp-controller add-on to simplify guest cluster deployment. note Harvester uses the planned infrastructure network so you must ensure that network connectivity is available and plan the IP pools in advance. Install and Enable the vm-dhcp-controller Add-On​ The vm-dhcp-controller add-on is not packed into the Harvester ISO, but you can download it from the experimental-addons repository. You can install the add-on by running the following command: kubectl apply -f https://raw.githubusercontent.com/harvester/experimental-addons/main/harvester-vm-dhcp-controller/harvester-vm-dhcp-controller.yaml After installation, enable the add-on on the Dashboard screen of the Harvester UI or using the command-line tool kubectl. Usage​ On the Dashboard screen of the Harvester UI, create a VM Network. Create an IPPool object using the command-line tool kubectl. cat &lt;&lt;EOF | kubectl apply -f - apiVersion: network.harvesterhci.io/v1alpha1 kind: IPPool metadata: name: net-48 namespace: default spec: ipv4Config: serverIP: 192.168.48.77 cidr: 192.168.48.0/24 pool: start: 192.168.48.81 end: 192.168.48.90 exclude: - 192.168.48.81 - 192.168.48.90 router: 192.168.48.1 dns: - 1.1.1.1 leaseTime: 300 networkName: default/net-48 EOF Create a VM that is connected to the VM Network you previously created. Wait for the corresponding VirtualMachineNetworkConfig object to be created and for the MAC address of the VM's network interface to be applied to the object. Check the .status field of the IPPool and VirtualMachineNetworkConfig objects, and verify that the IP address is allocated and assigned to the MAC address. $ kubectl get ippools.network net-48 -o yaml apiVersion: network.harvesterhci.io/v1alpha1 kind: IPPool metadata: creationTimestamp: &quot;2024-02-15T13:17:21Z&quot; finalizers: - wrangler.cattle.io/vm-dhcp-ippool-controller generation: 1 name: net-48 namespace: default resourceVersion: &quot;826813&quot; uid: 5efd44b7-3796-4f02-947e-3949cb4c8e3d spec: ipv4Config: cidr: 192.168.48.0/24 dns: - 1.1.1.1 leaseTime: 300 pool: end: 192.168.48.90 exclude: - 192.168.48.81 - 192.168.48.90 start: 192.168.48.81 router: 192.168.48.1 serverIP: 192.168.48.77 networkName: default/net-48 status: agentPodRef: name: default-net-48-agent namespace: harvester-system conditions: - lastUpdateTime: &quot;2024-02-15T13:17:21Z&quot; status: &quot;True&quot; type: Registered - lastUpdateTime: &quot;2024-02-15T13:17:21Z&quot; status: &quot;True&quot; type: CacheReady - lastUpdateTime: &quot;2024-02-15T13:17:30Z&quot; status: &quot;True&quot; type: AgentReady - lastUpdateTime: &quot;2024-02-15T13:17:21Z&quot; status: &quot;False&quot; type: Stopped ipv4: allocated: 192.168.48.81: EXCLUDED 192.168.48.84: ca:70:82:e6:84:6e 192.168.48.90: EXCLUDED available: 7 used: 1 lastUpdate: &quot;2024-02-15T13:48:20Z&quot; $ kubectl get virtualmachinenetworkconfigs.network test-vm -o yaml apiVersion: network.harvesterhci.io/v1alpha1 kind: VirtualMachineNetworkConfig metadata: creationTimestamp: &quot;2024-02-15T13:48:02Z&quot; finalizers: - wrangler.cattle.io/vm-dhcp-vmnetcfg-controller generation: 2 labels: harvesterhci.io/vmName: test-vm name: test-vm namespace: default ownerReferences: - apiVersion: kubevirt.io/v1 kind: VirtualMachine name: test-vm uid: a9f8ce12-fd6c-4bd2-b266-245d8e77dae3 resourceVersion: &quot;826809&quot; uid: 556440c7-eeeb-4daf-9c98-60ab39688ba8 spec: networkConfig: - macAddress: ca:70:82:e6:84:6e networkName: default/net-48 vmName: test-vm status: conditions: - lastUpdateTime: &quot;2024-02-15T13:48:20Z&quot; status: &quot;True&quot; type: Allocated - lastUpdateTime: &quot;2024-02-15T13:48:02Z&quot; status: &quot;False&quot; type: Disabled networkConfig: - allocatedIPAddress: 192.168.48.84 macAddress: ca:70:82:e6:84:6e networkName: default/net-48 state: Allocated Check the VM's serial console and verify that the IP address is correctly configured on the network interface (via DHCP). vm-dhcp-controller Pods and CRDs​ When the vm-dhcp-controller add-on is enabled, the following types of pods run: Controller: Reconciles CRD objects to determine allocation and mapping between IP and MAC addresses. The results are persisted in the IPPool objects.Webhook: Validates and mutates CRD objects when receiving requests (creation, updating, and deletion)Agent: Serves DHCP requests and ensures that the internal DHCP lease store is up to date. This is accomplished by syncing the specific IPPool object that the agent is associated with. Agents are spawned on-demand whenever you create new IPPool objects. The vm-dhcp-controller introduces the following new CRDs. IPPool (ippl)VirtualMachineNetworkConfig (vmnetcfg) IPPool CRD​ The IPPool CRD allows you to define IP pool information. You must map each IPPool object to a specific NetworkAttachmentDefinition (NAD) object, which must be created beforehand. note Multiple CRDs named &quot;IPPool&quot; are used in the Harvester ecosystem, including a similarly-named CRD in the loadbalancer.harvesterhci.io API group. To avoid issues, ensure that you are working with the IPPool CRD in the network.harvesterhci.io API group. For more information about IPPool CRD operations in relation to load balancers, see IP Pool. Example: apiVersion: network.harvesterhci.io/v1alpha1 kind: IPPool metadata: name: example namespace: default spec: ipv4Config: serverIP: 192.168.100.2 # The DHCP server's IP address cidr: 192.168.100.0/24 # The subnet information, must be in the CIDR form pool: start: 192.168.100.101 end: 192.168.100.200 exclude: - 192.168.100.151 - 192.168.100.187 router: 192.168.100.1 # The default gateway, if any dns: - 1.1.1.1 domainName: example.com domainSearch: - example.com ntp: - pool.ntp.org leaseTime: 300 networkName: default/example # The namespaced name of the NAD object After the IPPool object is created, the controller reconciliation process initializes the IP allocation module and spawns the agent pod for the network. $ kubectl get ippools.network example NAME NETWORK AVAILABLE USED REGISTERED CACHEREADY AGENTREADY example default/example 98 0 True True True VirtualMachineNetworkConfig CRD​ The VirtualMachineNetworkConfig CRD resembles a request for IP address issuance and is associated with NetworkAttachmentDefinition (NAD) objects. A sample VirtualMachineNetworkConfig object looks like the following: apiVersion: network.harvesterhci.io/v1alpha1 kind: VirtualMachineNetworkConfig metadata: name: test-vm namespace: default spec: networkConfig: - macAddress: 22:37:37:82:93:7d networkName: default/example vmName: test-vm After the VirtualMachineNetworkConfig object is created, the controller attempts to retrieve a list of unused IP addresses from the IP allocation module for each recorded MAC address. The IP-MAC mapping is then updated in the VirtualMachineNetworkConfig object and the corresponding IPPool objects. note Manual creation of VirtualMachineNetworkConfig objects for VMs is unnecessary in most cases because vm-dhcp-controller handles that task during the VirtualMachine reconciliation process. Automatically-created VirtualMachineNetworkConfig objects are deleted when VirtualMachine objects are removed.","keywords":"","version":"v1.3 (latest)"},{"title":"PCI Devices","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons/pcidevices","content":"PCI Devices Available as of v1.1.0 A PCIDevice in Harvester represents a host device with a PCI address. The devices can be passed through the hypervisor to a VM by creating a PCIDeviceClaim resource, or by using the UI to enable passthrough. Passing a device through the hypervisor means that the VM can directly access the device, and effectively owns the device. A VM can even install its own drivers for that device. This is accomplished by using the pcidevices-controller addon. To use the PCI devices feature, users need to enable the pcidevices-controller addon first. Once the pcidevices-controller addon is deployed successfully, it can take a few minutes for it to scan and the PCIDevice CRDs to become available. Enabling Passthrough on a PCI Device​ Now go to the Advanced -&gt; PCI Devices page: Search for your device by vendor name (e.g. NVIDIA, Intel, etc.) or device name. Select the devices you want to enable for passthrough: Then click Enable Passthrough and read the warning message. If you still want to enable these devices, click Enable and wait for all devices to be Enabled. caution Please do not use host-owned PCI devices (e.g., management and VLAN NICs). Incorrect device allocation may cause damage to your cluster, including node failure. Attaching PCI Devices to a VM​ After enabling these PCI devices, you can navigate to the Virtual Machines page and select Edit Config to pass these devices. Select PCI Devices and use the Available PCI Devices drop-down. Select the devices you want to attach from the list displayed and then click Save. Using a passed-through PCI Device inside the VM​ Boot the VM up, and run lspci inside the VM, the attached PCI devices will show up, although the PCI address in the VM won't necessarily match the PCI address in the host. Installing drivers for your PCI device inside the VM​ This is just like installing drivers in the host. The PCI passthrough feature will bind the host device to the vfio-pci driver, which gives VMs the ability to use their own drivers. Here is a screenshot of NVIDIA drivers being installed in a VM. It includes a CUDA example that proves that the device drivers work. SRIOV Network Devices​ Available as of v1.2.0 The pcidevices-controller addon can now scan network interfaces on the underlying hosts and check if they support SRIOV Virtual Functions (VFs). If a valid device is found, pcidevices-controller will generate a new SRIOVNetworkDevice object. To create VFs on a SriovNetworkDevice, you can click ⋮ &gt; Enable and then define the Number of Virtual Functions. The pcidevices-controller will define the VFs on the network interface and report the new PCI device status for the newly created VFs. On the next re-scan, the pcidevices-controller will create the PCIDevices for VFs. This can take up to 1 minute. You can now navigate to the PCI Devices page to view the new devices. We have also introduced a new filter to help you filter PCI devices by the underlying network interface. The newly created PCI device can be passed through to virtual machines like any other PCI device.","keywords":"","version":"v1.3 (latest)"},{"title":"Single-Node Clusters","type":0,"sectionRef":"#","url":"/v1.3/advanced/singlenodeclusters","content":"Single-Node Clusters As of Harvester release v1.2.0, single-node clusters are supported for implementations that require minimal initial deployment resources or that can tolerate lower resiliency. You can create single-node clusters using the standard installation methods (ISO, USB, and PXE boot). Single-node clusters support most Harvester features, including the creation of RKE2 clusters and node upgrades (with some limitations). However, this deployment type has the following key disadvantages: No high availability: Errors and updates that require rebooting of the node cause downtime to running VMs.No multi-replica support: Only one replica is created for each volume in Longhorn.No live migration and zero-downtime support during upgrades. Prerequisites​ Before you begin deploying your single-node cluster, ensure that the following requirements are addressed. Hardware: Use server-class hardware with sufficient resources to run Harvester and a production workload. Laptops and nested virtualization are not supported. Network: Configure ports based on the type of traffic to be transmitted among VMs. StorageClass: Create a new default StorageClass with the Number of Replicas parameter set to &quot;1&quot;. This ensures that only one replica is created for each volume in Longhorn. important The default StorageClass harvester-longhorn has a replica count value of 3 for high availability. If you use this StorageClass to create volumes for your single-node cluster, Longhorn is unable to create the configured number of replicas. This results in volumes being marked as &quot;Degraded&quot; on the Longhorn UI.","keywords":"Single Node","version":"v1.3 (latest)"},{"title":"StorageClass","type":0,"sectionRef":"#","url":"/v1.3/advanced/storageclass","content":"StorageClass A StorageClass allows administrators to describe the classes of storage they offer. Different Longhorn StorageClasses might map to replica policies, or to node schedule policies, or disk schedule policies determined by the cluster administrators. This concept is sometimes called profiles in other storage systems. note For support with other storage, please refer to Third-Party Storage Support Creating a StorageClass​ You can create one or more StorageClasses from the Advanced &gt; StorageClasses page. note After a StorageClass is created, nothing can be changed except Description. Header Section​ Name: name of the StorageClassDescription (optional): description of the StorageClass Parameters Tab​ Number of Replicas​ The number of replicas created for each volume in Longhorn. Defaults to 3. Stale Replica Timeout​ Determines when Longhorn would clean up an error replica after the replica's status is ERROR. The unit is minute. Defaults to 30 minutes in Harvester. Node Selector (Optional)​ Select the node tags to be matched in the volume scheduling stage. You can add node tags by going to Host &gt; Edit Config. Disk Selector (Optional)​ Select the disk tags to be matched in the volume scheduling stage. You can add disk tags by going to Host &gt; Edit Config. Migratable​ Whether Live Migration is supported. Defaults to Yes. Customize Tab​ Reclaim Policy​ Volumes dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class. The Delete mode is used by default. Delete: Deletes volumes and the underlying devices when the volume claim is deleted.Retain: Retains the volume for manual cleanup. Allow Volume Expansion​ Volumes can be configured to be expandable. This feature is Enabled by default, which allows users to resize the volume by editing the corresponding PVC object. note You can only use the volume expansion feature to grow a Volume, not to shrink it. Volume Binding Mode​ The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. The Immediate mode is used by default. Immediate: Binds and provisions a persistent volume once the PersistentVolumeClaim is created.WaitForFirstConsumer: Binds and provisions a persistent volume once a VM using the PersistentVolumeClaim is created. Data Locality Settings​ You can use the dataLocality parameter when at least one replica of a Longhorn volume must be scheduled on the same node as the pod that uses the volume (whenever possible). Harvester officially supports data locality as of v1.3.0. This applies even to volumes created from images. To configure data locality, create a new StorageClass on the Harvester UI (Storage Classess &gt; Create &gt; Parameters) and then add the following parameter: Key: dataLocalityValue: disabled or best-effort Data Locality Options​ Harvester currently supports the following options: disabled: When applied, Longhorn may or may not schedule a replica on the same node as the pod that uses the volume. This is the default option. best-effort: When applied, Longhorn always attempts to schedule a replica on the same node as the pod that uses the volume. Longhorn does not stop the volume even when a local replica is unavailable because of an environmental limitation (for example, insufficient disk space or incompatible disk tags). note Longhorn provides a third option called strict-local, which forces Longhorn to keep only one replica on the same node as the pod that uses the volume. Harvester does not support this option because it can affect certain operations such as VM Live Migration For more information, see Data Locality in the Longhorn documentation. Appendix - Use Case​ HDD Scenario​ With the introduction of StorageClass, users can now use HDDs for tiered or archived cold storage. caution HDD is not recommended for guest RKE2 clusters or VMs with good performance disk requirements. Recommended Practice​ First, add your HDD on the Host page and specify the disk tags as needed, such asHDD or ColdStorage. For more information on how to add extra disks and disk tags, see Multi-disk Management for details. Then, create a new StorageClass for the HDD (use the above disk tags). For hard drives with large capacity but slow performance, the number of replicas can be reduced to improve performance. You can now create a volume using the above StorageClass with HDDs mostly for cold storage or archiving purpose.","keywords":"","version":"v1.3 (latest)"},{"title":"VM Import","type":0,"sectionRef":"#","url":"/v1.3/advanced/addons/vmimport","content":"VM Import Available as of v1.1.0 Beginning with v1.1.0, users can import their virtual machines from VMWare and OpenStack into Harvester. This is accomplished using the vm-import-controller addon. To use the VM Import feature, users need to enable the vm-import-controller addon. By default, vm-import-controller leverages ephemeral storage, which is mounted from /var/lib/kubelet. During the migration, a large VM's node could run out of space on this mount, resulting in subsequent scheduling failures. To avoid this, users are advised to enable PVC-backed storage and customize the amount of storage needed. According to the best practice, the PVC size should be twice the size of the largest VM being migrated. This is essential as the PVC is used as scratch space to download the VM, and convert the disks into raw image files. vm-import-controller​ Currently, the following source providers are supported: VMWareOpenStack API​ The vm-import-controller introduces two CRDs. Sources​ Sources allow users to define valid source clusters. For example: apiVersion: migration.harvesterhci.io/v1beta1 kind: VmwareSource metadata: name: vcsim namespace: default spec: endpoint: &quot;https://vscim/sdk&quot; dc: &quot;DCO&quot; credentials: name: vsphere-credentials namespace: default The secret contains the credentials for the vCenter endpoint: apiVersion: v1 kind: Secret metadata: name: vsphere-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; As part of the reconciliation process, the controller will log into vCenter and verify whether the dc specified in the source spec is valid. Once this check is passed, the source is marked as ready and can be used for VM migrations. $ kubectl get vmwaresource.migration NAME STATUS vcsim clusterReady For OpenStack-based source clusters, an example definition is as follows: apiVersion: migration.harvesterhci.io/v1beta1 kind: OpenstackSource metadata: name: devstack namespace: default spec: endpoint: &quot;https://devstack/identity&quot; region: &quot;RegionOne&quot; credentials: name: devstack-credentials namespace: default The secret contains the credentials for the OpenStack endpoint: apiVersion: v1 kind: Secret metadata: name: devstack-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; &quot;project_name&quot;: &quot;admin&quot; &quot;domain_name&quot;: &quot;default&quot; &quot;ca_cert&quot;: &quot;pem-encoded-ca-cert&quot; The OpenStack source reconciliation process attempts to list VMs in the project and marks the source as ready. $ kubectl get opestacksource.migration NAME STATUS devstack clusterReady VirtualMachineImport​ The VirtualMachineImport CRD provides a way for users to define a source VM and map to the actual source cluster to perform VM export/import. A sample VirtualMachineImport looks like this: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: alpine-export-test namespace: default spec: virtualMachineName: &quot;alpine-export-test&quot; networkMapping: - sourceNetwork: &quot;dvSwitch 1&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;dvSwitch 2&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: vcsim namespace: default kind: VmwareSource apiVersion: migration.harvesterhci.io/v1beta1 This will trigger the controller to export the VM named &quot;alpine-export-test&quot; on the VMWare source cluster to be exported, processed and recreated into the harvester cluster This can take a while based on the size of the virtual machine, but users should see VirtualMachineImages created for each disk in the defined virtual machine. The list of items in networkMapping will define how the source network interfaces are mapped to the Harvester Networks. If a match is not found, each unmatched network interface is attached to the default managementNetwork. Once the virtual machine has been imported successfully, the object will reflect the status: $ kubectl get virtualmachineimport.migration NAME STATUS alpine-export-test virtualMachineRunning openstack-cirros-test virtualMachineRunning Similarly, users can define a VirtualMachineImport for an OpenStack source as well: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: openstack-demo namespace: default spec: virtualMachineName: &quot;openstack-demo&quot; #Name or UUID for instance networkMapping: - sourceNetwork: &quot;shared&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;public&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: devstack namespace: default kind: OpenstackSource apiVersion: migration.harvesterhci.io/v1beta1 note OpenStack allows users to have multiple instances with the same name. In such a scenario, users are advised to use the Instance ID. The reconciliation logic tries to perform a name-to-ID lookup when a name is used.","keywords":"","version":"v1.3 (latest)"},{"title":"Witness Node","type":0,"sectionRef":"#","url":"/v1.3/advanced/witness","content":"Witness Node Available as of v1.3.0 Harvester clusters deployed in production environments require a control plane for node and pod management. A typical three-node cluster has three management nodes that each contain the complete set of control plane components. One key component is etcd, which Kubernetes uses to store its data (configuration, state, and metadata). The etcd node count must always be an odd number (for example, 3 is the default count in Harvester) to ensure that a quorum is maintained. Some situations may require you to avoid deploying workloads and user data to management nodes. In these situations, one cluster node can be assigned the witness role, which limits it to functioning as an etcd cluster member. The witness node is responsible for establishing a member quorum (a majority of nodes), which must agree on updates to the cluster state. Witness nodes do not store any data, but the hardware recommendations for etcd nodes must still be considered. Using hardware with limited resources significantly affects cluster performance, as described in the article Slow etcd performance (performance testing and optimization). Harvester v1.3.0 supports clusters with two management nodes and one witness node (and optionally, one or more worker nodes). For more information about node roles in Harvester, see Role Management. important A node can be assigned the witness role only at the time it joins a cluster. Each cluster can have only one witness node. Creating a Harvester Cluster with a Witness Node​ You can assign the witness role to a node when it joins a newly created cluster. In the following example, a cluster with three nodes was created and the node harvester-node-1 was assigned the witness role. harvester-node-1 consumes less resources and only has etcd capabilities. NAME↑ STATUS ROLE VERSION PODS CPU MEM %CPU %MEM CPU/A MEM/A AGE harvester-node-0 Ready control-plane,etcd,master v1.27.10+rke2r1 70 1095 10143 10 63 10000 15976 4d13h harvester-node-1 Ready etcd v1.27.10+rke2r1 7 258 2258 2 14 10000 15976 4d13h harvester-node-2 Ready control-plane,etcd,master v1.27.10+rke2r1 36 840 6905 8 43 10000 15976 4d13h Because the cluster must have three nodes, the promote controller will promote the other two nodes. After that, the cluster will have two control-plane nodes and one witness node. Workloads on the Witness Node​ The witness node only runs the following essential workloads: harvester-node-managercloud-controller-manageretcdkube-proxyrke2-canalrke2-multus Upgrade a Cluster with a Witness Node​ The general upgrade requirements and procedures apply to clusters with a witness node. However, the existence of degraded volumes in such clusters may cause upgrade operations to fail. Longhorn Replicas in Clusters with a Witness Node​ Harvester uses Longhorn, a distributed block storage system, for management of block device volumes. Longhorn is provisioned to management and worker nodes but not to witness nodes, which do not store any data. Longhorn creates replicas of each volume to increase availability. Replicas contain a chain of snapshots of the volume, with each snapshot storing the change from a previous snapshot. In Harvester, the default StorageClass harvester-longhorn has a replica count value of 3. Limitations​ Witness nodes do not store any data. This means that in three-node clusters (no worker nodes), only two replicas are created for each Longhorn volume. However, the default StorageClass harvester-longhorn has a replica count value of 3 for high availability. If you use this StorageClass to create volumes, Longhorn is unable to create the configured number of replicas. This results in volumes being marked as Degraded on the Longhorn UI. In summary, you must use a StorageClass that matches the cluster configuration. 2 management nodes + 1 witness node: Create a new default StorageClass with the Number of Replicas parameter set to 2. This ensures that only two replicas are created for each Longhorn volume.2 management nodes + 1 witness node + 1 or more worker nodes: You can use the existing default StorageClass. If you already created volumes using the original default StorageClass, you can modify the replica count on the Volume screen of the embedded Longhorn UI. Known Issues​ 1. When creating a cluster with a witness node, the Network Config: Create screen on the Harvester UI is unable to identify any NICs that can be used with all nodes.​ The workaround is to select a non-witness node and then select a NIC that can be used with that specific node. You must repeat this procedure for every non-witness node in the cluster. The same uplink settings can be used across nodes. Related issue: [BUG] Unable to select NIC to create network config when cluster contains witness node 2. When selecting a target node for VM migration, the target list includes the witness node.​ Do not select the witness node as the migration target. If you do, VM migration will fail. Related issue: [BUG] The witness node should not be selected as a migration target","keywords":"","version":"v1.3 (latest)"},{"title":"Air Gapped Environment","type":0,"sectionRef":"#","url":"/v1.3/airgap","content":"Air Gapped Environment This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy​ In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation​ You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings​ You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI.Find the http-proxy setting, click ⋮ &gt; Edit settingEnter the value(s) for http-proxy, https-proxy and no-proxy. note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. When the nodes in the cluster do not use a proxy to communicate with each other, the CIDR needs to be added to http-proxy.noProxy after the first node is installed successfully. Please refer to fail to deploy a multi-node cluster. Guest Cluster Images​ All necessary images to install and run Harvester are conveniently packaged into the ISO, eliminating the need to pre-load images on bare-metal nodes. A Harvester cluster manages them independently and effectively behind the scenes. However, it's essential to understand a guest K8s cluster (e.g., RKE2 cluster) created by the Harvester node driver is a distinct entity from a Harvester cluster. A guest cluster operates within VMs and requires pulling images either from the internet or a private registry. If the Cloud Provider option is configured to Harvester in a guest K8s cluster, it deploys the Harvester cloud provider and Container Storage Interface (CSI) driver. As a result, we recommend monitoring each RKE2 release in your air gapped environment and pulling the required images into your private registry. Please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section on the Harvester support matrix page for the best Harvester cloud provider and CSI driver capability support.","keywords":"Harvester offline Air-gap HTTP proxy","version":"v1.3 (latest)"},{"title":"vGPU Support","type":0,"sectionRef":"#","url":"/v1.3/advanced/vgpusupport","content":"vGPU Support Available as of v1.3.0 Harvester now offers the capability to share NVIDIA GPU's supporting SRIOV based virtualisation as vGPU devices. The additional capability is provided by the pcidevices-controller addon, and leverages sriov-manage to manage the gpu. Please refer the Nvidia Documentation and your GPU documentation to identify if the GPU is supported. The nvidia-driver-toolkit addon needs to be enabled for users to be able to manage the lifecycle of vGPU's on GPU devices. Usage​ On the Harvester UI, go to Advanced &gt; SR-IOV GPU Devices and verify the following: GPU devices have been scanned. An associated sriovgpudevices.devices.harvesterhci.io object has been created. Locate the device that you want to enable, and then select : &gt; Enable. Go to the vGPU Devices screen and check the associated vgpudevices.devices.harvesterhci.io objects. Allow some time for the pcidevices-controller to scan the vGPU devices and for the Harvester UI to display the device information. Select a vGPU and configure a profile. :::note The list of profiles depends on the GPU and the underlying /sys tree of the host. For more information about the available profiles and their capabilities, see the NVIDIA documentation. After you select the first profile, the NVIDIA driver automatically configures the profiles available for the remaining vGPUs. ::: ::: Attach the vGPU to a new or existing VM. important Once a vGPU has been assigned to a VM, it may not be possible to disable the VM until the vGPU is removed. Limitations​ Attaching multiple vGPU's:​ Attaching multiple vGPUs to a VM may fail for the following reasons: Not all vGPU profiles support attachment of multiple vGPUs. The NVIDIA documentation lists the vGPU profiles that support this feature. For example, if you use NVIDIA A2 or A16 GPUs, note that only Q-series vGPUs allow you to attach multiple vGPUs. Only 1 GPU device in the VM definition can have ramFB enabled. To attach multiple vGPUs, you must edit the VM configuration (in YAML) and add virtualGPUOptions to all non-primary vGPU devices. virtualGPUOptions: display: ramFB: enabled: false Related issue: https://github.com/harvester/harvester/issues/5289 Cap on Usable vGPUs​ When vGPU support is enabled on a GPU, the NVIDIA driver creates 16 vGPU devices by default. After you select the first profile, the NVIDIA driver automatically configures the profiles available for the remaining vGPUs. The profile used also dictates the maximum number of vGPUs available for each GPU. Once the maximum is exhausted, no profiles can be selected for the remaining vGPUs and those devices cannot be configured. Example (NVIDIA A2 GPU): If you select the NVIDIA A2-4Q profile, you can only configure 4 vGPU devices. Once those devices are configured, you cannot select any profiles for the remaining vGPUs. Technical Deep dive​ pcidevices-controller introduces the following CRDs: sriovgpudevices.devices.harvesterhci.io vgpudevices.devices.harvesterhci.io On boot, pcidevices-controller scans the host for NVIDIA GPUs that support SR-IOV vGPU devices. When such devices are found, they are represented as a CRD. Example: apiVersion: devices.harvesterhci.io/v1beta1 kind: SRIOVGPUDevice metadata: creationTimestamp: &quot;2024-02-21T05:57:37Z&quot; generation: 2 labels: nodename: harvester-kgd9c name: harvester-kgd9c-000008000 resourceVersion: &quot;6641619&quot; uid: e3a97ee4-046a-48d7-820d-8c6b45cd07da spec: address: &quot;0000:08:00.0&quot; enabled: true nodeName: harvester-kgd9c status: vGPUDevices: - harvester-kgd9c-000008004 - harvester-kgd9c-000008005 - harvester-kgd9c-000008016 - harvester-kgd9c-000008017 - harvester-kgd9c-000008020 - harvester-kgd9c-000008021 - harvester-kgd9c-000008022 - harvester-kgd9c-000008023 - harvester-kgd9c-000008006 - harvester-kgd9c-000008007 - harvester-kgd9c-000008010 - harvester-kgd9c-000008011 - harvester-kgd9c-000008012 - harvester-kgd9c-000008013 - harvester-kgd9c-000008014 - harvester-kgd9c-000008015 vfAddresses: - &quot;0000:08:00.4&quot; - &quot;0000:08:00.5&quot; - &quot;0000:08:01.6&quot; - &quot;0000:08:01.7&quot; - &quot;0000:08:02.0&quot; - &quot;0000:08:02.1&quot; - &quot;0000:08:02.2&quot; - &quot;0000:08:02.3&quot; - &quot;0000:08:00.6&quot; - &quot;0000:08:00.7&quot; - &quot;0000:08:01.0&quot; - &quot;0000:08:01.1&quot; - &quot;0000:08:01.2&quot; - &quot;0000:08:01.3&quot; - &quot;0000:08:01.4&quot; - &quot;0000:08:01.5&quot; When a SRIOVGPUDevice is enabled, the pcidevices controller works with the nvidia-driver-toolkit daemonset to manage the GPU devices. On subsequent scan of the /sys tree by the pcidevices, the vGPU devices are scanned by the pcidevices controller and managed as VGPUDevices CRD NAME ADDRESS NODE NAME ENABLED UUID VGPUTYPE PARENTGPUDEVICE harvester-kgd9c-000008004 0000:08:00.4 harvester-kgd9c true dd6772a8-7db8-4e96-9a73-f94c389d9bc3 NVIDIA A2-4A 0000:08:00.0 harvester-kgd9c-000008005 0000:08:00.5 harvester-kgd9c true 9534e04b-4687-412b-833e-3ae95b97d4d1 NVIDIA A2-4Q 0000:08:00.0 harvester-kgd9c-000008006 0000:08:00.6 harvester-kgd9c true a16e5966-9f7a-48a9-bda8-0d1670e740f8 NVIDIA A2-4A 0000:08:00.0 harvester-kgd9c-000008007 0000:08:00.7 harvester-kgd9c true 041ee3ce-f95c-451e-a381-1c9fe71918b2 NVIDIA A2-4Q 0000:08:00.0 harvester-kgd9c-000008010 0000:08:01.0 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008011 0000:08:01.1 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008012 0000:08:01.2 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008013 0000:08:01.3 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008014 0000:08:01.4 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008015 0000:08:01.5 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008016 0000:08:01.6 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008017 0000:08:01.7 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008020 0000:08:02.0 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008021 0000:08:02.1 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008022 0000:08:02.2 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008023 0000:08:02.3 harvester-kgd9c false 0000:08:00.0 When a user enables and selects a profile for the VGPUDevice the pcidevices controller sets up the device and sets up the correct profile on the said device. apiVersion: devices.harvesterhci.io/v1beta1 kind: VGPUDevice metadata: creationTimestamp: &quot;2024-02-26T03:04:47Z&quot; generation: 8 labels: harvesterhci.io/parentSRIOVGPUDevice: harvester-kgd9c-000008000 nodename: harvester-kgd9c name: harvester-kgd9c-000008004 resourceVersion: &quot;21051017&quot; uid: b9c7af64-1e47-467f-bf3d-87b7bc3a8911 spec: address: &quot;0000:08:00.4&quot; enabled: true nodeName: harvester-kgd9c parentGPUDeviceAddress: &quot;0000:08:00.0&quot; vGPUTypeName: NVIDIA A2-4A status: configureVGPUTypeName: NVIDIA A2-4A uuid: dd6772a8-7db8-4e96-9a73-f94c389d9bc3 vGPUStatus: vGPUConfigured The pcidevices controller also runs a vGPU device plugin, which advertises the details of the various vGPU profiles to the kubelet. This is then used by the k8s scheduler to place the VM's requesting vGPU's to the correct nodes. (⎈|local:harvester-system)➜ ~ k get nodes harvester-kgd9c -o yaml | yq .status.allocatable cpu: &quot;24&quot; devices.kubevirt.io/kvm: 1k devices.kubevirt.io/tun: 1k devices.kubevirt.io/vhost-net: 1k ephemeral-storage: &quot;149527126718&quot; hugepages-1Gi: &quot;0&quot; hugepages-2Mi: &quot;0&quot; intel.com/82599_ETHERNET_CONTROLLER_VIRTUAL_FUNCTION: &quot;1&quot; memory: 131858088Ki nvidia.com/NVIDIA_A2-4A: &quot;2&quot; nvidia.com/NVIDIA_A2-4C: &quot;0&quot; nvidia.com/NVIDIA_A2-4Q: &quot;2&quot; pods: &quot;200&quot; The pcidevices controller also setups the integration with kubevirt and advertises the vGPU devices as externally managed devices in the Kubevirt CR to ensure that the VM can consume the vGPU.","keywords":"","version":"v1.3 (latest)"},{"title":"Settings","type":0,"sectionRef":"#","url":"/v1.3/advanced/index","content":"Settings This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca​ This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example​ -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- caution Changing this setting might cause a short downtime for single-node clusters. auto-disk-provision-paths [Experimental]​ This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. note This setting only adds formatted disks mounted to the system. caution This setting is applied to every Node in the cluster.All the data in these storage devices will be destroyed. Use at your own risk. Default: none Example​ The following example will add disks matching the glob pattern /dev/sd* or /dev/hd*: /dev/sd*,/dev/hd* backup-target​ This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation. Default: none Example​ { &quot;type&quot;: &quot;s3&quot;, &quot;endpoint&quot;: &quot;https://s3.endpoint.svc&quot;, &quot;accessKeyId&quot;: &quot;test-access-key-id&quot;, &quot;secretAccessKey&quot;: &quot;test-access-key&quot;, &quot;bucketName&quot;: &quot;test-bup&quot;, &quot;bucketRegion&quot;: &quot;us‑east‑2&quot;, &quot;cert&quot;: &quot;&quot;, &quot;virtualHostedStyle&quot;: false } cluster-registration-url​ This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example​ https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml note When you configure this setting, a new pod called cattle-cluster-agent-* is created in the namespace cattle-system for registration purposes. This pod uses the container image rancher/rancher-agent:related-version, which is not packed into the Harvester ISO and is instead determined by Rancher. The related-version is usually the same as the Rancher version. For example, when you register Harvester to Rancher v2.7.9, the image is rancher/rancher-agent:v2.7.9. For more information, see Find the required assets for your Rancher version in the Rancher documentation. Depending on your Harvester settings, the image is downloaded from either of the following locations: Harvester containerd-registry: You can configure a private registry for the Harvester cluster. Docker Hub (docker.io): This is the default option when you do not configure a private registry in Rancher. Alternatively, you can obtain a copy of the image and manually upload it to all Harvester nodes. containerd-registry​ This setting allows you to configure a private registry for the Harvester cluster. The value will be set in /etc/rancher/rke2/registries.yaml of each node. You can read RKE2 - Containerd Registry Configuration for more information. note If you set a username and password for a private registry, the system will automatically remove it to protect the credential after the system saves it in registries.yaml. Example​ { &quot;Mirrors&quot;: { &quot;docker.io&quot;: { &quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;], &quot;Rewrites&quot;: null } }, &quot;Configs&quot;: { &quot;myregistry.local:5000&quot;: { &quot;Auth&quot;: { &quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot; }, &quot;TLS&quot;: { &quot;InsecureSkipVerify&quot;: false } } } } csi-driver-config​ Available as of v1.2.0 If you install third-party CSI drivers in the Harvester cluster, you must configure some necessary information through this setting before using Backup &amp; Snapshot related features. Default: { &quot;driver.longhorn.io&quot;: { &quot;volumeSnapshotClassName&quot;: &quot;longhorn-snapshot&quot;, &quot;backupVolumeSnapshotClassName&quot;: &quot;longhorn&quot; } } Add the provisioner for the newly added CSI driver.Configure Volume Snapshot Class Name, which refers to the name of the VolumeSnapshotClass used to create volume snapshots or VM snapshots.Configure Backup Volume Snapshot Class Name, which refers to the name of the VolumeSnapshotClass used to create VM backups. default-vm-termination-grace-period-seconds​ Available as of v1.2.0 This setting allows you to specify a default termination grace period for stopping a virtual machine in seconds. Default: 120 http-proxy​ This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: &quot;httpProxy&quot;: &quot;http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Proxy URL for HTTPS requests: &quot;httpsProxy&quot;: &quot;https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Comma-separated list of hostnames and/or CIDRs: &quot;noProxy&quot;: &quot;&lt;hostname | CIDR&gt;&quot; caution If you configure httpProxy and httpsProxy, you must also put Harvester node's CIDR into noProxy, otherwise the Harvester cluster will be broken. If you also configure cluster-registration-url, you usually need to add the host of cluster-registration-url to noProxy as well, otherwise you cannot access the Harvester cluster from Rancher. Example​ { &quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc,172.16.0.0/16&quot; } note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. caution Changing this setting might cause a short downtime for single-node clusters. log-level​ This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panicfatalerrorwarn, warninginfodebugtrace Example​ debug ntp-servers​ Available as of v1.2.0 This setting allows you to configure NTP servers for time synchronization on the Harvester nodes. Using this setting, you can define NTP servers during installation or update NTP servers after installation. caution Modifying the NTP servers will replace the previous values for all nodes. Default: &quot;&quot; Example​ { &quot;ntpServers&quot;: [ &quot;0.suse.pool.ntp.org&quot;, &quot;1.suse.pool.ntp.org&quot; ] } overcommit-config​ This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { &quot;cpu&quot;:1600, &quot;memory&quot;:150, &quot;storage&quot;:200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100mCPU for it from Kubernetes scheduler. Example​ { &quot;cpu&quot;: 1000, &quot;memory&quot;: 200, &quot;storage&quot;: 300 } release-download-url​ Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester Example of the version.yaml​ apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL} server-version​ This setting displays the version of Harvester server. Example​ v1.0.0-abcdef-head ssl-certificates​ This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example​ { &quot;ca&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;publicCertificate&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;privateKey&quot;: &quot;-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----&quot; } caution Changing this setting might cause a short downtime on single-node clusters. ssl-parameters​ This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols: Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers: Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list isECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305. Default: none note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example​ The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list toECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305. { &quot;protocols&quot;: &quot;TLSv1.2 TLSv1.3&quot;, &quot;ciphers&quot;: &quot;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305&quot; } storage-network​ By default, Longhorn uses the default management network in the Harvester cluster that is limited to a single interface and shared with other workloads cluster-wide. This setting allows you to configure a segregated storage network when network isolation is preferred. For details, please refer to the Harvester Storage Network caution Any change to storage-network requires shutting down all VMs before applying this setting. IP Range should be IPv4 CIDR format and 4 times the number of your cluster nodes. Default: &quot;&quot; Example​ { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } support-bundle-image​ Available as of v1.2.0 This setting allows you to configure the support bundle image, with various versions available in rancher/support-bundle-kit. Default: { &quot;repository&quot;: &quot;rancher/support-bundle-kit&quot;, &quot;tag&quot;: &quot;v0.0.25&quot;, &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot; } support-bundle-namespaces​ Available as of v1.2.0 This setting allows you to specify additional namespaces when collecting a support bundle. The support bundle will only capture resources from pre-defined namespaces by default. Here is the pre-defined namespaces list: cattle-dashboardscattle-fleet-local-systemcattle-fleet-systemcattle-fleet-clusters-systemcattle-monitoring-systemfleet-localharvester-systemlocallonghorn-systemcattle-logging-system If you select more namespaces, it will append to the pre-defined namespaces list. Default: none support-bundle-timeout​ Available as of v1.2.0 This setting allows you to define the number of minutes Harvester allows for the completion of the support bundle generation process. The process is considered to have failed when the data collection and file packing tasks are not completed within the configured number of minutes. Harvester will not continue or retry support bundle generation processes that have timed out. When the value is &quot;0&quot;, the timeout feature is disabled. Default: 10 support-bundle-expiration​ Available as of v1.3.0 This setting allows you to define the number of minutes Harvester waits before deleting a support bundle that has been packaged but not downloaded (either deliberately or unsuccessfully) or retained. The minimum value is 30. Default: 30 upgrade-checker-enabled​ This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example​ false upgrade-checker-url​ This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example​ https://your.upgrade.checker-url/v99/checkupgrade vip-pools​ Deprecated as of v1.2.0, use IP Pool instead This setting allows you to configure the global or namespace IP address pools of the VIP by CIDR or IP range. Default: {} note Configuring multi-CIDR or IP range from UI is only available from Harvester v1.1.1. Example​ { &quot;default&quot;: &quot;172.16.0.0/24,172.16.1.0/24&quot;, &quot;demo&quot;: &quot;172.16.2.50-172.16.2.100,172.16.2.150-172.16.3.200&quot; } vm-force-reset-policy​ This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready, it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {&quot;enable&quot;:true, &quot;period&quot;:300} note When a host is unavailable or is powered off, the VM only reboots and does not migrate. Example​ { &quot;enable&quot;: &quot;true&quot;, &quot;period&quot;: 300 } UI Settings​ branding​ Available as of v1.2.0 This setting allows you to globally re-brand the UI by customizing the Harvester product name, logos, and color scheme. Default: Harvester You can set the following options and values: Private Label: This option replaces &quot;Harvester&quot; with the value you provide in most places.Logo: Upload light and dark logos to replace the Harvester logo in the top-level navigation header.Favicon: Upload an icon to replace the Harvester favicon in the browser tab.Primary Color: You can override the primary color used throughout the UI with a custom color of your choice.Link Color: You can override the link color used throughout the UI with a custom color of your choice. ui-index​ This setting allows you to configure the HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example​ https://your.static.dashboard-ui/index.html ui-plugin-index​ This setting allows you to configure the JS address for the Harvester plugin (when accessing Harvester from Rancher). Default: https://releases.rancher.com/harvester-ui/plugin/harvester-latest/harvester-latest.umd.min.js Example​ https://your.static.dashboard-ui/*.umd.min.js ui-source​ This setting allows you to configure how to load the UI source. You can set the following values: auto: The default. Auto-detect whether to use bundled UI or not.external: Use external UI source.bundled: Use the bundled UI source. Example​ external ","keywords":"","version":"v1.3 (latest)"},{"title":"Storage Network","type":0,"sectionRef":"#","url":"/v1.3/advanced/storagenetwork","content":"Storage Network Harvester uses Longhorn as its built-in storage system to provide block device volumes for VMs and Pods. If the user wishes to isolate Longhorn replication traffic from the Kubernetes cluster network (i.e. the management network) or other cluster-wide workloads. Users can allocate a dedicated storage network for Longhorn replication traffic to get better network bandwidth and performance. For more information, please see Longhorn Storage Network note Configuring Longhorn settings directly is not recommended, as this can lead to untested situations. Prerequisites​ There are some prerequisites before configuring the Harvester Storage Network setting. Well-configured Cluster Network and VLAN Config. Users have to ensure the Cluster Network is configured and VLAN Config will cover all nodes and ensure the network connectivity is working and expected in all nodes. All VMs should be stopped. We recommend checking the VM status with the following command and should get an empty result.kubectl get -A vmi All pods that are attached to Longhorn Volumes should be stopped. Users could skip this step with the Harvester Storage Network setting. Harvester will stop Longhorn-related pods automatically. All ongoing image uploads or downloads should be either completed or deleted. caution If the Harvester cluster was upgraded from v1.0.3, please check if Whereabouts CNI is installed properly before you move on to the next step. We will always recommend following this guide to check. Issue 3168 describes that the Harvester cluster will not always install Whereabouts CNI properly. Verify the ippools.whereabouts.cni.cncf.io CRD exists with the following command. kubectl get crd ippools.whereabouts.cni.cncf.io If the Harvester cluster doesn't have ippools.whereabouts.cni.cncf.io, please add these two CRDs before configuring storage-network setting. kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_ippools.yaml kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml Configuration Example​ VLAN ID Please check with your network switch setting, and provide a dedicated VLAN ID for Storage Network. Well-configured Cluster Network and VLAN Config Please refer Networking page for more details and configure Cluster Network and VLAN Config but not Networks. IP range for Storage Network IP range should not conflict or overlap with Kubernetes cluster networks(10.42.0.0/16, 10.43.0.0/16, 10.52.0.0/16 and 10.53.0.0/16 are reserved).IP range should be in IPv4 CIDR format and Longhorn pods use Storage Network as follows: instance-manger-e and instance-manager-r pods: These require 2 IPs per node. During an upgrade, two versions of these pods will exist (old and new), and the old version will be deleted once the upgrade is successful.backing-image-ds pods: These are employed to process on-the-fly uploads and downloads of backing image data sources. These pods will be removed once the image uploads or downloads are completed.backing-image-manager pods: 1 IP per disk, similar to the instance manager pods. Two versions of these will coexist during an upgrade, and the old ones will be removed after the upgrade is completed.The required number of IPs is calculated using a simple formula: Required Number of IPs = Number of Nodes * 4 + Number of Disks * 2 + Number of Images to Download/Upload For example, if your cluster has five nodes, each node has two disks, and ten images will be uploaded simultaneously, the IP range should be greater than or equal to /26 (5 * 4 + 5 * 2 * 2 + 10 = 50). We will take the following configuration as an example to explain the details of the Storage Network VLAN ID for Storage Network: 100Cluster Network: storageIP range: 192.168.0.0/24 Configuration Process​ Harvester will create Multus NetworkAttachmentDefinition from the configuration, stop pods related to Longhorn Volume, update Longhorn setting, and restart previous pods. Before Applying Harvester Storage Network Setting​ Here we have two cases. Expect that VM VLAN traffic and Longhorn Storage Network use the same group of physical interfaces.Expect that VM VLAN traffic and Longhorn Storage Network use different physical interfaces. Longhorn will send replication traffic through the specific interfaces shown as the red line in the figure. Same Physical Interfaces​ Take eth2 and eth3 as an example for VM VLAN traffic and Longhorn Storage Network simultaneously. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth2 and eth3 and remember the ClusterNetwork name for the further step. Different Physical Interfaces​ eth2 and eth3 are for VM VLAN Traffic. eth4 and eth5 are for Longhorn Storage Network. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth4 and eth5 for Storage Network and remember the ClusterNetwork name for the further step. Harvester Storage Network Setting​ Harvester Storage Network setting will need range, clusterNetwork, vlan field to construct Multus NetworkAttachmentDefinition for Storage Network usage. You could apply this setting via Web UI or CLI. Web UI​ Harvester Storage Network setting could be easily modified on the Settings &gt; storage-network page. CLI​ Users could use this command to edit Harvester Storage Network setting. kubectl edit settings.harvesterhci.io storage-network The value format is JSON string or empty string as shown in below. { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } The full configuration will be like this example. apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: name: storage-network value: '{&quot;vlan&quot;:100,&quot;clusterNetwork&quot;:&quot;storage&quot;,&quot;range&quot;:&quot;192.168.0.0/24&quot;}' caution Because of the design, Harvester will treat extra and insignificant characters in JSON string as a different configuration. After Applying Harvester Storage Network Setting​ After applying Harvester's Storage Network setting, Harvester will stop all pods that are related to Longhorn volumes. Currently, Harvester has some pods listed below that will be stopped during setting. PrometheusGrafanaAlertmanagerVM Import Controller Harvester will also create a new NetworkAttachmentDefinition and update the Longhorn Storage Network setting. Once the Longhorn setting is updated, Longhorn will restart all instance-manager-r, instance-manager-e, and backing-image-manager pods to apply the new network configuration, and Harvester will restart the pods. note Harvester will not start VM automatically. Users should check whether the configuration is completed or not in the next section and start VM manually on demand. Verify Configuration is Completed​ Step 1​ Check if Harvester Storage Network setting's status is True and the type is configured. kubectl get settings.harvesterhci.io storage-network -o yaml Completed Setting Example: apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: annotations: storage-network.settings.harvesterhci.io/hash: da39a3ee5e6b4b0d3255bfef95601890afd80709 storage-network.settings.harvesterhci.io/net-attach-def: &quot;&quot; storage-network.settings.harvesterhci.io/old-net-attach-def: &quot;&quot; creationTimestamp: &quot;2022-10-13T06:36:39Z&quot; generation: 51 name: storage-network resourceVersion: &quot;154638&quot; uid: 2233ad63-ee52-45f6-a79c-147e48fc88db status: conditions: - lastUpdateTime: &quot;2022-10-13T13:05:17Z&quot; reason: Completed status: &quot;True&quot; type: configured Step 2​ Verify the readiness of all Longhorn instance-manager-e, instance-manager-r, and backing-image-manager pods, and confirm that their networks are correctly configured. Execute the following command to inspect a pod's details: kubectl -n longhorn-system describe pod &lt;pod-name&gt; If you encounter an event resembling the following one, the Storage Network might have run out of its available IPs: Events: Type Reason Age From Message ---- ------ ---- ---- ------- .... Warning FailedCreatePodSandBox 2m58s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox &quot;04e9bc160c4f1da612e2bb52dadc86702817ac557e641a3b07b7c4a340c9fc48&quot;: plugin type=&quot;multus&quot; name=&quot;multus-cni-network&quot; failed (add): [longhorn-system/ba cking-image-ds-default-image-lxq7r/7d6995ee-60a6-4f67-b9ea-246a73a4df54:storagenetwork-sdfg8]: error adding container to network &quot;storagenetwork-sdfg8&quot;: erro r at storage engine: Could not allocate IP in range: ip: 172.16.0.1 / - 172.16.0.6 / range: net.IPNet{IP:net.IP{0xac, 0x10, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0xff, 0xf8}} .... Please reconfigure the Storage Network with a sufficient IP range. note If the Storage Network has run out of IPs, you might encounter the same error when you upload/download images. Please delete the related images and reconfigure the Storage Network with a sufficient IP range. Step 3​ Check the k8s.v1.cni.cncf.io/network-status annotations and ensure that an interface named lhnet1 exists, with an IP address within the designated IP range. Users could use the following command to show all Longhorn Instance Manager to verify. kubectl get pods -n longhorn-system -l longhorn.io/component=instance-manager -o yaml Correct Network Example: apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/containerID: 2518b0696f6635896645b5546417447843e14208525d3c19d7ec6d7296cc13cd cni.projectcalico.org/podIP: 10.52.2.122/32 cni.projectcalico.org/podIPs: 10.52.2.122/32 k8s.v1.cni.cncf.io/network-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] k8s.v1.cni.cncf.io/networks: '[{&quot;namespace&quot;: &quot;harvester-system&quot;, &quot;name&quot;: &quot;storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;}]' k8s.v1.cni.cncf.io/networks-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] kubernetes.io/psp: global-unrestricted-psp longhorn.io/last-applied-tolerations: '[{&quot;key&quot;:&quot;kubevirt.io/drain&quot;,&quot;operator&quot;:&quot;Exists&quot;,&quot;effect&quot;:&quot;NoSchedule&quot;}]' Omitted... Start VM Manually​ After verifying the configuration, users could start VM manually on demand.","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Cluster Network","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-cluster-network","content":"Create a Cluster Network POST /apis/network.harvesterhci.io/v1beta1/clusternetworks Create a ClusterNetwork object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-network-attachment-definition","content":"Create a Network Attachment Definition POST /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions Create a NetworkAttachmentDefinition object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create an Upgrade","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-upgrade","content":"Create an Upgrade POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades Create a Upgrade object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Node Network","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-node-network","content":"Create a Node Network POST /apis/network.harvesterhci.io/v1beta1/nodenetworks Create a NodeNetwork object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Support Bundle","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-support-bundle","content":"Create a Support Bundle POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles Create a SupportBundle object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Key Pair","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-key-pair","content":"Create a Key Pair POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs Create a KeyPair object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine-instance-migration","content":"Create a Virtual Machine Instance Migration POST /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations Create a VirtualMachineInstanceMigration object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine-template","content":"Create a Virtual Machine Template POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates Create a VirtualMachineTemplate object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Key Pair","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-key-pair","content":"Delete a Key Pair DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Delete a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Cluster Network","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-cluster-network","content":"Delete a Cluster Network DELETE /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Delete a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-persistent-volume-claim","content":"Create a Persistent Volume Claim POST /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims Create a PersistentVolumeClaim object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine-image","content":"Create a Virtual Machine Image POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages Create a VirtualMachineImage object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Support Bundle","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-support-bundle","content":"Delete a Support Bundle DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Delete a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Node Network","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-node-network","content":"Delete a Node Network DELETE /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Delete a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-network-attachment-definition","content":"Delete a Network Attachment Definition DELETE /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Delete a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine","content":"Delete a Virtual Machine DELETE /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-persistent-volume-claim","content":"Delete a Persistent Volume Claim DELETE /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Delete a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete an Upgrade","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-upgrade","content":"Delete an Upgrade DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Delete a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine-image","content":"Delete a Virtual Machine Image DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine-backup","content":"Delete a Virtual Machine Backup DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine-restore","content":"Delete a Virtual Machine Restore DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine-instance-migration","content":"Delete a Virtual Machine Instance Migration DELETE /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine-restore","content":"Create a Virtual Machine Restore POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores Create a VirtualMachineRestore object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Harvester APIs","type":0,"sectionRef":"#","url":"/v1.3/api/harvester-apis","content":"Version: v1beta1 Harvester APIs This section introduces the APIs of the Harvester server. You can find out more about Harvester's API definitions here.","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine-template","content":"Delete a Virtual Machine Template DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.3/api/delete-namespaced-virtual-machine-template-version","content":"Delete a Virtual Machine Template Version DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Key Pairs For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-key-pair-for-all-namespaces","content":"List Key Pairs For All Namespaces GET /apis/harvesterhci.io/v1beta1/keypairs Get a list of all KeyPair objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Key Pairs","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-key-pair","content":"List Key Pairs GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs Get a list of KeyPair objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Network Attachment Definitions","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-network-attachment-definition","content":"List Network Attachment Definitions GET /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions Get a list of NetworkAttachmentDefinition objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Node Networks","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-node-network","content":"List Node Networks GET /apis/network.harvesterhci.io/v1beta1/nodenetworks Get a list of NodeNetwork objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Support Bundles","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-support-bundle","content":"List Support Bundles GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles Get a list of SupportBundle objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Cluster Networks","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-cluster-network","content":"List Cluster Networks GET /apis/network.harvesterhci.io/v1beta1/clusternetworks Get a list of ClusterNetwork objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Upgrades","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-upgrade","content":"List Upgrades GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades Get a list of Upgrade objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Persistent Volume Claims","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-persistent-volume-claim","content":"List Persistent Volume Claims GET /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims Get a list of PersistentVolumeClaim objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Images","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-image","content":"List Virtual Machine Images GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages Get a list of VirtualMachineImage objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Instance Migrations","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-instance-migration","content":"List Virtual Machine Instance Migrations GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations Get a list of VirtualMachineInstanceMigration objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Templates","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-template","content":"List Virtual Machine Templates GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates Get a list of VirtualMachineTemplate objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Restores","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-restore","content":"List Virtual Machine Restores GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores Get a list of VirtualMachineRestore objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Network Attachment Definitions For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-network-attachment-definition-for-all-namespaces","content":"List Network Attachment Definitions For All Namespaces GET /apis/k8s.cni.cncf.io/v1/network-attachment-definitions Get a list of all NetworkAttachmentDefinition objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Persistent Volume Claims For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-persistent-volume-claim-for-all-namespaces","content":"List Persistent Volume Claims For All Namespaces GET /api/v1/persistentvolumeclaims Get a list of all PersistentVolumeClaim objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Support Bundles For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-support-bundle-for-all-namespaces","content":"List Support Bundles For All Namespaces GET /apis/harvesterhci.io/v1beta1/supportbundles Get a list of all SupportBundle objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Upgrades For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-upgrade-for-all-namespaces","content":"List Upgrades For All Namespaces GET /apis/harvesterhci.io/v1beta1/upgrades Get a list of all Upgrade objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Instances","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-instance","content":"List Virtual Machine Instances GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstances Get a list of VirtualMachineInstance objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object activePods object property name* string conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] evacuationNodeName string fsFreezeStatus string guestOSInfo object id string kernelRelease string kernelVersion string machine string name string prettyName string version string versionId string interfaces object[] Array [ infoSource string interfaceName string ipAddress string ipAddresses string[] mac string name string ] launcherContainerImageVersion string migrationMethod string migrationState object abortRequested boolean abortStatus string completed boolean endTimestamp string failed boolean migrationConfiguration object allowAutoConverge boolean allowPostCopy boolean bandwidthPerMigration string completionTimeoutPerGiB int64 disableTLS boolean network string nodeDrainTaintKey string parallelMigrationsPerCluster int64 parallelOutboundMigrationsPerNode int64 progressTimeout int64 unsafeMigrationOverride boolean migrationPolicyName string migrationUid string mode string sourceNode string startTimestamp string targetAttachmentPodUID string targetCPUSet int32[] targetDirectMigrationNodePorts object property name* int32 targetNode string targetNodeAddress string targetNodeDomainDetected boolean targetNodeTopology string targetPod string migrationTransport string nodeName string phase string phaseTransitionTimestamps object[] Array [ phase string phaseTransitionTimestamp string ] qosClass string reason string runtimeUser int64 topologyHints object tscFrequency int64 virtualMachineRevisionName string volumeStatus object[] Array [ hotplugVolume object attachPodName string attachPodUID string memoryDumpVolume object claimName string endTimestamp string startTimestamp string targetFileName string message string name stringrequired persistentVolumeClaimInfo object accessModes string[] capacity object property name* string Default value: [object Object] filesystemOverhead string preallocated boolean requests object property name* string Default value: [object Object] volumeMode string phase string reason string size int64 target stringrequired ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machines","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine","content":"List Virtual Machines GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines Get a list of VirtualMachine objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Images For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-image-for-all-namespaces","content":"List Virtual Machine Images For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachineimages Get a list of all VirtualMachineImage objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Template Versions","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-template-version","content":"List Virtual Machine Template Versions GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions Get a list of VirtualMachineTemplateVersion objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Backups","type":0,"sectionRef":"#","url":"/v1.3/api/list-namespaced-virtual-machine-backup","content":"List Virtual Machine Backups GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups Get a list of VirtualMachineBackup objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Instance Migrations For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-instance-migration-for-all-namespaces","content":"List Virtual Machine Instance Migrations For All Namespaces GET /apis/kubevirt.io/v1/virtualmachineinstancemigrations Get a list of all VirtualMachineInstanceMigration objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Templates For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-template-for-all-namespaces","content":"List Virtual Machine Templates For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinetemplates Get a list of all VirtualMachineTemplate objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Restores For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-restore-for-all-namespaces","content":"List Virtual Machine Restores For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinerestores Get a list of all VirtualMachineRestore objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Cluster Network","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-cluster-network","content":"Patch a Cluster Network PATCH /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Patch a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Key Pair","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-key-pair","content":"Patch a Key Pair PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Patch a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Backups For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-backup-for-all-namespaces","content":"List Virtual Machine Backups For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinebackups Get a list of all VirtualMachineBackup objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-network-attachment-definition","content":"Patch a Network Attachment Definition PATCH /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Patch a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Node Network","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-node-network","content":"Patch a Node Network PATCH /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Patch a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Support Bundle","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-support-bundle","content":"Patch a Support Bundle PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Patch a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-persistent-volume-claim","content":"Patch a Persistent Volume Claim PATCH /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Patch a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch an Upgrade","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-upgrade","content":"Patch an Upgrade PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Patch a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine","content":"Patch a Virtual Machine PATCH /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine-backup","content":"Patch a Virtual Machine Backup PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine-backup","content":"Create a Virtual Machine Backup POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups Create a VirtualMachineBackup object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine-image","content":"Patch a Virtual Machine Image PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine-instance-migration","content":"Patch a Virtual Machine Instance Migration PATCH /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine","content":"Create a Virtual Machine POST /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines Create a VirtualMachine object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machines For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-for-all-namespaces","content":"List Virtual Machines For All Namespaces GET /apis/kubevirt.io/v1/virtualmachines Get a list of all VirtualMachine objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine-restore","content":"Patch a Virtual Machine Restore PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine-template","content":"Patch a Virtual Machine Template PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-network-attachment-definition","content":"Read a Network Attachment Definition GET /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Get a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Instances For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-instance-for-all-namespaces","content":"List Virtual Machine Instances For All Namespaces GET /apis/kubevirt.io/v1/virtualmachineinstances Get a list of all VirtualMachineInstance objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object activePods object property name* string conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] evacuationNodeName string fsFreezeStatus string guestOSInfo object id string kernelRelease string kernelVersion string machine string name string prettyName string version string versionId string interfaces object[] Array [ infoSource string interfaceName string ipAddress string ipAddresses string[] mac string name string ] launcherContainerImageVersion string migrationMethod string migrationState object abortRequested boolean abortStatus string completed boolean endTimestamp string failed boolean migrationConfiguration object allowAutoConverge boolean allowPostCopy boolean bandwidthPerMigration string completionTimeoutPerGiB int64 disableTLS boolean network string nodeDrainTaintKey string parallelMigrationsPerCluster int64 parallelOutboundMigrationsPerNode int64 progressTimeout int64 unsafeMigrationOverride boolean migrationPolicyName string migrationUid string mode string sourceNode string startTimestamp string targetAttachmentPodUID string targetCPUSet int32[] targetDirectMigrationNodePorts object property name* int32 targetNode string targetNodeAddress string targetNodeDomainDetected boolean targetNodeTopology string targetPod string migrationTransport string nodeName string phase string phaseTransitionTimestamps object[] Array [ phase string phaseTransitionTimestamp string ] qosClass string reason string runtimeUser int64 topologyHints object tscFrequency int64 virtualMachineRevisionName string volumeStatus object[] Array [ hotplugVolume object attachPodName string attachPodUID string memoryDumpVolume object claimName string endTimestamp string startTimestamp string targetFileName string message string name stringrequired persistentVolumeClaimInfo object accessModes string[] capacity object property name* string Default value: [object Object] filesystemOverhead string preallocated boolean requests object property name* string Default value: [object Object] volumeMode string phase string reason string size int64 target stringrequired ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.3/api/create-namespaced-virtual-machine-template-version","content":"Create a Virtual Machine Template Version POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions Create a VirtualMachineTemplateVersion object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Node Network","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-node-network","content":"Read a Node Network GET /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Get a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Cluster Network","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-cluster-network","content":"Read a Cluster Network GET /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Get a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Key Pair","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-key-pair","content":"Read a Key Pair GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Get a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Support Bundle","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-support-bundle","content":"Read a Support Bundle GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Get a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-persistent-volume-claim","content":"Read a Persistent Volume Claim GET /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Get a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read an Upgrade","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-upgrade","content":"Read an Upgrade GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Get a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-image","content":"Read a Virtual Machine Image GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-instance-migration","content":"Read a Virtual Machine Instance Migration GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-template","content":"Read a Virtual Machine Template GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-restore","content":"Read a Virtual Machine Restore GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Cluster Network","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-cluster-network","content":"Replace a Cluster Network PUT /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Update a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"List Virtual Machine Template Versions For All Namespaces","type":0,"sectionRef":"#","url":"/v1.3/api/list-virtual-machine-template-version-for-all-namespaces","content":"List Virtual Machine Template Versions For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinetemplateversions Get a list of all VirtualMachineTemplateVersion objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Key Pair","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-key-pair","content":"Replace a Key Pair PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Update a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-network-attachment-definition","content":"Replace a Network Attachment Definition PUT /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Update a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Patch a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.3/api/patch-namespaced-virtual-machine-template-version","content":"Patch a Virtual Machine Template Version PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Support Bundle","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-support-bundle","content":"Replace a Support Bundle PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Update a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace an Upgrade","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-upgrade","content":"Replace an Upgrade PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Update a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Node Network","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-node-network","content":"Replace a Node Network PUT /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Update a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-persistent-volume-claim","content":"Replace a Persistent Volume Claim PUT /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Update a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine-instance-migration","content":"Replace a Virtual Machine Instance Migration PUT /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine-image","content":"Replace a Virtual Machine Image PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine-template","content":"Replace a Virtual Machine Template PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine-restore","content":"Replace a Virtual Machine Restore PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/v1.3/authentication","content":"Authentication After installation, user will be prompted to set the password for the default admin user on the first-time login. note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","keywords":"Harvester harvester Rancher rancher Authentication","version":"v1.3 (latest)"},{"title":"Developer Mode","type":0,"sectionRef":"#","url":"/v1.3/developer/developer-mode-installation","content":"Developer Mode attention Developer mode is intended to be used for development and testing purposes. Usage of this mode in production environments is not supported. Prerequisites​ The node has passed the host-checkHelm 3 and Git are installed on your local machine. Installation of the First Node​ You can install Harvester on an RKE2 cluster using the Helm CLI. For more information about installing and configuring the Harvester Helm chart, see the readme. Create an RKE2 configuration file. sudo mkdir -p /etc/rancher/rke2 cat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml disable: - rke2-snapshot-controller - rke2-snapshot-controller-crd - rke2-snapshot-validation-webhook node-label: - harvesterhci.io/managed=true token: token cni: - multus - canal EOF Install RKE2. curl -sfL https://get.rke2.io | sudo sh - sudo systemctl enable rke2-server.service --now Create a kubeconfig file. mkdir -p ~/.kube sudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config sudo chown $(id -u):$(id -g) ~/.kube/config Install system-upgrade-controller. This Kubernetes-native upgrade controller for nodes installs upgrade.cattle.io/v1 CRDs. kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.13.1/system-upgrade-controller.yaml note If you are unable to locate the kubectl binary in /usr/local/bin, check /var/lib/rancher/rke2/bin. Create the cattle-system namespace. kubectl create ns cattle-system Add the Rancher chart repository. helm repo add rancher-latest https://releases.rancher.com/server-charts/latest Install the Rancher v2.7.5 chart. helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set tls=external \\ --set rancherImagePullPolicy=IfNotPresent \\ --set rancherImage=rancher/rancher \\ --set rancherImageTag=v2.7.5 \\ --set noDefaultAdmin=false \\ --set features=&quot;multi-cluster-management=false\\,multi-cluster-management-agent=false&quot; \\ --set useBundledSystemChart=true \\ --set bootstrapPassword=admin Clone the rancher/charts repository. git clone https://github.com/rancher/charts -b dev-v2.7 Install the rancher-monitoring-crd chart. helm install rancher-monitoring-crd ./charts/charts/rancher-monitoring-crd/102.0.2+up40.1.2/ Create the harvester-system namespace. kubectl create ns harvester-system Clone the harvester/harvester repository. git clone https://github.com/harvester/harvester.git Install the harvester-crd chart. helm install harvester-crd ./harvester/deploy/charts/harvester-crd --namespace harvester-system Install the Harvester chart using kube-vip running on a static IP. VIP_ADDRESS=&quot;replace with an IP which is allocated to any device, such as 192.168.5.131&quot; helm install harvester ./harvester/deploy/charts/harvester --namespace harvester-system \\ --set harvester-node-disk-manager.enabled=true \\ --set &quot;harvester-node-disk-manager.labelFilter={COS_*,HARV_*}&quot; \\ --set harvester-network-controller.enabled=true \\ --set harvester-network-controller.vipEnabled=true \\ --set harvester-load-balancer.enabled=true \\ --set kube-vip.enabled=true \\ --set kube-vip-cloud-provider.enabled=true \\ --set longhorn.enabled=true \\ --set longhorn.defaultSettings.defaultDataPath=/var/lib/harvester/defaultdisk \\ --set longhorn.defaultSettings.taintToleration=kubevirt.io/drain:NoSchedule \\ --set rancherEmbedded=true \\ --set service.vip.enabled=true \\ --set service.vip.mode=static \\ --set service.vip.ip=${VIP_ADDRESS} Access the Harvester UI at https://${VIP_ADDRESS}. The default password is admin. Installation of Other Nodes​ Create an RKE2 configuration file. sudo mkdir -p /etc/rancher/rke2 cat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml server: https://&lt;vip address&gt;:9345 token: token EOF Install the RKE2 agent. curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=&quot;agent&quot; sudo sh - sudo systemctl enable rke2-agent.service --now Uninstallation​ sudo /usr/local/bin/rke2-uninstall.sh ","keywords":"Harvester harvester Rancher rancher Developer Mode Installation","version":"v1.3 (latest)"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/v1.3/faq","content":"FAQ This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node?​ $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard?​ username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster?​ Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM?​ # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/reference/cli.html#clean How can I reset the administrator password?​ In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher --no-headers | head -1 | awk '{ print $1 }') -c rancher -- reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt; I added an additional disk with partitions. Why is it not getting detected?​ As of Harvester v1.0.2, we no longer support adding additional partitioned disks, so be sure to delete all partitions first (e.g., using fdisk). Why are there some Harvester pods that become ErrImagePull/ImagePullBackOff?​ This is likely because your Harvester cluster is an air-gapped setup, and some pre-loaded container images are missing. Kubernetes has a mechanism that does garbage collection against bloated image stores. When the partition which stores container images is over 85% full, kubelet tries to prune the images based on the last time they were used, starting with the oldest, until the occupancy is lower than 80%. These numbers (85% and 80%) are default High/Low thresholds that come with Kubernetes. To recover from this state, do one of the following depending on the cluster's configuration: Pull the missing images from sources outside of the cluster (if it's an air-gapped environment, you might need to set up an HTTP proxy beforehand).Manually import the images from the Harvester ISO image. note Take v1.1.2 as an example, download the Harvester ISO image from the official URL. Then extract the image list from the ISO image to decide which image tarball we're going to import. For instance, we want to import the missing container image rancher/harvester-upgrade $ curl -sfL https://releases.rancher.com/harvester/v1.1.2/harvester-v1.1.2-amd64.iso -o harvester.iso $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images-lists images-lists $ grep -R &quot;rancher/harvester-upgrade&quot; images-lists/ images-lists/harvester-images-v1.1.2.txt:docker.io/rancher/harvester-upgrade:v1.1.2 Find out the location of the image tarball, and extract it from the ISO image. Decompress the extracted zstd image tarball. $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images/harvester-images-v1.1.2.tar.zst harvester.tar.zst $ zstd -d --rm harvester.tar.zst Upload the image tarball to the Harvester nodes that need recover. Finally, execute the following command to import the container images on each of them. $ ctr -n k8s.io images import harvester.tar $ rm harvester.tar Find the missing images on that node from the other nodes, then export the images from the node where the images still exist and import them on the missing node. To prevent this from happening, we recommend cleaning up unused container images from the previous version after each successful Harvester upgrade if the image store disk space is stressed. We provided a harv-purge-images script that makes cleaning up disk space easy, especially for container image storage. The script has to be executed on each Harvester node. For example, if the cluster was originally in v1.1.2, and now it gets upgraded to v1.2.0, you can do the following to discard the container images that are only used in v1.1.2 but no longer needed in v1.2.0: # on each node $ ./harv-purge-images.sh v1.1.2 v1.2.0 caution The script only downloads the image lists and compares the two to calculate the difference between the two versions. It does not communicate with the cluster and, as a result, does not know what version the cluster was upgraded from.We published image lists for each version released since v1.1.0. For clusters older than v1.1.0, you have to clean up the old images manually.","keywords":"","version":"v1.3 (latest)"},{"title":"Host Management","type":0,"sectionRef":"#","url":"/v1.3/host/","content":"Host Management Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance​ For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node​ Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you’re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node​ caution Before removing a node from a Harvester cluster, determine if the remaining nodes have enough computing and storage resources to take on the workload from the node to be removed. Check the following: Current resource usage in the cluster (on the Hosts screen of the Harvester UI)Ability of the remaining nodes to maintain enough replicas for all volumes If the remaining nodes do not have enough resources, VMs might fail to migrate and volumes might degrade when you remove a node. 1. Check if the node can be removed from the cluster.​ You can safely remove a control plane node depending on the quantity and availability of other nodes in the cluster. The cluster has three control plane nodes and one or more worker nodes. When you remove a control plane node, a worker node will be promoted to control plane node. Harvester v1.3.0 allows you to assign a role to each node that joins a cluster. In earlier Harvester versions, worker nodes were randomly selected for promotion. If you prefer to promote specific nodes, please see Role Management and Harvester Configuration for more information. The cluster has three control plane nodes and no worker nodes. You must add a new node to the cluster before removing a control plane node. This ensures that the cluster always has three control plane nodes and that a quorum can be formed even if one control plane node fails. The cluster has only two control plane nodes and no worker nodes. Removing a control plane node in this situation is not recommended because etcd data is not replicated in a single-node cluster. Failure of a single node can cause etcd to lose its quorum and shut the cluster down. 2. Check the status of volumes.​ Access the embedded Longhorn UI. Go to the Volume screen. Verify that the state of all volumes is Healthy. 3. Evict replicas from the node to be removed.​ Access the embedded Longhorn UI. Go to the Node screen. Select the node that you want to remove, click the icon in the Operation column, and then select Edit node and disks. Configure the following settings: Node Scheduling: Select Disable.Evict Requested&quot; Select True. Click Save. Go back to the Node screen and verify that Replicas value for the node to be removed is 0. important Eviction cannot be completed if the remaining nodes cannot accept replicas from the node to be removed. In this case, some volumes will remain in the Degraded state until you add more nodes to the cluster. 4. Manage non-migratable VMs.​ Live migration cannot be performed for VMs with certain properties. The VM has PCI passthrough devices or vGPU devices. A PCI device is bound to a node. You must remove the PCI device from the VM, or delete the VM and then create a new VM from a backup or snapshot. The VM has a node selector or affinity rules that bind it to the node to be removed. You must change the node selector or affinity rules. The VM is on a VM network that binds it to the node to be removed. You must select a different VM network. tip Create a backup or snapshot for each non-migratable VM before modifying the settings that bind it to the node that you want to remove. 5. Evict workloads from the node to be removed.​ If your cluster is running Harvester v1.1.2 or later, you can enable Maintenance Mode on the node to automatically live-migrate VMs and workloads. You can also manually live-migrate VMs to other nodes. All workloads have been successfully evicted if the node state is Maintenance. important If a cluster has only two control plane nodes, Harvester does not allow you to enable Maintenance Mode on any node. You can manually drain the node to be removed using the following command: kubectl drain &lt;node_name&gt; --force --ignore-daemonsets --delete-local-data --pod-selector='app!=csi-attacher,app!=csi-provisioner' Again, removing a control plane node in this situation is not recommended because etcd data is not replicated. Failure of a single node can cause etcd to lose its quorum and shut the cluster down. 6. Remove the node.​ On the Harvester UI, go to the Hosts screen. Locate the node that you want to remove, and then click ⋮ &gt; Delete. 7. Delete RKE2 services on the node.​ Log in to the node using the root account. Run the script /opt/rke2/bin/rke2-uninstall.sh. note There's a known issue about node hard delete. Once resolved, you can skip this step. Role Management​ Hardware issues may force you to replace the management node. In earlier Harvester versions, accurately promoting a specific worker node to a management node was not easy. Harvester v1.3.0 improves the process by introducing the following roles: Management: Allows a node to be prioritized when Harvester promotes nodes to management nodes.Witness: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster.Worker: Restricts a node to being a worker node (never promoted to management node) in a specific cluster. caution Harvester currently allows only one witness node in the cluster. For more information about assigning roles to nodes, see ISO Installation. Multi-disk Management​ Add Additional Disks​ Users can view and add multiple disks as additional data volumes from the edit host page. Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config. Select the Storage tab and click Add Disk. caution As of Harvester v1.0.2, we no longer support adding partitions as additional disks. If you want to add it as an additional disk, be sure to delete all partitions first (e.g., using fdisk). Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. Last, you can click ⋮ &gt; Edit Config again to check the newly added disk. Meanwhile, you can also add the &quot;Host/Disk&quot; tag (details are described in the next section). note In order for Harvester to identify the disks, each disk needs to have a unique WWN. Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above. However, you can still add a virtual disk with the SCSI controller. The WWN information could be added manually along with the disk attach operation. For more details, please refer to the script. Storage Tags​ The storage tag feature enables only certain nodes or disks to be used for storing Longhorn volume data. For example, performance-sensitive data can use only the high-performance disks which can be tagged as fast, ssd or nvme, or only the high-performance nodes tagged as baremetal. This feature supports both disks and nodes. Setup​ The tags can be set up through the Harvester UI on the host page: Click Hosts -&gt; Edit Config -&gt; StorageClick Add Host/Disk Tags to start typing and hit enter to add new tags.Click Save to update tags.On the StorageClasses page, create a new storage class and select those defined tags on the Node Selector and Disk Selector fields. All the existing scheduled volumes on the node or disk won’t be affected by the new tags. note When multiple tags are specified for a volume, the disk and the nodes (that the disk belongs to) must have all the specified tags to become usable. Remove disks​ Before removing a disk, you must first evict Longhorn replicas on the disk. note The replica data would be rebuilt to another disk automatically to keep the high availability. Identify the disk to remove (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select the node name and go to the Storage tab.Find the disk you want to remove. Let's assume we want to remove /dev/sdb, and the disk's mount point is /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04. Evict replicas (Longhorn dashboard)​ Please follow this session to enable the embedded Longhorn dashboard.Visit the Longhorn dashboard and go to the Node page.Expand the node containing the disk. Confirm the mount point /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04 is in the disks list. Select Edit node and disks. Scroll to the disk you want to remove. Set Scheduling to Disable.Set Eviction Requested to True.Select Save. Do not select the delete icon. The disk will be disabled. Please wait until the disk replica count becomes 0 to proceed with removing the disk. Remove the disk (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select ⋮ &gt; Edit Config.Go to the Storage tab and select x to remove the disk. Select Save to remove the disk. Ksmtuned Mode​ Available as of v1.1.0 Ksmtuned is a KSM automation tool deployed as a DaemonSet to run Ksmtuned on each node. It will start or stop the KSM by watching the available memory percentage ratio (i.e. Threshold Coefficient). By default, you need to manually enable Ksmtuned on each node UI. You will be able to see the KSM statistics from the node UI after 1-2 minutes.(check KSM for more details). Quick Run​ Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config.Select the Ksmtuned tab and select Run in Run Strategy.(Optional) You can modify Threshold Coefficient as needed. Click Save to update.Wait for about 1-2 minutes and you can check its Statistics by clicking Your Node &gt; Ksmtuned tab. Parameters​ Run Strategy: Stop: Stop Ksmtuned and KSM. VMs can still use shared memory pages.Run: Run Ksmtuned.Prune: Stop Ksmtuned and prune KSM memory pages. Threshold Coefficient: configures the available memory percentage ratio. If the available memory is less than the threshold, KSM will be started; otherwise, KSM will be stopped. Merge Across Nodes: specifies if pages from different NUMA nodes can be merged. Mode: Standard: The default mode. The control node ksmd uses about 20% of a single CPU. It uses the following parameters: Boost: 0 Decay: 0 Maximum Pages: 100 Minimum Pages: 100 Sleep Time: 20 High-performance: Node ksmd uses 20% to 100% of a single CPU and has higher scanning and merging efficiency. It uses the following parameters: Boost: 200 Decay: 50 Maximum Pages: 10000 Minimum Pages: 100 Sleep Time: 20 Customized: You can customize the configuration to reach the performance that you want. Ksmtuned uses the following parameters to control KSM efficiency: Parameters\tDescriptionBoost\tThe number of scanned pages is incremented each time if the available memory is less than the Threshold Coefficient. Decay\tThe number of scanned pages is decremented each time if the available memory is greater than the Threshold Coefficient. Maximum Pages\tMaximum number of pages per scan. Minimum Pages\tThe minimum number of pages per scan, also the configuration for the first run. Sleep Time (ms)\tThe interval between two scans, which is calculated with the formula (Sleep Time * 16 * 1024* 1024 / Total Memory). Minimum: 10ms. For example, assume you have a 512GiB memory node that uses the following parameters: Boost: 300 Decay: 100 Maximum Pages: 5000 Minimum Pages: 1000 Sleep Time: 50 When Ksmtuned starts, initialize pages_to_scan in KSM to 1000 (Minimum Pages) and set sleep_millisecs to 10 (50 * 16 * 1024 * 1024 / 536870912 KiB &lt; 10). KSM starts when the available memory falls below the Threshold Coefficient. If it detects that it is running, pages_to_scan increments by 300 (Boost) every minute until it reaches 5000 (Maximum Pages). KSM will stop when the available memory is above the Threshold Coefficient. If it detects that it is stopped, pages_to_scan decrements by 100 (Decay) every minute until it reaches 1000 (Minimum Pages). NTP Configuration​ Time synchronization is an important aspect of distributed cluster architecture. Because of this, Harvester now provides a simpler way for configuring NTP settings. In previous Harvester versions, NTP settings were mainly configurable during the installation process. To modify the settings, you needed to manually update the configuration file on each node. Beginning with version v1.2.0, Harvester is supporting NTP configuration on the Harvester UI Settings screen (Advanced &gt; Settings). You can configure NTP settings for the entire Harvester cluster at any time, and the settings are applied to all nodes in the cluster. You can set up multiple NTP servers at once. You can check the settings in the node.harvesterhci.io/ntp-service annotation in Kubernetes nodes: ntpSyncStatus: Status of the connection to NTP servers (possible values: disabled, synced and unsynced)currentNtpServers: List of existing NTP servers $ kubectl get nodes harvester-node-0 -o yaml |yq -e '.metadata.annotations.[&quot;node.harvesterhci.io/ntp-service&quot;]' {&quot;ntpSyncStatus&quot;:&quot;synced&quot;,&quot;currentNtpServers&quot;:&quot;0.suse.pool.ntp.org 1.suse.pool.ntp.org&quot;} Note: Do not modify the NTP configuration file on each node. Harvester will automatically sync the settings that you configured on the Harvester UI to the nodes.If you upgraded Harvester from an earlier version, the ntp-servers list on the Settings screen will be empty (see screenshot). You must manually configure the NTP settings because Harvester is unaware of the previous settings and is unable to detect conflicts. Cloud-Native Node Configuration​ You may need to customize one or more nodes after installing Harvester. This process usually entails updating the runtime configuration and modifying files in the /oem directory of each node to make changes persist after rebooting. In Harvester v1.3.0, these customizations can be described in a Kubernetes manifest and then applied to the underlying cluster using kubectl or other GitOps-centric tools such as Fleet. danger Misconfigurations might compromise the ability of a Harvester node to boot up, or even damage the overall stability of the cluster. You can prevent such issues by reading the Elemental toolkit documentation to learn how to correctly customize Elemental. Creating a CloudInit Resource​ Harvester node customization is bounded only by your creativity and by what the Elemental toolkit markup can syntactically express. The documentation, therefore, cannot provide an exhaustive list of possible customizations and use cases. Example: You want to add an SSH authorized key for the default rancher user on all nodes. Start by creating a Kubernetes manifest for a CloudInit resource. file: ssh_access.yaml apiVersion: node.harvesterhci.io/v1beta1 kind: CloudInit metadata: name: ssh-access spec: matchSelector: {} filename: 99_ssh.yaml contents: | stages: network: - authorized_keys: rancher: - ssh-ed25519 AAAA... This manifest describes an Elemental cloud-init document that will be applied to all nodes (because the empty matchSelector: {} field matches everything). The YAML document in the .spec.contents field will be rendered to /oem/99_ssh.yaml (because of the .spec.filename field.) Apply this example using the command kubectl apply -f ssh_access.yaml. tip Reboot the relevant Harvester nodes so that the Elemental toolkit executor can apply the new configuration at boot. CloudInit Resource Spec​ Field\tRequired\tDescriptionmatchSelector\tYes\tSetting that allows you to specify the nodes that will receive the configuration changes. filename\tYes\tName of the file that appears in /oem. contents\tYes\tElemental toolkit cloud-init-style file that will be rendered to a file in /oem. paused\tNo\tWhen set to true, the file will not be updated on nodes as it changes. The matchSelector field can be used to target specific nodes or groups of nodes based on their labels. Example: matchSelector: kubernetes.io/hostname: &quot;harvester-node-1&quot; note All label key-value pairs listed in the matchSelector field must match the labels of the intended nodes. In the following example, matchSelector will match harvester-node-1 only if that node also has the example.com/role label with the value role-a. matchSelector: kubernetes.io/hostname: &quot;harvester-node-1&quot; example.com/role: &quot;role-a&quot; Updating a CloudInit Resource​ You can use the command kubectl edit to update a CloudInit resource. However, there is a caveat if the matchSelector field is updated to exclude one or more nodes from the customization. See the note in the Deleting a CloudInit Resource section regarding rolling back customizations. # kubectl edit cloudinit CLOUDINIT_NAME Deleting a CloudInit Resource​ You can use the command kubectl delete to remove a CloudInit resource from the Harvester cluster. # kubectl delete cloudinit CLOUDINIT_NAME note Harvester is unable to &quot;roll back&quot; previously described customizations because the CloudInit resource can describe anything that can be expressed as an Elemental toolkit customization, including arbitrary shell commands. In the Creating a CloudInit Resource example, the YAML file contains the authorized_keys stanza. This is an append-only action in the Elemental toolkit. When the resource is changed or deleted, the authorized_keys file in Rancher will still contain the old public key. You are responsible for amending or creating a CloudInit resource that rolls the changes back (if necessary) before you reboot the node. Troubleshooting CloudInit Rollouts​ If an Elemental toolkit cloud-init document does not appear in /oem or does not contain the expected contents, the status block of the CloudInit resource might contain useful hints. # kubectl get cloudinit CLOUDINIT_NAME -o yaml status: rollouts: harvester-dngmf: conditions: - lastTransitionTime: &quot;2024-02-28T22:31:23Z&quot; message: &quot;&quot; reason: CloudInitApplicable status: &quot;True&quot; type: Applicable - lastTransitionTime: &quot;2024-02-28T22:31:23Z&quot; message: Local file checksum is the same as the CloudInit checksum reason: CloudInitChecksumMatch status: &quot;False&quot; type: OutOfSync - lastTransitionTime: &quot;2024-02-28T22:31:23Z&quot; message: 99_ssh.yaml is present under /oem reason: CloudInitPresentOnDisk status: &quot;True&quot; type: Present The harvester-node-manager pod(s) in the harvester-system namespace may also contain some hints as to why it is not rendering a file to a node. This pod is part of a daemonset, so it may be worth checking the pod that is running on the node of interest.","keywords":"","version":"v1.3 (latest)"},{"title":"Harvester Configuration","type":0,"sectionRef":"#","url":"/v1.3/install/harvester-configuration","content":"Harvester Configuration Configuration Example​ Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: scheme_version: 1 server_url: https://cluster-VIP:443 token: TOKEN_VALUE os: ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files: - encoding: &quot;&quot; content: test content owner: root path: /etc/test.txt permissions: '0755' hostname: myhost modules: - kvm - nvme sysctls: kernel.printk: &quot;4 4 1 7&quot; kernel.kptr_restrict: &quot;1&quot; dns_nameservers: - 8.8.8.8 - 1.1.1.1 ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password: rancher environment: http_proxy: http://myserver https_proxy: http://myserver labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7C&quot; method: dhcp force_efi: true device: /dev/sda data_disk: /dev/sdb silent: true iso_url: http://myserver/test.iso poweroff: true no_format: true debug: true tty: ttyS0 vip: 10.10.0.19 vip_hw_addr: 52:54:00:ec:0e:0b vip_mode: dhcp force_mbr: false addons: harvester_vm_import_controller: enabled: false values_content: &quot;&quot; harvester_pcidevices_controller: enabled: false values_content: &quot;&quot; rancher_monitoring: enabled: true values_content: &quot;&quot; rancher_logging: enabled: false values_content: &quot;&quot; harvester_seeder: enabled: false values_content: &quot;&quot; system_settings: auto-disk-provision-paths: &quot;&quot; Configuration Reference​ Below is a reference of all configuration keys. caution Security Risks: The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. note Configuration Priority: When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname. scheme_version​ Definition​ The version of scheme reserved for future configuration migration. This configuration is mandatory for migrating the configuration to a new scheme version. It tells Harvester the previous version and the need to migrate. note This field didn't take any effect in the current Harvester version. caution Make sure that your custom configuration always has the correct scheme version. server_url​ Definition​ server_url is the URL of the Harvester cluster, which is used for the new node to join the cluster. This configuration is mandatory when the installation is in JOIN mode. The default format of server_url is https://cluster-VIP:443. note To ensure a high availability (HA) Harvester cluster, please use either the Harvester cluster VIP or a domain name in server_url. Example​ server_url: https://cluster-VIP:443 install: mode: join token​ Definition​ The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example​ token: myclustersecret Or a node token token: &quot;K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4&quot; os.ssh_authorized_keys​ Definition​ A list of SSH authorized keys that should be added to the default user, rancher. SSH keys can be obtained from GitHub user accounts by using the formatgithub:${USERNAME}. This is done by downloading the keys from https://github.com/${USERNAME}.keys. Example​ os: ssh_authorized_keys: - &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D&quot; - &quot;github:ibuildthecloud&quot; os.write_files​ A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: &quot;&quot;: content data are written in plain text. In this case, the encoding field can be also omitted.b64, base64: content data are base64-encoded.gz, gzip: content data are gzip-compressed.gz+base64, gzip+base64, gz+b64, gzip+b64: content data are gzip-compressed first and then base64-encoded. Example os: write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner: root:root path: /etc/connman/main.conf permissions: '0644' - content: | # My new /etc/sysconfig/samba file SMDBOPTIONS=&quot;-D&quot; path: /etc/sysconfig/samba - content: !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path: /bin/arch permissions: '0555' - content: | 15 * * * * root ship_logs path: /etc/crontab os.persistent_state_paths​ Definition​ The os.persistent_state_paths option allows you to configure custom paths where modifications made to files will persist across reboots. Any changes to files in these paths will not be lost after a reboot. Example​ Refer to the following example config for installing rook-ceph in Harvester: os: persistent_state_paths: - /var/lib/rook - /var/lib/ceph modules: - rbd - nbd os.after_install_chroot_commands​ Definition​ You can add additional software packages with after_install_chroot_commands. The after-install-chroot stage, provided by elemental-toolkit, allows you to execute commands not restricted by file system write issues, ensuring the persistence of user-defined commands even after a system reboot. Example​ Refer to the following example config for installing an RPM package in Harvester: os: after_install_chroot_commands: - rpm -ivh &lt;the url of rpm package&gt; DNS resolution is unavailable in the after-install-chroot stage, and the nameserver might not be available. If you need to access a domain name to install a package using an URL, create a temporary /etc/resolv.conf file first. For example: os: after_install_chroot_commands: - &quot;rm -f /etc/resolv.conf &amp;&amp; echo 'nameserver 8.8.8.8' | sudo tee /etc/resolv.conf&quot; - &quot;mkdir /usr/local/bin&quot; - &quot;curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 &amp;&amp; chmod 700 get_helm.sh &amp;&amp; ./get_helm.sh&quot; - &quot;rm -f /etc/resolv.conf &amp;&amp; ln -s /var/run/netconfig/resolv.conf /etc/resolv.conf&quot; note Upgrading Harvester causes the changes to the OS in the after-install-chroot stage to be lost. You must also configure the after-upgrade-chroot to make your changes persistent across an upgrade. Refer to Runtime persistent changes before upgrading Harvester. os.hostname​ Definition​ Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value. Example​ os: hostname: myhostname os.modules​ Definition​ A list of kernel modules to be loaded on start. Example​ os: modules: - kvm - nvme os.sysctls​ Definition​ Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf. Values must be specified as strings. Example​ os: sysctls: kernel.printk: 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict: &quot;1&quot; # force the YAML parser to read as a string os.dns_nameservers​ Definition​ Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example​ os: dns_nameservers: - 8.8.8.8 - 1.1.1.1 os.ntp_servers​ Definition​ Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines. Example​ os: ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org os.password​ Definition​ The password for the default user, rancher. By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from/etc/shadow. You can also encrypt a password using OpenSSL. For the encryption algorithms supported by Harvester, please refer to the table below. Algorithm\tCommand\tSupportSHA-512\topenssl passwd -6\tYes SHA-256\topenssl passwd -5\tYes MD5\topenssl passwd -1\tYes MD5, Apache variant\topenssl passwd -apr1\tYes AIX-MD5\topenssl passwd -aixmd5\tNo Example​ Encrypted: os: password: &quot;$6$kZYUnRaTxNdg4W8H$WSEJydGWsNpaRbbbRdTDLJ2hDLbkizxSFGW2RtexlqG6njEATaGQG9ssztjaKDCsaNUPBZ1E1YdsvSLMAi/IO/&quot; Or clear text: os: password: supersecure os.environment​ Definition​ Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example​ os: environment: http_proxy: http://myserver https_proxy: http://myserver note This example sets the HTTP(S) proxy for foundational OS components. To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy. os.labels​ Definition​ Labels to be added to this Node. Example​ os: labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue os.sshd.sftp​ Definition​ Subsystem used to configure the OpenSSH Daemon (sshd). Harvester currently only supports sftp. Example​ os: sshd: sftp: true # The SFTP subsystem is enabled. install.mode​ Definition​ Harvester installation mode: create: Creating a new Harvester installation.join: Join an existing Harvester installation. Need to specify server_url. Example​ install: mode: create install.role​ Definition​ Role assigned to a node at the time of installation. When unspecified, Harvester assigns the default role. default: Allows a node to function as a management node or a worker node.management: Allows a node to be prioritized when Harvester promotes nodes to management nodes.worker: Restricts a node to being a worker node (never promoted to management node) in a specific cluster.witness: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster. install.management_interface​ Definition​ Configure network interfaces for the host machine. Valid configuration fields are: method: Method to assign an IP to this network. The following are supported: static: Manually assign an IP and gateway.dhcp: Request an IP from the DHCP server. ip: Static IP for this network. Required if static method is chosen.subnet_mask: Subnet mask for this network. Required if static method is chosen.gateway: Gateway for this network. Required if static method is chosen.interfaces: An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name: The name of the slave interface for the bonded network.interfaces.hwAddr: The hardware MAC address of the interface. bond_options: Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlbmiimon: 100 mtu: The MTU for the interface.vlan_id: The VLAN ID for the interface. note Harvester uses the systemd net naming scheme. Please make sure the interface name is present on the target machine before installation. Example​ install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7D&quot; # The hwAddr is optional method: dhcp bond_options: mode: balance-tlb miimon: 100 mtu: 1492 vlan_id: 101 install.force_efi​ Force EFI installation even when EFI is not detected. Default: false. install.device​ The device to install the OS. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. install.silent​ Reserved. install.iso_url​ ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff​ Shutdown the machine after installation instead of rebooting install.no_format​ Do not partition and format, assume layout exists already. install.debug​ Run the installation with additional logging and debugging enabled for the installed system. install.persistent_partition_size​ Definition​ Configure the size of partition COS_PERSISTENT in Gi or Mi. This partition is used to store data like system packages and container images. The default and minimum value is 150Gi. Example​ install: persistent_partition_size: 150Gi install.skipchecks​ Definition​ Allow installation to proceed even if minimum requirements for production use are not met. Default: false. The installer automatically checks if the hardware meets the minimum requirements for production use. When performing automated installation via PXE Boot, if any of the checks fail, installation is stopped, and warnings are printed to the system console and saved to /var/log/console.log in the installation environment. To override this behavior, set install.skipchecks=true. When set to true, warning messages are still saved to /var/log/console.log, but the installation proceeds even if hardware requirements for production use are not met. Example​ install: skipchecks: true install.tty​ Definition​ The tty device used for the console. Example​ install: tty: ttyS0,115200n8 install.vip​ install.vip_mode​ install.vip_hw_addr​ Definition​ install.vip: The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://&lt;VIP&gt;.install.vip_mode dhcp: Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided.static: Harvester uses a static VIP. install.vip_hw_addr: The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp. See Management Address for more information. Example​ Configure a static VIP. install: vip: 192.168.0.100 vip_mode: static Configure a DHCP VIP. install: vip: 10.10.0.19 vip_mode: dhcp vip_hw_addr: 52:54:00:ec:0e:0b install.force_mbr​ Definition​ By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. note Harvester creates an additional partition for storing VM data ifinstall.data_disk is configured to use the same storage device as the one set for install.device. When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example​ install: force_mbr: true install.data_disk​ Available as of v1.0.1 Definition​ Sets the default storage device to store the VM data. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. Default: Same storage device as the one set for install.device Example​ install: data_disk: /dev/sdb install.addons​ Available as of v1.2.0 Definition​ Sets the default enabled/disabled status of Harvester addons. Default: The addons are disabled. Example​ install: addons: rancher_monitoring: enabled: true rancher_logging: enabled: false Harvester v1.2.0 ships with five addons: vm-import-controller (chartName: harvester-vm-import-controller)pcidevices-controller (chartName: harvester-pcidevices-controller)rancher-monitoringrancher-loggingharvester-seeder (experimental) install.harvester.storage_class.replica_count​ Available as of v1.1.2 Definition​ Sets the replica count of Harvester's default storage class harvester-longhorn. Default: 3 Supported values: 1, 2, 3. All other values are considered 3. In edge scenarios where users may deploy single-node Harvester clusters, they can set this value to 1. In most scenarios, it is recommended to keep the default value 3 for storage high availability. Please refer to longhorn-replica-count for more details. Example​ install: harvester: storage_class: replica_count: 1 install.harvester.longhorn.default_settings.guaranteedEngineManagerCPU​ Available as of v1.2.0 Definition​ Sets the default percentage of the total allocatable CPU on each node will be reserved for each Longhorn engine manager Pod. Default: 12 Supported values: 0-12. All other values are considered 12. This integer value indicates what percentage of the total allocatable CPU on each node will be reserved for each engine manager Pod. In edge scenarios where users may deploy single-node Harvester clusters, they can set this parameter to a value smaller than 12. In most scenarios, it is recommended to keep the default value for system high availability. Before setting the value, please refer to longhorn-guaranteed-engine-manager-cpu for more details. Example​ install: harvester: longhorn: default_settings: guaranteedEngineManagerCPU: 6 install.harvester.longhorn.default_settings.guaranteedReplicaManagerCPU​ Available as of v1.2.0 Definition​ Sets the default percentage of the total allocatable CPU on each node will be reserved for each Longhorn replica manager Pod. Default: 12 Supported values: 0-12. All other values are considered 12. This integer value indicates what percentage of the total allocatable CPU on each node will be reserved for each replica manager Pod. In edge scenarios where users may deploy single-node Harvester clusters, can set this parameter to a value smaller than 12. In most scenarios, it is recommended to keep the default value for system high availability. Before setting the value, please refer to longhorn-guaranteed-replica-manager-cpu for more details. Example​ install: harvester: longhorn: default_settings: guaranteedReplicaManagerCPU: 6 system_settings​ Definition​ You can overwrite the default Harvester system settings by configuring system_settings. See the Settings page for additional information and the list of all the options. note Overwriting system settings only works when Harvester is installed in &quot;create&quot; mode. If you install Harvester in &quot;join&quot; mode, this setting is ignored. Installing in &quot;join&quot; mode will adopt the system settings from the existing Harvester system. Example​ The example below overwrites containerd-registry, http-proxy and ui-source settings. The values must be a string. system_settings: containerd-registry: '{&quot;Mirrors&quot;: {&quot;docker.io&quot;: {&quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;]}}, &quot;Configs&quot;: {&quot;myregistry.local:5000&quot;: {&quot;Auth&quot;: {&quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot;}, &quot;TLS&quot;: {&quot;InsecureSkipVerify&quot;: false}}}}' http-proxy: '{&quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc&quot;}' ui-source: auto ","keywords":"Harvester harvester Rancher rancher Harvester Configuration","version":"v1.3 (latest)"},{"title":"ISO Installation","type":0,"sectionRef":"#","url":"/v1.3/install/index","content":"ISO Installation Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image. To get the ISO image, download 💿 harvester-v1.x.x-amd64.iso from the Harvester releases page. During the installation, you can either choose to create a new Harvester cluster or join the node to an existing Harvester cluster. The following video shows a quick overview of an ISO installation. Installation Steps​ Mount the Harvester ISO file and boot the server by selecting the Harvester Installer option. The installer automatically checks the hardware and displays warning messages if the minimum requirements are not met. The Hardware Checks screen is not displayed if all checks are passed. Use the arrow keys to choose an installation mode. By default, the first node will be the management node of the cluster. Create a new Harvester cluster: creates an entirely new Harvester cluster. Join an existing Harvester cluster: joins an existing Harvester cluster. You need the VIP and cluster token of the cluster you want to join. Install Harvester binaries only: If you choose this option, additional setup is required after the first bootup. info When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config by providing a URL of Harvester configuration on the customize the host step. In this case, at least three different zones are required. Choose a role for the node. You are required to perform this step if you selected the installation mode Join an existing Harvester cluster. Default Role: Allows a node to function as a management node or a worker node. This role does not have any specific privileges or restrictions.Management Role: Allows a node to be prioritized when Harvester promotes nodes to management nodes.Witness Role: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster.Worker Role: Restricts a node to being a worker node (never promoted to management node) in a specific cluster. Choose the installation disk you want to install the Harvester cluster on and the data disk you want to store VM data on. By default, Harvester uses GUID Partition Table (GPT) partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select Master boot record (MBR). Installation disk: The disk to install the Harvester cluster on. Data disk: The disk to store VM data on. Choosing a separate disk to store VM data is recommended.Persistent size: If you only have one disk or use the same disk for both OS and VM data, you need to configure persistent partition size to store system packages and container images. The default and minimum persistent partition size is 150 GiB. You can specify a size like 200Gi or 153600Mi. Configure the HostName of the node. Configure network interface(s) for the management network. By default, Harvester creates a bonded NIC named mgmt-bo, and the IP address can be configured via DHCP or statically assigned. note It is not possible to change the node IP throughout the lifecycle of a Harvester cluster. If using DHCP, you must ensure the DHCP server always offers the same IP for the same node. If the node IP is changed, the related node cannot join the cluster and might even break the cluster. In addition, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. For example: Linux~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see DHCP Server Configuration. (Optional) Configure the DNS Servers. Use commas as a delimiter to add more DNS servers. Leave it blank to use the default DNS server. Configure the virtual IP (VIP) by selecting a VIP Mode. This VIP is used to access the cluster or for other nodes to join the cluster. note If using DHCP to configure the IP address, you need to configure a static MAC-to-IP address mapping on your DHCP server to have a persistent virtual IP (VIP), and the VIP must be unique. Configure the Cluster token. This token is used for adding other nodes to the cluster. Configure and confirm a Password to access the node. The default SSH user is rancher. Configure NTP servers to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org. Use commas as a delimiter to add more NTP servers. (Optional) If you need to use an HTTP proxy to access the outside world, enter the Proxy address. Otherwise, leave this blank. (Optional) You can choose to import SSH keys by providing HTTP URL. For example, your GitHub public keys https://github.com/&lt;username&gt;.keys can be used. (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Review and confirm your installation options. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, your node restarts. After the restart, the Harvester console displays the management URL and status. The default URL of the web interface is https://your-virtual-ip. You can use F12 to switch from the Harvester console to the Shell and type exit to go back to the Harvester console. note Choosing Install Harvester binaries only on the first page requires additional setup after the first bootup. You will be prompted to set the password for the default admin user when logging in for the first time. Known Issue​ Installer may crash when using an older graphics card/monitor​ In some cases, if you are using an older graphics card/monitor, you may encounter a panic: invalid dimensions error during ISO installation. We are working on this known issue and planning a fix for a future release. You can try to use another GRUB entry to force it to use the resolution of 1024x768 when booting up. If you are using a version earlier than v1.1.1, please try the following workaround: Boot up with the ISO, and press E to edit the first menu entry: Append vga=792 to the line started with $linux: Press Ctrl+X or F10 to boot up. Fail to join nodes using FQDN to a cluster which has custom SSL certificate configured​ You may encounter that newly joined nodes stay in the Not Ready state indefinitely. This is likely the outcome if you already have a set of custom SSL certificates configured on the to-be-joined Harvester cluster and provide an FQDN instead of a VIP address for the management address during the Harvester installation. You can check the SSL certificates on the Harvester dashboard's setting page or using the command line tool kubectl get settings.harvesterhci.io ssl-certificates to see if there is any custom SSL certificate configured (by default, it is empty). The second thing to look at is the joining nodes. Try to get access to the nodes via consoles or SSH sessions and then check the log of rancherd: $ journalctl -u rancherd.service Oct 06 03:36:06 node-0 systemd[1]: Starting Rancher Bootstrap... Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/50-defaults.yaml]&quot; Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/91-harvester-bootstrap-repo.yaml]&quot; Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Loading config file [/etc/rancher/rancherd/config.yaml]&quot; Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Bootstrapping Rancher (v2.7.5/v1.25.9+rke2r1)&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;Writing plan file to /var/lib/rancher/rancherd/plan/plan.json&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;Applying plan with checksum &quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231006-033608-applied.plan/_0&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;Running command: /usr/bin/env [sh /var/lib/rancher/rancherd/install.sh]&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent configuration directory /etc/rancher/agent&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent var directory /var/lib/rancher/agent&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stderr]: [WARN] /usr/local is read-only or a mount point; installing to /opt/rancher-system-agent&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Determined CA is necessary to connect to Rancher&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded CA certificate&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Value from https://harvester.192.168.48.240.sslip.io:443/cacerts is an x509 certificate&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully tested Rancher connection&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Downloading rancher-system-agent binary from https://harvester.192.168.48.240.sslip.io:443/assets/rancher-system-agent-amd64&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded the rancher-system-agent binary.&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Downloading rancher-system-agent-uninstall.sh script from https://harvester.192.168.48.240.sslip.io:443/assets/system-agent-uninstall.sh&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded the rancher-system-agent-uninstall.sh script.&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Generating Cattle ID&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded Rancher connection information&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] systemd: Creating service file&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Creating environment file /etc/systemd/system/rancher-system-agent.env&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Enabling rancher-system-agent.service&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stderr]: Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Starting/restarting rancher-system-agent.service&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231006-033608-applied.plan/_1&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;Running command: /usr/bin/rancherd [probe]&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stderr]: time=\\&quot;2023-10-06T03:36:09Z\\&quot; level=info msg=\\&quot;Running probes defined in /var/lib/rancher/rancherd/plan/plan.json\\&quot;&quot; Oct 06 03:36:10 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:10Z&quot; level=info msg=&quot;[stderr]: time=\\&quot;2023-10-06T03:36:10Z\\&quot; level=info msg=\\&quot;Probe [kubelet] is unhealthy\\&quot;&quot; The above log shows that rancherd is waiting for kubelet to become healthy. rancherd is doing nothing wrong and is working as expected. The next step is to check the rancher-system-agent: $ journalctl -u rancher-system-agent.service Oct 06 03:43:51 node-0 systemd[1]: rancher-system-agent.service: Scheduled restart job, restart counter is at 88. Oct 06 03:43:51 node-0 systemd[1]: Stopped Rancher System Agent. Oct 06 03:43:51 node-0 systemd[1]: Started Rancher System Agent. Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Rancher System Agent version v0.3.3 (9e827a5) is starting&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Using directory /var/lib/rancher/agent/work for work&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Starting remote watch of plans&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Initial connection to Kubernetes cluster failed with error Get \\&quot;https://harvester.192.168.48.240.sslip.io/version\\&quot;: x509: certificate signed by unknown authority, removing CA data and trying again&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=fatal msg=&quot;error while connecting to Kubernetes cluster with nullified CA data: Get \\&quot;https://harvester.192.168.48.240.sslip.io/version\\&quot;: x509: certificate signed by unknown authority&quot; Oct 06 03:43:51 node-0 systemd[1]: rancher-system-agent.service: Main process exited, code=exited, status=1/FAILURE Oct 06 03:43:51 node-0 systemd[1]: rancher-system-agent.service: Failed with result 'exit-code'. If you see a similar log output, you need to manually add the CA to the trust list on each joining node with the following commands: # prepare the CA as embedded-rancher-ca.pem on the nodes $ sudo cp embedded-rancher-ca.pem /etc/pki/trust/anchors/ $ sudo update-ca-certificates After adding the CA to the trust list, the nodes can join to the cluster successfully.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine","content":"Read a Virtual Machine GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Install Harvester Binaries Only","type":0,"sectionRef":"#","url":"/v1.3/install/install-binaries-mode","content":"Install Harvester Binaries Only Available as of v1.2.0 The Install Harvester binaries only mode allows you to install and configure binaries only, making it ideal for cloud and edge use cases. Background​ Currently when a new Harvester node is launched it needs to be the first node in the cluster or join an existing cluster. These two modes are useful when you already know enough about the environment to install the Harvester node. However, for use cases such as bare-metal cloud providers and the edge, these installation modes load the OS and Harvester content to the node without letting you configure the network. Moreover, the K8s and networking configuration will not be applied. If you choose Install Harvester binaries only, you will need to perform additional configuration after the first bootup: Create/Join option for HarvesterManagement network interface detailsCluster tokenNode password Then, the installer will apply the endpoint configuration and boot Harvester. No further reboots will be required. Stream disk mode​ Harvester has published a raw image artifact for pre-installed Harvester. The Harvester installer now allows streaming a pre-installed image directly to disk to support better integration with cloud providers. On Equinix Metal, you can use the following kernel arguments to use the streaming mode: ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl root=live:http://${artifactEndpoint}/harvester-v1.2.0-rootfs-amd64.squashfs harvester.install.automatic=true harvester.scheme_version=1 harvester.install.device=/dev/vda harvester.os.password=password harvester.install.raw_disk_image_path=http://${artifactEndpoint}/harvester-v1.2.0-amd64.raw harvester.install.mode=install console=tty1 harvester.install.tty=tty1 harvester.install.config_url=https://metadata.platformequinix.com/userdata harvester.install.management_interface.interfaces=&quot;name:enp1s0&quot; harvester.install.management_interface.method=dhcp harvester.install.management_interface.bond_options.mode=balance-tlb harvester.install.management_interface.bond_options.miimon=100 note When streaming to disk, it is recommended to host the raw disk artifact closer to the targets, as the raw disk artifact is nearly 16G in size.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Instance","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-instance","content":"Read a Virtual Machine Instance GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstances/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineInstance object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object activePods object property name* string conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] evacuationNodeName string fsFreezeStatus string guestOSInfo object id string kernelRelease string kernelVersion string machine string name string prettyName string version string versionId string interfaces object[] Array [ infoSource string interfaceName string ipAddress string ipAddresses string[] mac string name string ] launcherContainerImageVersion string migrationMethod string migrationState object abortRequested boolean abortStatus string completed boolean endTimestamp string failed boolean migrationConfiguration object allowAutoConverge boolean allowPostCopy boolean bandwidthPerMigration string completionTimeoutPerGiB int64 disableTLS boolean network string nodeDrainTaintKey string parallelMigrationsPerCluster int64 parallelOutboundMigrationsPerNode int64 progressTimeout int64 unsafeMigrationOverride boolean migrationPolicyName string migrationUid string mode string sourceNode string startTimestamp string targetAttachmentPodUID string targetCPUSet int32[] targetDirectMigrationNodePorts object property name* int32 targetNode string targetNodeAddress string targetNodeDomainDetected boolean targetNodeTopology string targetPod string migrationTransport string nodeName string phase string phaseTransitionTimestamps object[] Array [ phase string phaseTransitionTimestamp string ] qosClass string reason string runtimeUser int64 topologyHints object tscFrequency int64 virtualMachineRevisionName string volumeStatus object[] Array [ hotplugVolume object attachPodName string attachPodUID string memoryDumpVolume object claimName string endTimestamp string startTimestamp string targetFileName string message string name stringrequired persistentVolumeClaimInfo object accessModes string[] capacity object property name* string Default value: [object Object] filesystemOverhead string preallocated boolean requests object property name* string Default value: [object Object] volumeMode string phase string reason string size int64 target stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Net Install ISO","type":0,"sectionRef":"#","url":"/v1.3/install/net-install","content":"Net Install ISO The Harvester net install ISO is a minimal installation image that contains only the core OS components, allowing the installer to boot and then install the Harvester OS on a disk. After installation is completed, the Harvester OS pulls all required container images from the internet (mostly from Docker Hub). You can use the net install ISO in the following situations: The virtual media implementation on a server is buggy or slow. Community users have reported that ISO redirection is too slow to preload all images onto a system. For more information, see Issue 2651.You have a private registry that contains all Harvester images, as well as the knowledge and experience required to configure image mirrors for containerd. caution You must always use the full ISO to bootstrap a Harvester cluster (in other words, use the ISO without the -net-install suffix). The full ISO contains all required images, and the installer preloads those images during installation. You can easily reach the Docker Hub rate limit when using a net install ISO to bootstrap the Harvester cluster. Usage​ Download the net install ISO from the GitHub Releases page, and then boot the ISO to install Harvester. Net install ISO file names have the suffix net-install (for example, https://releases.rancher.com/harvester/v1.3.0/harvester-v1.3.0-amd64-net-install.iso). PXE Installation​ If you decide to use the net install ISO as the PXE installation source, add the following parameter when booting the kernel: harvester.install.with_net_images=true Please check PXE Boot Installation for more information.","keywords":"Harvester Net ISO Installation BMC ISO Redirection BMC Virtual Media","version":"v1.3 (latest)"},{"title":"Management Address","type":0,"sectionRef":"#","url":"/v1.3/install/management-address","content":"Management Address Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address​ To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath='{.metadata.annotations}' Example of output: {&quot;kube-vip.io/hwaddr&quot;:&quot;02:00:00:09:7f:3f&quot;,&quot;kube-vip.io/requestedIP&quot;:&quot;10.84.102.31&quot;} Usages​ The management address: Allows the access to the Harvester API/UI via HTTPS protocol.Allows other nodes to join the cluster.","keywords":"VIP","version":"v1.3 (latest)"},{"title":"Hardware and Network Requirements","type":0,"sectionRef":"#","url":"/v1.3/install/requirements","content":"Hardware and Network Requirements As an HCI solution on bare metal servers, there are minimum node hardware and network requirements for installing and running Harvester. A three-node cluster is required to fully realize the multi-node features of Harvester. The first node that is added to the cluster is by default the management node. When the cluster has three or more nodes, the two nodes added after the first are automatically promoted to management nodes to form a high availability (HA) cluster. Certain versions of Harvester support the deployment of single-node clusters. Such clusters do not support high availability, multiple replicas, and live migration. Hardware Requirements​ Harvester nodes have the following hardware requirements and recommendations for installation and testing. Type\tRequirements and RecommendationsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum for testing; 16-core or above required for production Memory\t32 GB minimum for testing; 64 GB or above required for production Disk Capacity\t250 GB minimum for testing (180 GB minimum when using multiple disks); 500 GB or above required for production Disk Performance\t5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for etcd Network Card\t1 Gbps Ethernet minimum for testing; 10Gbps Ethernet required for production Network Switch\tTrunking of ports required for VLAN support important Use server-class hardware to achieve the best results. Laptops and nested virtualization are not supported.Each node must have a unique product_uuid (fetched from /sys/class/dmi/id/product_uuid) to prevent errors from occurring during VM live migration and other operations. For more information, see Issue #4025. Network Requirements​ Harvester nodes have the following network requirements for installation. Port Requirements for Harvester Nodes​ Harvester nodes require the following port connections or inbound rules. Typically, all outbound traffic is allowed. Protocol\tPort\tSource\tDescriptionTCP\t2379\tHarvester management nodes\tEtcd client port TCP\t2381\tHarvester management nodes\tEtcd health checks TCP\t2380\tHarvester management nodes\tEtcd peer port TCP\t10010\tHarvester management and compute nodes\tContainerd TCP\t6443\tHarvester management nodes\tKubernetes API TCP\t9345\tHarvester management nodes\tKubernetes API TCP\t10252\tHarvester management nodes\tKube-controller-manager health checks TCP\t10257\tHarvester management nodes\tKube-controller-manager secure port TCP\t10251\tHarvester management nodes\tKube-scheduler health checks TCP\t10259\tHarvester management nodes\tKube-scheduler secure port TCP\t10250\tHarvester management and compute nodes\tKubelet TCP\t10256\tHarvester management and compute nodes\tKube-proxy health checks TCP\t10258\tHarvester management nodes\tCloud-controller-manager TCP\t9091\tHarvester management and compute nodes\tCanal calico-node felix TCP\t9099\tHarvester management and compute nodes\tCanal CNI health checks UDP\t8472\tHarvester management and compute nodes\tCanal CNI with VxLAN TCP\t2112\tHarvester management nodes\tKube-vip TCP\t6444\tHarvester management and compute nodes\tRKE2 agent TCP\t10246/10247/10248/10249\tHarvester management and compute nodes\tNginx worker process TCP\t8181\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t8444\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t10245\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t80\tHarvester management and compute nodes\tNginx TCP\t9796\tHarvester management and compute nodes\tNode-exporter TCP\t30000-32767\tHarvester management and compute nodes\tNodePort port range TCP\t22\tHarvester management and compute nodes\tsshd UDP\t68\tHarvester management and compute nodes\tWicked TCP\t3260\tHarvester management and compute nodes\tiscsid Port Requirements for Integrating Harvester with Rancher​ If you want to integrate Harvester with Rancher, you need to make sure that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. When provisioning VMs with Kubernetes clusters from Rancher into Harvester, you need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise, the cluster won't be manageable by Rancher. For more information, refer to Rancher Architecture. Port Requirements for K3s or RKE/RKE2 Clusters​ For the port requirements for guest clusters deployed inside Harvester VMs, refer to the following links: K3s NetworkingRKE PortsRKE2 Networking","keywords":"Installation Requirements","version":"v1.3 (latest)"},{"title":"Update Harvester Configuration After Installation","type":0,"sectionRef":"#","url":"/v1.3/install/update-harvester-configuration","content":"Update Harvester Configuration After Installation Harvester's OS has an immutable design, which means most files in the OS revert to their pre-configured state after a reboot. The Harvester OS loads the pre-configured values of system components from configuration files during the boot time. This page describes how to edit some of the most-requested Harvester configurations. To update a configuration, you must first update the runtime value in the system and then update configuration files to make the changes persistent between reboots. note If you upgrade from a version before v1.1.2, the cloud-init file in examples will be /oem/99_custom.yaml. Please substitute the value if needed. DNS servers​ Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Edit /etc/sysconfig/network/config and update the following line. Use a space to separate DNS server addresses if there are multiple servers. NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot; Update and reload the configuration with the following command: netconfig update Confirm the file /etc/resolv.conf contains the correct DNS servers with the cat command: cat /etc/resolv.conf Configuration persistence​ Beginning with v1.1.2, the persistent name of the cloud-init file is /oem/90_custom.yaml. Harvester now uses a newer version of Elemental, which creates the file during installation. When upgrading from an earlier version to v1.1.2 or later, Harvester retains the old file name (/oem/99_custom.yaml) to avoid confusion. You can manually rename the file to /oem/90_custom.yaml if necessary. Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the value under the yaml path stages.initramfs[0].commands. The commands array must contain a line to manipulate the NETCONFIG_DNS_STATIC_SERVERS config. Add the line if the line doesn't exist. The following example adds a line to change the NETCONFIG_DNS_STATIC_SERVERS config: stages: initramfs: - commands: - sed -i 's/^NETCONFIG_DNS_STATIC_SERVERS.*/NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot;/' /etc/sysconfig/network/config Replace the DNS server addresses and save the file. Harvester sets up new servers after rebooting. NTP servers​ We introduce the new mechanism for the NTP configuration in Harvester v1.2.0. For more information about NTP settings in Harvester v1.2.0 and later versions, see the NTP servers. Password of user rancher​ Runtime change​ Log in to a Harvester node as user rancher. See how to log into a Harvester node for more details.To reset the password for the user rancher, run the command passwd. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].users.rancher.passwd. Refer to the configuration os.password for details on how to specify the password in an encrypted form. Bonding slaves​ You can update the slave interfaces of Harvester's management bonding interface mgmt-bo. Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Identify the interface names with the following command: ip a Edit /etc/sysconfig/network/ifcfg-mgmt-bo and update the lines associated with bonding slaves and bonding mode: BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' Restart the network with the wicked ifreload command: wicked ifreload mgmt-bo caution A mistake in the configuration may disrupt the SSH session. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].files. More specifically, update the content of the /etc/sysconfig/network/ifcfg-mgmt-bo file and edit the BONDING_SLAVE_X and BONDING_MODULE_OPTS entries accordingly: stages: initramfs: - ... files: - path: /etc/sysconfig/network/ifcfg-mgmt-bo permissions: 384 owner: 0 group: 0 content: |+ STARTMODE='onboot' BONDING_MASTER='yes' BOOTPROTO='none' POST_UP_SCRIPT=&quot;wicked:setup_bond.sh&quot; BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' DHCLIENT_SET_DEFAULT_ROUTE='no' encoding: &quot;&quot; ownerstring: &quot;&quot; - path: /etc/sysconfig/network/ifcfg-ens6 permissions: 384 owner: 0 group: 0 content: | STARTMODE='hotplug' BOOTPROTO='none' encoding: &quot;&quot; ownerstring: &quot;&quot; note If you didn't select an interface during installation, you must add an entry to initialize the interface. Please check the /etc/sysconfig/network/ifcfg-ens6 file creation in the above example. The file name should be /etc/sysconfig/network/ifcfg-&lt;interface-name&gt;.","keywords":"Harvester configuration Configuration","version":"v1.3 (latest)"},{"title":"PXE Boot Installation","type":0,"sectionRef":"#","url":"/v1.3/install/pxe-boot-install","content":"PXE Boot Installation Starting from version 0.2.0, Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples. Prerequisite​ Nodes need to have at least 8 GiB of RAM because the installer loads the full ISO file into tmpfs. info The installer automatically checks if the hardware meets the minimum requirements for production use. If any of the checks fail, installation will be stopped. To override this behavior, set either the configuration file option install.skipchecks=true or the kernel parameter harvester.install.skipchecks=true. Preparing HTTP Servers​ An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10, and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/. Preparing Boot Files​ Download the required files from the Harvester releases page. The ISO: harvester-&lt;version&gt;-amd64.isoThe kernel: harvester-&lt;version&gt;-vmlinuz-amd64The initrd: harvester-&lt;version&gt;-initrd-amd64The rootfs squashfs image: harvester-&lt;version&gt;-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts​ When performing an automatic installation, there are two modes: CREATE: we are installing a node to construct an initial Harvester cluster.JOIN: we are installing a node to join an existing Harvester cluster. Harvester v1.3.0 introduces roles that you can assign to nodes to support different scenarios. For more information, see Harvester Configuration. CREATE Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml scheme_version: 1 token: token # Replace with a desired token os: hostname: node1 # Set a hostname. This can be omitted if DHCP server offers hostnames ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install: mode: create management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console vip: 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use vip_mode: static # Or dhcp, check configuration file for more information # vip_hw_addr: 52:54:00:ec:0e:0b # Leave empty when vip_mode is static For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create. note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp). See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter. JOIN Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml scheme_version: 1 server_url: https://10.100.0.99:443 # Should be the VIP set up in &quot;CREATE&quot; config token: token os: hostname: node2 ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password dns_nameservers: - 1.1.1.1 - 8.8.8.8 install: mode: join management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join. DHCP Server Configuration​ note In the PXE installation scenario, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. In the ISO installation scenario, when the management network interface is in DHCP mode, you are also required to add the routers option (option routers) when configuring the DHCP server. For example: Harvester Host:~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see ISC DHCPv4 Option Configuration. The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16; subnet 10.100.0.0 netmask 255.255.255.0 { option routers 10.100.0.10; option domain-name-servers 192.168.2.1; range 10.100.0.100 10.100.0.253; } group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } group { # join group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-join-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-join&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node2 { hardware ethernet 52:54:00:69:d5:92; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration​ For more information about Harvester configuration, please refer to the Harvester configuration page. By default, the first node will be the management node of the cluster. When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config. In this case, at least three different zones are required. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support​ UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program​ Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration​ If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } elsif substring (option vendor-class-identifier, 0, 10) = &quot;HTTPClient&quot; { # UEFI HTTP Boot option vendor-class-identifier &quot;HTTPClient&quot;; filename &quot;http://10.100.0.10/harvester/ipxe.efi&quot;; } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi. The iPXE Script for UEFI Boot​ It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-&lt;version&gt;-vmlinuz initrd=harvester-&lt;version&gt;-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot The parameter initrd=harvester-&lt;version&gt;-initrd is required. Useful Kernel Parameters​ Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7). ip=dhcp​ If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces. rd.net.dhcp.retry=&lt;cnt&gt;​ Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=&lt;cnt&gt;to retry DHCP request for &lt;cnt&gt; times. harvester.install.skipchecks=true​ Installation is stopped if the hardware checks fail (because the minimum requirements for production use are not met). To override this behavior, set the kernel parameter harvester.install.skipchecks=true. When set to true, warning messages are still saved to /var/log/console.log, but the installation proceeds even if hardware requirements for production use are not met. harvester.install.with_net_images=true​ The installer does not preload images during installation and instead pulls all required images from the internet after installation is completed. Usage of this parameter is not recommended in most cases. For more information, see Net Install ISO.","keywords":"Harvester harvester Rancher rancher Install Harvester Installing Harvester Harvester Installation PXE Boot Install","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-template-version","content":"Read a Virtual Machine Template Version GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"USB Installation","type":0,"sectionRef":"#","url":"/v1.3/install/usb-install","content":"USB Installation Create a bootable USB flash drive​ There are a couple of ways to create a USB installation flash drive. caution Known Issue: For the v1.2.0 ISO image, there is a known issue where the interactive ISO installation will get stuck using the USB method. To resolve this, you can use the patched ISO. This patched version only corrects the partition label, and there are no other changes. You can also use the related sha512 file to verify the ISO. Refer to the Harvester interactive ISO hangs with the USB installation method for details and a workaround. No matter which tool you use, creating a bootable device erases your USB device data. Please back up all data on your USB device before making a bootable device. Rufus​ Rufus allows you to create an ISO image on your USB flash drive on a Windows computer. Open Rufus and insert a clean USB stick into your computer. Rufus automatically detects your USB. Select the USB device you want to use from the Device drop-down menu. For Boot Selection, choose Select and find the Harvester installation ISO image you want to burn onto the USB. info If using older versions of Rufus, both DD mode and ISO mode works. DD mode works just like the dd command in Linux, and you can't browse partitions after you create a bootable device. ISO mode creates partitions on your device automatically and copies files to these partitions, and you can browse these partitions even after you create a bootable device. balenaEtcher​ balenaEtcher supports writing an image to a USB flash drive on most Linux distros, macOS, and Windows. It has a GUI and is easy to use. Select the Harvester installation ISO. Select the target USB device to create a USB installation flash drive. dd command​ You can use the 'dd' command on Linux or other platforms to create a USB installation flash drive. Ensure you choose the correct device; the following command erases data on the selected device. # sudo dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb_device&gt; bs=64k Known issues​ A GRUB _ text is displayed, but nothing happens when booting from a USB installation flash drive​ If you use the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. For example, select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. The representation varies from system to system. Graphics issue​ Firmwares of some graphic cards are not shipped in v0.3.0. You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Harvester installer is not displayed​ If a USB flash driver boots, but you can't see the harvester installer, try one of the following workarounds: Plug the USB flash drive into a USB 2.0 slot.For version v0.3.0 or above, remove the console=ttyS0 parameter when booting. Press e to edit the GRUB menu entry and remove the console=ttyS0 parameter. Harvester interactive ISO hangs with the USB installation method​ During installation from a USB flash drive with v1.2.0 ISO image (created by tools like balenaEtcher, dd, etc.), the installation process may get stuck on the initial image loading process because a required label is missing on the boot partition. Therefore, the installation cannot mount the data partition correctly, causing some checks in dracut to be blocked. If you encounter this issue, you'll observe the following similar output, and the process will hang for at least 50 minutes (the default timeout value from dracut). Workaround​ To address this problem, you can manually modify the root partition as follows: # Replace the `CDLABEL=COS_LIVE` with your USB data partition. Usually, your USB data partition is the first partition with the device name `sdx` that hangs on your screen. # Original $linux ($root)/boot/kernel cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable net.ifnames=1 # Modified $linux ($root)/boot/kernel cdroot root=live:/dev/sda1 rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable net.ifnames=1 The modified parameter should look like the following: After making this adjustment, press Ctrl + x to initiate booting. You should now enter the installer as usual. Related issue: [BUG] v1.2.0 Interactive ISO Fails to Install On Some Bare-Metal Devices","keywords":"","version":"v1.3 (latest)"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.3/monitoring/harvester-monitoring","content":"Monitoring Available as of v1.2.0 The monitoring feature is now implemented with an addon and is disabled by default in new installations. Users can enable/disable rancher-monitoring addon from the Harvester WebUI after installation. Users can also enable/disable the rancher-monitoring addon in their Harvester installation by customizing the harvester-configuration file. For Harvester clusters upgraded from version v1.1.x, the monitoring feature is converted to an addon automatically and kept enabled as before. Dashboard Metrics​ Harvester has provided a built-in monitoring integration using Prometheus. Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. note Only admin users are able to view the cluster dashboard metrics. Additionally, Grafana is provided by rancher-monitoring, so the default admin password is: prom-operator Reference: values.yaml VM Detail Metrics​ For VMs, you can view VM metrics by clicking on the VM details page &gt; VM Metrics. note The current Memory Usage is calculated based on (1 - free/total) * 100%, not (used/total) * 100%. For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100%, roughly 40%. How to Configure Monitoring Settings​ Available as of v1.0.2 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: PrometheusPrometheus Node Exporter From UI​ On the Advanced page, you can view and change the resource settings as follows: Go to the Advanced &gt; Addons page and select the rancher-monitoring page.From the Prometheus tab, change the resource requests and limits.Select Save when finished configuring the settings for the rancher-monitoring addon. The Monitoring deployments restart within a few seconds. Please be aware that the reboot can take time to reload previous data. note The UI configuration is only visible when the rancher-monitoring addon is enabled. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. caution When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM (out of memory). In that case, you should increase the value of limits.memory. From CLI​ You can use the following kubectl command to change resource configurations for the rancher-monitoring addon: kubectl edit addons.harvesterhci.io -n cattle-monitoring-system rancher-monitoring. The resource path and default values are as follows: apiVersion: harvesterhci.io/v1beta1 kind: Addon metadata: name: rancher-monitoring namespace: cattle-monitoring-system spec: valuesContent: | prometheus: prometheusSpec: resources: limits: cpu: 1000m memory: 2500Mi requests: cpu: 850m memory: 1750Mi note You can still make configuration adjustments when the addon is disabled. However, these changes only take effect when you re-enable the addon. Alertmanager​ Harvester uses Alertmanager to collect and manage all the alerts that happened/happening in the cluster. Alertmanager Config​ Enable/Disable Alertmanager​ Alertmanager is enabled by default. You may disable it from the following config path. Change Resource Setting​ You can also change the resource settings of Alertmanager as shown in the picture above. Configure AlertmanagerConfig from WebUI​ To send the alerts to third-party servers, you need to config AlertmanagerConfig. On the WebUI, navigate to Monitoring &amp; Logging -&gt; Monitoring -&gt; Alertmanager Configs. On the Alertmanager Config: Create page, click Namespace to select the target namespace from the drop-down list and set the Name. After this, click Create in the lower right corner. Click the Alertmanager Configs you just created to continue the configuration. Click Add Receiver. Set the Name for the receiver. After this, select the receiver type, for example, Webhook, and click Add Webhook. Fill in the required parameters and click Create. Configure AlertmanagerConfig from CLI​ You can also add AlertmanagerConfig from the CLI. Exampe: a Webhook receiver in the default namespace. cat &lt;&lt; EOF &gt; a-single-receiver.yaml apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: amc-example # namespace: your value labels: alertmanagerConfig: example spec: route: continue: true groupBy: - cluster - alertname receiver: &quot;amc-webhook-receiver&quot; receivers: - name: &quot;amc-webhook-receiver&quot; webhookConfigs: - sendResolved: true url: &quot;http://192.168.122.159:8090/&quot; EOF # kubectl apply -f a-single-receiver.yaml alertmanagerconfig.monitoring.coreos.com/amc-example created # kubectl get alertmanagerconfig -A NAMESPACE NAME AGE default amc-example 27s Example of an Alert Received by Webhook​ Alerts sent to the webhook server will be in the following format: { 'receiver': 'longhorn-system-amc-example-amc-webhook-receiver', 'status': 'firing', 'alerts': [], 'groupLabels': {}, 'commonLabels': {'alertname': 'LonghornVolumeStatusWarning', 'container': 'longhorn-manager', 'endpoint': 'manager', 'instance': '10.52.0.83:9500', 'issue': 'Longhorn volume is Degraded.', 'job': 'longhorn-backend', 'namespace': 'longhorn-system', 'node': 'harv2', 'pod': 'longhorn-manager-r5bgm', 'prometheus': 'cattle-monitoring-system/rancher-monitoring-prometheus', 'service': 'longhorn-backend', 'severity': 'warning'}, 'commonAnnotations': {'description': 'Longhorn volume is Degraded for more than 5 minutes.', 'runbook_url': 'https://longhorn.io/docs/1.3.0/monitoring/metrics/', 'summary': 'Longhorn volume is Degraded'}, 'externalURL': 'https://192.168.122.200/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy', 'version': '4', 'groupKey': '{}/{namespace=&quot;longhorn-system&quot;}:{}', 'truncatedAlerts': 0 } note Different receivers may present the alerts in different formats. For details, please refer to the related documents. Known Limitation​ The AlertmanagerConfig is enforced by the namespace. Gloabl-level AlertmanagerConfig without a namespace is not supported. We have already created a GithHb issue to track upstream changes. Once the feature is available, Harvester will adopt it. View and Manage Alerts​ From Alertmanager Dashboard​ You can visit the original dashboard of Alertmanager from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy/#/alerts The overall view of the Alertmanager dashboard is as follows. You can view the details of an alert: From Prometheus Dashboard​ You can visit the original dashboard of Prometheus from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-prometheus:9090/proxy/ The Alerts menu in the top navigation bar shows all defined rules in Prometheus. You can use the filters Inactive, Pending, and Firing to quickly find the information that you need. Troubleshooting​ For Monitoring support and troubleshooting, please refer to the troubleshooting page .","keywords":"","version":"v1.3 (latest)"},{"title":"Harvester Network Deep Dive","type":0,"sectionRef":"#","url":"/v1.3/networking/deep-dive","content":"Harvester Network Deep Dive The network topology below reveals how we implement the Harvester network. The diagram contains the built-in cluster network mgmt and a custom cluster network called oob. As shown above, the Harvester network primarily focuses on OSI model layer 2. We leverage Linux network devices and protocols to construct traffic paths for the communication between VM to VM, VM to host, and VM to external network devices. The Harvester network is composed of three layers, including: KubeVirt networking layer Harvester networking layer External networking layer KubeVirt Networking​ The general purpose of KubeVirt is to run VM inside the Kubernetes pod. The KubeVirt network builds the network path between the pod and VM. Please refer to the KubeVirt official document for more details. Harvester Networking​ Harvester networking is designed to build the network path between pods and the host network. It implements a management network, VLAN networks and untagged networks. We can refer to the last two networks as bridge networks, because bridge plays a vital role in their implementation. Bridge Network​ We leverage multus CNI and bridge CNI to implement the bridge network. Multus CNI is a Container Network Interface (CNI) plugin for Kubernetes that can attach multiple network interfaces to a pod. Its capability allows a VM to have one NIC for the management network and multiple NICs for the bridge network. Using the bridge CNI, the VM pod will be plugged into the L2 bridge specified in the Network Attachment Definition config. # Example 1 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;vlan100&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;mgmt-br&quot;, &quot;promiscMode&quot;: true, &quot;vlan&quot;: 100, } # Example 2 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;untagged-network&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;oob-br&quot;, &quot;promiscMode&quot;: true, &quot;ipam&quot;: {} } Example 1 is a typical VLAN configuration with VLAN ID 100, while Example 2 is an untagged network configuration with no VLAN ID. The VM pod configured using Example 1 will be plugged into the bridge mgmt-br, while the VM pod using Example 2 will be plugged into the bridge oob-br. To achieve high availability and fault tolerance, a bond device where the real NICs are bound is created to serve as the uplink of the bridge. By default, this bond device will allow the target tagged traffic/packets to pass through. harvester-0:/home/rancher # bridge -c vlan show dev oob-bo port vlan ids oob-bo 1 PVID Egress Untagged 100 200 The example above shows that the bond oob-bo allows packages with tag 1, 100 or 200. Management Network​ The management network is based on Canal. It is worth mentioning that the Canal interface where the Harvester configures the node IP is the bridge mgmt-br or a VLAN sub-interface of mgmt-br. This design has two benefits: The built-in mgmt cluster network supports both the management network and bridge network.With the VLAN network interface, we can assign a VLAN ID to the management network. As components of the mgmt cluster network, it's not allowed to delete or modify the bridge mgmt-br, the bond mgmt-bo and the VLAN device. External Networking​ External network devices typically refer to switches and DHCP servers. With a cluster network, we can group host NICs and connect them to different switches for traffic isolation. Below are some usage instructions. To allow tagged packets to pass, you need to set the port type of the external switch or other devices (such as a DHCP server) to trunk or hybrid mode and allow the specified VLAN tag. You need to configure link aggregation on the switch based on the bond mode of the peer host. Link aggregation can work in manual mode or LACP mode. The following lists the correspondence between bond mode and link aggregation mode. Bond Mode\tLink Aggregation Modemode 0(balance-rr)\tmanual mode 1(active-backup)\tnone mdoe 2(balance-oxr)\tmanual mode 3(broadcast)\tmanual mode 4(802.3ad)\tLACP mode 5(balance-tlb)\tnone mode 6(balance-alb)\tnone If you want VMs in a VLAN to be able to obtain IP addresses through the DHCP protocol, configure an IP pool for that VLAN in the DHCP server.","keywords":"Harvester Networking Topology","version":"v1.3 (latest)"},{"title":"VM Network","type":0,"sectionRef":"#","url":"/v1.3/networking/harvester-network","content":"VM Network Harvester provides three types of networks for virtual machines (VMs), including: Management NetworkVLAN NetworkUntagged Network The management network is usually used for VMs whose traffic only flows inside the cluster. If your VMs need to connect to the external network, use the VLAN network or untagged network. Available as of v1.0.1 Harvester also introduced storage networking to separate the storage traffic from other cluster-wide workloads. Please refer to the storage network document for more details. Management Network​ Harvester uses Canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, you can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. How to use management network​ Since the management network is built-in and doesn't require extra operations, you can add it directly when configuring the VM network. important Network interfaces of VMs connected to the management network have an MTU value of 1450. This is because a VXLAN overlay network typically has a slightly higher per-packet overhead. If any of your workloads involve transmission of network traffic, you must specify the appropriate MTU value for the affected VM network interfaces and bridges. VLAN Network​ The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. Create a VM Network​ Go to Networks &gt; VM Networks. Select Create. Configure the following settings: Namespace Name Description (optional) On the Basics tab, configure the following settings: Type: Select L2VlanNetwork.Vlan ID Cluster Network On the Route tab, select an option and then specify the related IPv4 addresses. Auto(DHCP): The Harvester network controller retrieves the CIDR and gateway addresses from the DHCP server. You can specify the DHCP server address. Manual: Specify the CIDR and gateway addresses. important Harvester uses the information to verify that all nodes can access the VM network you are creating. If that is the case, the Network connectivity column on the VM Networks screen indicates that the network is active. Otherwise, the screen indicates that an error has occurred. Create a VM with VLAN Network​ You can now create a new VM using the VLAN network configured above: Click the Create button on the Virtual Machines page.Specify the required parameters and click the Networks tab.Either configure the default network to be a VLAN network or select an additional network to add. Untagged Network​ As is known, the traffic under a VLAN network has a VLAN ID tag and we can use the VLAN network with PVID (default 1) to communicate with any normal untagged traffic. However, some network devices may not expect to receive an explicitly tagged VLAN ID that matches the native VLAN on the switch the uplink belongs to. That's the reason why we provide the untagged network. How to use untagged network​ The usage of untagged network is similar to the VLAN network. To create a new untagged network, go to the Networks &gt; VM Networks page and click the Create button. You have to specify the name, select the type Untagged Network and choose the cluster network. note Starting from Harvester v1.1.2, Harvester supports updating and deleting VM networks. Make sure to stop all affected VMs before updating or deleting VM networks.","keywords":"Harvester Network","version":"v1.3 (latest)"},{"title":"Read a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.3/api/read-namespaced-virtual-machine-backup","content":"Read a Virtual Machine Backup GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Cluster Network","type":0,"sectionRef":"#","url":"/v1.3/networking/index","content":"Cluster Network Concepts​ Cluster Network​ Available as of v1.1.0 In Harvester v1.1.0, we introduced a new concept called cluster network for traffic isolation. The following diagram describes a typical network architecture that separates data-center (DC) traffic from out-of-band (OOB) traffic. We abstract the sum of devices, links, and configurations on a traffic-isolated forwarding path on Harvester as a cluster network. In the above case, there will be two cluster networks corresponding to two traffic-isolated forwarding paths. Network Configuration​ Specifications including network devices of the Harvester hosts can be different. To be compatible with such a heterogeneous cluster, we designed the network configuration. Network configuration only works under a certain cluster network. Each network configuration corresponds to a set of hosts with uniform network specifications. Therefore, multiple network configurations are required for a cluster network on non-uniform hosts. VM Network​ A VM network is an interface in a virtual machine that connects to the host network. As with a network configuration, every network except the built-in management network must be under a cluster network. Harvester supports adding multiple networks to one VM. If a network's cluster network is not enabled on some hosts, the VM that owns this network will not be scheduled to those hosts. Please refer to network part for more details about networks. Relationship Between Cluster Network, Network Config, VM Network​ The following diagram shows the relationship between a cluster network, a network config, and a VM network. All Network Configs and VM Networks are grouped under a cluster network. A label can be assigned to each host to categorize hosts based on their network specifications. A network config can be added for each group of hosts using a node selector. For example, in the diagram above, the hosts in ClusterNetwork-A are divided into three groups as follows: The first group includes host0, which corresponds to network-config-A.The second group includes host1 and host2, which correspond to network-config-B.The third group includes the remaining hosts (host3, host4, and host5), which do not have any related network config and therefore do not belong to ClusterNetwork-A. The cluster network is only effective on hosts that are covered by the network configuration. A VM using a VM network under a specific cluster network can only be scheduled on a host where the cluster network is active. In the diagram above, we can see that: ClusterNetwork-A is active on host0, host1, and host2. VM0 uses VM-network-A, so it can be scheduled on any of these hosts.VM1 uses both VM-network-B and VM-network-C, so it can only be scheduled on host2 where both ClusterNetwork-A and ClusterNetwork-B are active.VM0, VM1, and VM2 cannot run on host3 where the two cluster networks are inactive. Overall, this diagram provides a clear visualization of the relationship between cluster networks, network configurations, and VM networks, as well as how they impact VM scheduling on hosts. Cluster Network Details​ Built-in Cluster Network​ Harvester provides a built-in cluster network called mgmt. It's different from the custom cluster network. The mgmt cluster network: Cannot be deleted.Does not need any network configuration.Is enabled on all hosts and cannot be disabled.Shares the same traffic egress with the management network. If there is no need for traffic separation, you can put all your network under the mgmt cluster network. Custom Cluster Network​ You are allowed to add the custom cluster network, which will not be available until it's enabled on some hosts by adding a network configuration. How to create a new cluster network​ To create a cluster network, go to the Networks &gt; ClusterNetworks/Configs page and click the Create button. You only need to specify the name. Click the Create Network Config button on the right of the cluster network to create a new network configuration. In the Node Selector tab, specify the name and choose one of the three methods to select nodes where the network configuration will apply. If you want to cover the unselected nodes, you can create another network configuration. Click the Uplink tab to add the NICs, and configure the bond options and link attributes. The bond mode defaults to active-backup. note The NICs drop-down list shows all the common NICs on all the selected nodes. The drop-down list will change as you select different nodes.The text enp7s3 (1/3 Down) in the NICs drop-down list indicates that the enp7s3 NIC is down in one of the three selected nodes. In this case, you need to find the NIC, set it up, and refresh this page. After this, it should be selectable. note Starting with Harvester v1.1.2, Harvester supports updating network configs. Make sure to stop all affected VMs before updating network configs.","keywords":"Harvester Networking ClusterNetwork NetworkConfig Network","version":"v1.3 (latest)"},{"title":"Load Balancer","type":0,"sectionRef":"#","url":"/v1.3/networking/loadbalancer","content":"Load Balancer Available as of v1.2.0 The Harvester load balancer (LB) is a built-in Layer 4 load balancer that distributes incoming traffic across workloads deployed on Harvester virtual machines (VMs) or guest Kubernetes clusters. VM load balancer​ Features​ Harvester VM load balancer supports the following features: Address assignment: Get the LB IP address from a DHCP server or a pre-defined IP pool.Protocol support: Supports both TCP and UDP protocols for load balancing.Multiple listeners: Create multiple listeners to handle incoming traffic on different ports or with other protocols.Label selector: The LB uses label selectors to match the backend servers. Therefore, you must configure the corresponding labels for the backend VMs you want to add to the LB.Health check: Only send traffic to healthy backend instances. Limitations​ Harvester VM load balancer has the following limitations: Namespace restriction: This restriction facilitates permission management and ensures the LB only uses VMs in the same namespace as the backend servers.IPv4-only: The LB is only compatible with IPv4 addresses for VMs.Guest agent installation: Installing the guest agent on each backend VM is required to obtain IP addresses. Connectivity Requirement: Network connectivity must be established between backend VMs and Harvester hosts. When a VM has multiple IP addresses, the LB will select the first one as the backend address.Access Restriction: The VM LB address is exposed only within the same network as the Harvester hosts. To access the LB from outside the network, you must provide a route from outside to the LB address. note Harvester VM load balancer doesn't support Windows VMs because the guest agent is not available for Windows VMs. How to create​ To create a new Harvester VM load balancer: Go to the Networks &gt; Load Balancers page and select Create.Select the Namespace and specify the Name.Go to the Basic tab to choose the IPAM mode, which can be DHCP or IP Pool. If you select IP Pool, prepare an IP pool first, specify the IP pool name, or choose auto. If you choose auto, the LB automatically selects an IP pool according to the IP pool selection policy.Go to the Listeners tab to add listeners. You must specify the Port, Protocol, and Backend Port for each listener.Go to the Backend Server Selector tab to add label selectors. To add the VM to the LB, go to the Virtual Machine &gt; Instance Labels tab to add the corresponding labels to the VM.Go to the Health Check tab to enable health check and specify the parameters, including the Port, Success Threshold, Failure Threshold, Interval, and Timeout if the backend service supports health check. Refer to Health Checks for more details. Health Checks​ The Harvester load balancer supports TCP health checks. You can specify the parameters in the Harvester UI if you've enabled the Health Check option. Name\tValue Type\tRequired\tDefault\tDescriptionHealth Check Port\tint\ttrue\tN/A\tSpecifies the port. The prober will access the address composed of the backend server IP and the port. Health Check Success Threshold\tint\tfalse\t1\tSpecifies the health check success threshold. Disabled by default. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. Health Check Failure Threshold\tint\tfalse\t3\tSpecifies the health check failure threshold. Disabled by default. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. Health Check Period\tint\tfalse\t5\tSpecifies the health check period in seconds. Disabled by default. Health Check Timeout\tint\tfalse\t3\tSpecifies the timeout of every health check in seconds. Disabled by default. Guest Kubernetes cluster load balancer​ In conjunction with Harvester Cloud Provider, the Harvester load balancer provides load balancing for LB services in the guest cluster.When you create, update, or delete an LB service on a guest cluster with Harvester Cloud Provider, the Harvester Cloud Provider will create a Harvester LB automatically. For more details, refer to Harvester Cloud Provider.","keywords":"Load Balancer","version":"v1.3 (latest)"},{"title":"Logging","type":0,"sectionRef":"#","url":"/v1.3/logging/harvester-logging","content":"Logging Available as of v1.2.0 It is important to know what is happening/has happened in the Harvester Cluster. Harvester collects the cluster running log, kubernetes audit and event log right after the cluster is powered on, which is helpful for monitoring, logging, auditing and troubleshooting. Harvester supports sending those logs to various types of log servers. note The size of logging data is related to the cluster scale, workload and other factors. Harvester does not use persistent storage to store log data inside the cluster. Users need to set up a log server to receive logs accordingly. The logging feature is now implemented with an addon and is disabled by default in new installations. Users can enable/disable the rancher-logging addon from the Harvester UI after installation. Users can also enable/disable the rancher-logging addon in their Harvester installation by customizing the harvester-configuration file. For Harvester clusters upgraded from version v1.1.x, the logging feature is converted to an addon automatically and kept enabled as before. High-level Architecture​ The Banzai Cloud Logging operator now powers both Harvester and Rancher as an in-house logging solution. In Harvester's practice, the Logging, Audit and Event shares one architecture, the Logging is the infrastructure, while the Audit and Event are on top of it. Logging​ The Harvester logging infrastructure allows you to aggregate Harvester logs into an external service such as Graylog, Elasticsearch, Splunk, Grafana Loki and others. Collected Logs​ See below for a list logs that are collected: Logs from all cluster PodsKernel logs from each nodeLogs from select systemd services from each node rke2-serverrke2-agentrancherdrancher-system-agentwickediscsid note Users are able to configure and modify where the aggregated logs are sent, as well as some basic filtering. It is not supported to change which logs are collected. Configuring Log Resources​ Underneath Banzai Cloud's logging operator are fluentd and fluent-bit, which handle the log routing and collecting respectively. If desired, you can modify how many resources are dedicated to those components. From UI​ Go to the Advanced &gt; Addons page and select the rancher-logging addon.From the Fluentbit tab, change the resource requests and limits.From the Fluentd tab, change the resource requests and limits.Select Save when finished configuring the settings for the rancher-logging addon. note The UI configuration is only visible when the rancher-logging addon is enabled. From CLI​ You can use the following kubectl command to change resource configurations for the rancher-logging addon: kubectl edit addons.harvesterhci.io -n cattle-logging-system rancher-logging. The resource path and default values are as follows. apiVersion: harvesterhci.io/v1beta1 kind: Addon metadata: name: rancher-logging namespace: cattle-logging-system spec: valuesContent: | fluentbit: resources: limits: cpu: 200m memory: 200Mi requests: cpu: 50m memory: 50Mi fluentd: resources: limits: cpu: 1000m memory: 800Mi requests: cpu: 100m memory: 200Mi note You can still make configuration adjustments when the addon is disabled. However, these changes only take effect when you re-enable the addon. Configuring Log Destinations​ Logging is backed by the Banzai Cloud Logging Operator, and so is controlled by Flows/ClusterFlows and Outputs/ClusterOutputs. You can route and filter logs as you like by applying these CRDs to the Harvester cluster. When applying new Ouptuts and Flows to the cluster, it can take some time for the logging operator to effectively apply them. So please allow a few minutes for the logs to start flowing. Clustered vs Namespaced​ One important thing to understand when routing logs is the difference between ClusterFlow vs Flow and ClusterOutput vs Output. The main difference between the clustered and non-clustered version of each is that the non-clustered versions are namespaced. The biggest implication of this is that Flows can only access Outputs that are within the same namespace, but can still access any ClusterOutput. For more information, see the documentation: Flows/ClusterFlowsOutputs/ClusterOutputs From UI​ note UI images are for Output and Flow whose configuration process is almost identical to their clustered counterparts. Any differences will be noted in the steps below. Creating Outputs​ Choose the option to create a new Output or ClusterOutput.If creating an Output, select the desired namespace.Add a name for the resources.Select the logging type.Select the logging output type. Configure the output buffer if necessary. Add any labels or annotations. Once done, click Create on the lower right. note Depending on the output selected (Splunk, Elasticsearch, etc), there will be additional fields to specify in the form. Output​ The fields present in the Output form will change depending on the Output chosen, in order to expose the fields present for each output plugin. Output Buffer​ The Output Buffer editor allows you to describe how you want the output buffer to behave. You can find the documentation for the buffer fields here. Labels &amp; Annotations​ You can append labels and annotations to the created resource. Creating Flows​ Choose the option to create a new Flow or ClusterFlow.If creating a Flow, select the desired namespace.Add a name for the resource.Select any nodes whose logs to include or exclude. Select target Outputs and ClusterOutputs. Add any filters if desired. Once done, click Create on the lower left. Matches​ Matches allow you to filter which logs you want to include in the Flow. The form only allows you to include or exclude node logs, but if needed, you can add other match rules supported by the resource by selecting Edit as YAML. For more information about the match directive, see Routing your logs with match directive. Outputs​ Outputs allow you to select one or more OutputRefs to send the aggregated logs to. When creating or editing a Flow / ClusterFlow, it is required that the user selects at least one Output. note There must be at least one existing ClusterOutput or Output that can be attached to the flow, or you will not be able to create / edit the flow. Filters​ Filters allow you to transform, process, and mutate the logs. In the text edit, you will find descriptions of the supported filters, but for more information, you can visit the list of supported filters. From CLI​ To configure log routes via the command line, you only need to define the YAML files for the relevant resources: # elasticsearch-logging.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: elasticsearch-example namespace: fleet-local labels: example-label: elasticsearch-example annotations: example-annotation: elasticsearch-example spec: elasticsearch: host: &lt;url-to-elasticsearch-server&gt; port: 9200 --- apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: elasticsearch-example namespace: fleet-local spec: match: - select: {} globalOutputRefs: - elasticsearch-example And then apply them: kubectl apply -f elasticsearch-logging.yaml Referencing Secrets​ There are 3 ways Banzai Cloud allows specifying secret values via yaml values. The simplest is to use the value key, which is a simple string value for the desired secret. This method should only be used for testing and never in production: aws_key_id: value: &quot;secretvalue&quot; The next is to use valueFrom, which allows referencing a specific value from a secret by a name and key pair: aws_key_id: valueFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; Some plugins require a file to read from rather than simply receiving a value from the secret (this is often the case for CA cert files). In these cases, you need to use mountFrom, which will mount the secret as a file to the underlying fluentd deployment and point the plugin to the file. The valueFrom and mountFrom object look the same: tls_cert_path: mountFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; For more information, you can find the related documentation here. Example Outputs​ Elasticsearch​ For the simplest deployment, you can deploy Elasticsearch on your local system using docker: docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled=false -e node.name=es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23 Make sure that you have set vm.max_map_count to be &gt;= 262144 or the docker command above will fail. Once the Elasticsearch server is up, you can create the yaml file for the ClusterOutput and ClusterFlow: cat &lt;&lt; EOF &gt; elasticsearch-example.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: elasticsearch-example namespace: cattle-logging-system spec: elasticsearch: host: 192.168.0.119 port: 9200 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: elasticsearch-example namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - elasticsearch-example EOF And apply the file: kubectl apply -f elasticsearch-example.yaml After allowing some time for the logging operator to apply the resources, you can test that the logs are flowing: $ curl localhost:9200/fluentd/_search { &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 11603, &quot;max_score&quot;: 1, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;fluentd&quot;, &quot;_type&quot;: &quot;fluentd&quot;, &quot;_id&quot;: &quot;yWHr0oMBXcBggZRJgagY&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;stream&quot;: &quot;stderr&quot;, &quot;logtag&quot;: &quot;F&quot;, &quot;message&quot;: &quot;I1013 02:29:43.020384 1 csi_handler.go:248] Attaching \\&quot;csi-974b4a6d2598d8a7a37b06d06557c428628875e077dabf8f32a6f3aa2750961d\\&quot;&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;csi-attacher-5d4cc8cfc8-hd4nb&quot;, &quot;namespace_name&quot;: &quot;longhorn-system&quot;, &quot;pod_id&quot;: &quot;c63c2014-9556-40ce-a8e1-22c55de12e70&quot;, &quot;labels&quot;: { &quot;app&quot;: &quot;csi-attacher&quot;, &quot;pod-template-hash&quot;: &quot;5d4cc8cfc8&quot; }, &quot;annotations&quot;: { &quot;cni.projectcalico.org/containerID&quot;: &quot;857df09c8ede7b8dee786a8c8788e8465cca58f0b4d973c448ed25bef62660cf&quot;, &quot;cni.projectcalico.org/podIP&quot;: &quot;10.52.0.15/32&quot;, &quot;cni.projectcalico.org/podIPs&quot;: &quot;10.52.0.15/32&quot;, &quot;k8s.v1.cni.cncf.io/network-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;k8s.v1.cni.cncf.io/networks-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;kubernetes.io/psp&quot;: &quot;global-unrestricted-psp&quot; }, &quot;host&quot;: &quot;harvester-node-0&quot;, &quot;container_name&quot;: &quot;csi-attacher&quot;, &quot;docker_id&quot;: &quot;f10e4449492d4191376d3e84e39742bf077ff696acbb1e5f87c9cfbab434edae&quot;, &quot;container_hash&quot;: &quot;sha256:03e115718d258479ce19feeb9635215f98e5ad1475667b4395b79e68caf129a6&quot;, &quot;container_image&quot;: &quot;docker.io/longhornio/csi-attacher:v3.4.0&quot; } } }, ... ] } } Graylog​ You can follow the instructions here to deploy and view cluster logs via Graylog: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;all-logs-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;example-gelf-hs&quot; --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;example-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: gelf: host: &quot;192.168.122.159&quot; port: 12202 protocol: &quot;udp&quot; Splunk​ You can follow the instructions here to deploy and view cluster logs via Splunk. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: splunkHec: hec_host: 192.168.122.101 hec_port: 8088 insecure_ssl: true index: harvester-log-index hec_token: valueFrom: secretKeyRef: key: HECTOKEN name: splunk-hec-token2 buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: globalOutputRefs: - harvester-logging-splunk Loki​ You can follow the instructions in the logging HEP on deploying and viewing cluster logs via Grafana Loki. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-loki namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - harvester-loki --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-loki namespace: cattle-logging-system spec: loki: url: http://loki-stack.cattle-logging-system.svc:3100 extra_labels: logOutput: harvester-loki Audit​ Harvester collects Kubernetes audit and is able to send the audit to various types of log servers. The policy file to guide kube-apiserver is here. Audit Definition​ In kubernetes, the audit data is generated by kube-apiserver according to defined policy. ... Audit policy Audit policy defines rules about what events should be recorded and what data they should include. The audit policy object structure is defined in the audit.k8s.io API group. When an event is processed, it's compared against the list of rules in order. The first matching rule sets the audit level of the event. The defined audit levels are: None - don't log events that match this rule. Metadata - log request metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body. Request - log event metadata and request body but not response body. This does not apply for non-resource requests. RequestResponse - log event metadata, request and response bodies. This does not apply for non-resource requests. Audit Log Format​ Audit Log Format in Kubernetes​ Kubernetes apiserver logs audit with following JSON format into a local file. { &quot;kind&quot;:&quot;Event&quot;, &quot;apiVersion&quot;:&quot;audit.k8s.io/v1&quot;, &quot;level&quot;:&quot;Metadata&quot;, &quot;auditID&quot;:&quot;13d0bf83-7249-417b-b386-d7fc7c024583&quot;, &quot;stage&quot;:&quot;RequestReceived&quot;, &quot;requestURI&quot;:&quot;/apis/flowcontrol.apiserver.k8s.io/v1beta2/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1&quot;, &quot;verb&quot;:&quot;create&quot;, &quot;user&quot;:{&quot;username&quot;:&quot;system:apiserver&quot;,&quot;uid&quot;:&quot;d311c1fe-2d96-4e54-a01b-5203936e1046&quot;,&quot;groups&quot;:[&quot;system:masters&quot;]}, &quot;sourceIPs&quot;:[&quot;::1&quot;], &quot;userAgent&quot;:&quot;kube-apiserver/v1.24.7+rke2r1 (linux/amd64) kubernetes/e6f3597&quot;, &quot;objectRef&quot;:{&quot;resource&quot;:&quot;prioritylevelconfigurations&quot;, &quot;apiGroup&quot;:&quot;flowcontrol.apiserver.k8s.io&quot;, &quot;apiVersion&quot;:&quot;v1beta2&quot;}, &quot;requestReceivedTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot;, &quot;stageTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot; } Audit Log Format before Being Sent to Log Servers​ Harvester keeps the audit log unchanged before sending it to the log server. Audit Log Output/ClusterOutput​ To output audit related log, the Output/ClusterOutput requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit Only from the Type drpo-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: http: endpoint: &quot;http://192.168.122.159:8096/&quot; open_timeout: 3 format: type: &quot;json&quot; buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Audit Log Flow/ClusterFlow​ To route audit related logs, the Flow/ClusterFlow requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit. When you config from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;harvester-audit-webhook&quot; loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Harvester​ Event​ Harvester collects Kubernetes event and is able to send the event to various types of log servers. Event Definition​ Kubernetes events are objects that show you what is happening inside a cluster, such as what decisions were made by the scheduler or why some pods were evicted from the node. All core components and extensions (operators/controllers) may create events through the API Server. Events have no direct relationship with log messages generated by the various components, and are not affected with the log verbosity level. When a component creates an event, it often emits a corresponding log message. Events are garbage collected by the API Server after a short time (typically after an hour), which means that they can be used to understand issues that are happening, but you have to collect them to investigate past events. Events are the first thing to look at for application, as well as infrastructure operations when something is not working as expected. Keeping them for a longer period is essential if the failure is the result of earlier events, or when conducting post-mortem analysis. Event Log Format​ Event Log Format in Kubernetes​ A kubernetes event example: { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;count&quot;: 1, &quot;eventTime&quot;: null, &quot;firstTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;involvedObject&quot;: { &quot;apiVersion&quot;: &quot;kubevirt.io/v1&quot;, &quot;kind&quot;: &quot;VirtualMachineInstance&quot;, &quot;name&quot;: &quot;vm-ide-1&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604601&quot;, &quot;uid&quot;: &quot;1bd4133f-5aa3-4eda-bd26-3193b255b480&quot; }, &quot;kind&quot;: &quot;Event&quot;, &quot;lastTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;message&quot;: &quot;VirtualMachineInstance defined.&quot;, &quot;metadata&quot;: { &quot;creationTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;name&quot;: &quot;vm-ide-1.170e43cbdd833b62&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604626&quot;, &quot;uid&quot;: &quot;0114f4e7-1d4a-4201-b0e5-8cc8ede202f4&quot; }, &quot;reason&quot;: &quot;Created&quot;, &quot;reportingComponent&quot;: &quot;&quot;, &quot;reportingInstance&quot;: &quot;&quot;, &quot;source&quot;: { &quot;component&quot;: &quot;virt-handler&quot;, &quot;host&quot;: &quot;harv1&quot; }, &quot;type&quot;: &quot;Normal&quot; }, Event Log Format before Being Sent to Log Servers​ Each event log has the format of: {&quot;stream&quot;:&quot;&quot;,&quot;logtag&quot;:&quot;F&quot;,&quot;message&quot;:&quot;&quot;,&quot;kubernetes&quot;:{&quot;&quot;}}. The kubernetes event is in the field message. { &quot;stream&quot;:&quot;stdout&quot;, &quot;logtag&quot;:&quot;F&quot;, &quot;message&quot;:&quot;{ \\\\&quot;verb\\\\&quot;:\\\\&quot;ADDED\\\\&quot;, \\\\&quot;event\\\\&quot;:{\\\\&quot;metadata\\\\&quot;:{\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1.170e446c3f890433\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;0b44b6c7-b415-4034-95e5-a476fcec547f\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612482\\\\&quot;,\\\\&quot;creationTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;managedFields\\\\&quot;:[{\\\\&quot;manager\\\\&quot;:\\\\&quot;virt-controller\\\\&quot;,\\\\&quot;operation\\\\&quot;:\\\\&quot;Update\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;v1\\\\&quot;,\\\\&quot;time\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;}]},\\\\&quot;involvedObject\\\\&quot;:{\\\\&quot;kind\\\\&quot;:\\\\&quot;VirtualMachineInstance\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;1bd4133f-5aa3-4eda-bd26-3193b255b480\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;kubevirt.io/v1\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612477\\\\&quot;},\\\\&quot;reason\\\\&quot;:\\\\&quot;SuccessfulDelete\\\\&quot;,\\\\&quot;message\\\\&quot;:\\\\&quot;Deleted PodDisruptionBudget kubevirt-disruption-budget-hmmgd\\\\&quot;,\\\\&quot;source\\\\&quot;:{\\\\&quot;component\\\\&quot;:\\\\&quot;disruptionbudget-controller\\\\&quot;},\\\\&quot;firstTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;lastTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;count\\\\&quot;:1,\\\\&quot;type\\\\&quot;:\\\\&quot;Normal\\\\&quot;,\\\\&quot;eventTime\\\\&quot;:null,\\\\&quot;reportingComponent\\\\&quot;:\\\\&quot;\\\\&quot;,\\\\&quot;reportingInstance\\\\&quot;:\\\\&quot;\\\\&quot;} }&quot;, &quot;kubernetes&quot;:{&quot;pod_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;namespace_name&quot;:&quot;cattle-logging-system&quot;,&quot;pod_id&quot;:&quot;d3453153-58c9-456e-b3c3-d91242580df3&quot;,&quot;labels&quot;:{&quot;app.kubernetes.io/instance&quot;:&quot;harvester-default-event-tailer&quot;,&quot;app.kubernetes.io/name&quot;:&quot;event-tailer&quot;,&quot;controller-revision-hash&quot;:&quot;harvester-default-event-tailer-747b9d4489&quot;,&quot;statefulset.kubernetes.io/pod-name&quot;:&quot;harvester-default-event-tailer-0&quot;},&quot;annotations&quot;:{&quot;cni.projectcalico.org/containerID&quot;:&quot;aa72487922ceb4420ebdefb14a81f0d53029b3aec46ed71a8875ef288cde4103&quot;,&quot;cni.projectcalico.org/podIP&quot;:&quot;10.52.0.178/32&quot;,&quot;cni.projectcalico.org/podIPs&quot;:&quot;10.52.0.178/32&quot;,&quot;k8s.v1.cni.cncf.io/network-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;k8s.v1.cni.cncf.io/networks-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;kubernetes.io/psp&quot;:&quot;global-unrestricted-psp&quot;},&quot;host&quot;:&quot;harv1&quot;,&quot;container_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;docker_id&quot;:&quot;455064de50cc4f66e3dd46c074a1e4e6cfd9139cb74d40f5ba00b4e3e2a7ab2d&quot;,&quot;container_hash&quot;:&quot;docker.io/banzaicloud/eventrouter@sha256:6353d3f961a368d95583758fa05e8f4c0801881c39ed695bd4e8283d373a4262&quot;,&quot;container_image&quot;:&quot;docker.io/banzaicloud/eventrouter:v0.1.0&quot;} } Event Log Output/ClusterOutput​ Events share the Output/ClusterOutput with Logging. Select Logging/Event from the Type drop-down list. Event Log Flow/ClusterFlow​ Compared with the normal Logging Flow/ClusterFlow, the Event related Flow/ClusterFlow, has one more match field with the value of event-tailer. When you configure from the Harvester dashboard, the field is added automatically. Select Event from the Type drop-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-event-webhook namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: - select: labels: app.kubernetes.io/name: event-tailer globalOutputRefs: - harvester-event-webhook ","keywords":"Harvester Logging Audit Event","version":"v1.3 (latest)"},{"title":"Harvester Cloud Provider","type":0,"sectionRef":"#","url":"/v1.3/rancher/cloud-provider","content":"Harvester Cloud Provider RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and Harvester cluster storage passthrough support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2 cluster.How to use the Harvester load balancer. Backward Compatibility Notice​ note Please note a known backward compatibility issue if you're using the Harvester cloud provider version v0.2.2 or higher. If your Harvester version is below v1.2.0 and you intend to use newer RKE2 versions (i.e., &gt;= v1.26.6+rke2r1, v1.25.11+rke2r1, v1.24.15+rke2r1), it is essential to upgrade your Harvester cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester cloud provider. For a detailed support matrix, please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section of the official website. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hope to remove this limitation in a future release of Harvester. Deploying to the RKE1 Cluster with Harvester Node Driver​ When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Deploying to the RKE2 Cluster with Harvester Node Driver​ When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Deploying to the RKE2 custom cluster (experimental)​ Use generate_addon.sh to generate a cloud-config and place it into the directory /etc/kubernetes/cloud-config on every custom node. curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | bash -s &lt;serviceaccount name&gt; &lt;namespace&gt; note The generate_addon.sh script depends on kubectl and jq to operate the Harvester cluster. The script needs access to the Harvester Cluster kubeconfig to work. You can find the kubeconfig file from one of the Harvester management nodes in the /etc/rancher/rke2/rke2.yaml path. The namespace needs to be the namespace in which the guest cluster will be created. Configure the Cloud Provider to Harvester and select Create to spin up the cluster. Deploying to the K3s cluster with Harvester node driver (experimental)​ When spinning up a K3s cluster using the Harvester node driver, you can perform the following steps to deploy the harvester cloud provider: Use generate_addon.sh to generate cloud config. curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | bash -s &lt;serviceaccount name&gt; &lt;namespace&gt; The output will look as follows: ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: &lt;CACERT&gt; server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: &lt;TOKEN&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: &lt;CONTENT&gt; owner: root:root path: /etc/kubernetes/cloud-config permissions: '0644' Copy and paste the cloud-init user data content to Machine Pools &gt;Show Advanced &gt; User Data. Add the following HelmChart yaml of harvester-cloud-provider to Cluster Configuration &gt; Add-On Config &gt; Additional Manifest. apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.2.2 helmVersion: v3 Disable the in-tree cloud provider in the following ways: Click the Edit as YAML button. Disable servicelb and set disable-cloud-controller: true to disable the default K3s cloud controller. machineGlobalConfig: disable: - servicelb disable-cloud-controller: true Add cloud-provider=external to use the Harvester cloud provider. machineSelectorConfig: - config: kubelet-arg: - cloud-provider=external protect-kernel-defaults: false With these settings in place a K3s cluster should provision successfully while using the external cloud provider. Upgrade Cloud Provider​ Upgrade RKE2​ The cloud provider can be upgraded by upgrading the RKE2 version. You can upgrade the RKE2 cluster via the Rancher UI as follows: Click ☰ &gt; Cluster Management.Find the guest cluster that you want to upgrade and select ⋮ &gt; Edit Config.Select Kubernetes Version.Click Save. Upgrade RKE/K3s​ RKE/K3s upgrade cloud provider via the Rancher UI, as follows: Click ☰ &gt; RKE/K3s Cluster &gt; Apps &gt; Installed Apps.Find the cloud provider chart and select ⋮ &gt; Edit/Upgrade.Select Version. Click Next &gt; Update. Load Balancer Support​ Once you've deployed the Harvester cloud provider, you can leverage the Kubernetes LoadBalancer service to expose a microservice within the guest cluster to the external world. Creating a Kubernetes LoadBalancer service assigns a dedicated Harvester load balancer to the service, and you can make adjustments through the Add-on Config within the Rancher UI. IPAM​ Harvester's built-in load balancer offers both DHCP and Pool modes, and you can configure it by adding the annotation cloudprovider.harvesterhci.io/ipam: $mode to its corresponding service. Starting from Harvester cloud provider &gt;= v0.2.0, it also introduces a unique Share IP mode. A service shares its load balancer IP with other services in this mode. DCHP: A DHCP server is required. The Harvester load balancer will request an IP address from the DHCP server. Pool: An IP pool must be configured first. The Harvester load balancer controller will allocate an IP for the load balancer service following the IP pool selection policy. Share IP: When creating a new load balancer service, you can re-utilize an existing load balancer service IP. The new service is referred to as a secondary service, while the currently chosen service is the primary one. To specify the primary service in the secondary service, you can add the annotation cloudprovider.harvesterhci.io/primary-service: $primary-service-name. However, there are two known limitations: Services that share the same IP address can't use the same port.Secondary services cannot share their IP with additional services. note Modifying the IPAM mode isn't allowed. You must create a new service if you intend to change the IPAM mode. Health checks​ Beginning with Harvester cloud provider v0.2.0, additional health checks of the LoadBalancer service within the guest Kubernetes cluster are no longer necessary. Instead, you can configure liveness and readiness probes for your workloads. Consequently, any unavailable pods will be automatically removed from the load balancer endpoints to achieve the same desired outcome.","keywords":"Harvester harvester RKE rke RKE2 rke2 Harvester Cloud Provider","version":"v1.3 (latest)"},{"title":"Harvester CSI Driver","type":0,"sectionRef":"#","url":"/v1.3/rancher/csi-driver","content":"Harvester CSI Driver The Harvester Container Storage Interface (CSI) Driver provides a standard CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines that run as guest Kubernetes nodes are in the same namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hopeto remove this limitation in a future release of Harvester. note Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only(ROX) and read-write(RWX) support. Deploying with Harvester RKE1 node driver​ Select the Harvester(Out-of-tree) option. Install Harvester CSI Driver from the Rancher marketplace. Deploying with Harvester RKE2 node driver​ When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed automatically when Harvester cloud provider is selected. Install CSI driver manually in the RKE2 cluster​ If you prefer to install the Harvester CSI driver without enabling the Harvester cloud provider, you can refer to the following steps: Prerequisites of manual install​ Ensure that you have the following prerequisites in place: You have kubectl and jq installed on your system.You have the kubeconfig file for your bare-metal Harvester cluster. You can find the kubeconfig file from one of the Harvester management nodes in the /etc/rancher/rke2/rke2.yaml path. export KUBECONFIG=/path/to/your/harvester-kubeconfig Perform the following steps to deploy the Harvester CSI driver manually: Deploy Harvester CSI driver​ Generate the cloud-config. You can generate the cloud-config file using the generate_addon_csi.sh script. It is available on the harvester/harvester-csi-driver repo. &lt;serviceaccount name&gt; usually corresponds to your guest cluster name, and &lt;namespace&gt; should match the machine pool's namespace. ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; RKE2 The generated output will be similar to the following one: ########## cloud-config ############ apiVersion: v1 clusters: - cluster: &lt;token&gt; server: https://&lt;YOUR HOST HARVESTER VIP&gt;:6443 name: default contexts: - context: cluster: default namespace: default user: rke2-guest-01-default-default name: rke2-guest-01-default-default current-context: rke2-guest-01-default-default kind: Config preferences: {} users: - name: rke2-guest-01-default-default user: token: &lt;token&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: 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 owner: root:root path: /var/lib/rancher/rke2/etc/config-files/cloud-provider-config permissions: '0644' Copy and paste the cloud-init user data content to Machine Pools &gt; Show Advanced &gt; User Data. The cloud-provider-config file will be created after you apply the cloud-init user data above. You can find it on the guest Kubernetes nodes at the path /var/lib/rancher/rke2/etc/config-files/cloud-provider-config. Configure the Cloud Provider either to Default - RKE2 Embedded or External. Select Create to create your RKE2 cluster. Once the RKE2 cluster is ready, install the Harvester CSI Driver chart from the Rancher marketplace. You do not need to change the cloud-config path by default. note If you prefer not to install the Harvester CSI driver using Rancher (Apps &gt; Charts), you can use Helm instead. The Harvester CSI driver is packaged as a Helm chart. For more information, see https://charts.harvesterhci.io. By following the above steps, you should be able to see those CSI driver pods are up and running on the kube-system namespace, and you can verify it by provisioning a new PVC using the default StorageClass harvester on your RKE2 cluster. Deploying with Harvester K3s node driver​ You can follow the Deploy Harvester CSI Driver steps described in the RKE2 section. The only difference is in generating the cloud-init config where you need to specify the provider type as k3s: ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; k3s Customize the Default StorageClass​ The Harvester CSI driver provides the interface for defining the default StorageClass. If the default StorageClass in unspecified, the Harvester CSI driver uses the default StorageClass of the host Harvester cluster. You can use the parameter host-storage-class to customize the default StorageClass. Create a StorageClass for the host Harvester cluster. Example: Deploy the CSI driver with the parameter host-storage-class. Example: Verify that the Harvester CSI driver is ready. On the PersistentVolumeClaims screen, create a PVC. Select Use a Storage Class to provision a new Persistent Volume and specify the StorageClass you created. Example: Once the PVC is created, note the name of the provisioned volume and verify that the status is Bound. Example: On the Volumes screen, verify that the volume was provisioned using the StorageClass that you created. Example: Passthrough Custom StorageClass​ Beginning with Harvester CSI driver v0.1.15, it's possible to create a PersistentVolumeClaim (PVC) using a different Harvester StorageClass on the guest Kubernetes Cluster. note Harvester CSI driver v0.1.15 is supported out of the box starting with the following RKE2 versions. For RKE1, manual installation of the CSI driver chart is required: v1.23.16+rke2r1 and laterv1.24.10+rke2r1 and laterv1.25.6+rke2r1 and laterv1.26.1+rke2r1 and laterv1.27.1+rke2r1 and later Prerequisites​ Add the following prerequisites to your Harvester cluster to ensure the Harvester CSI driver displays error messages correctly. Proper RBAC settings are essential for error message visibility, especially when creating a PVC with a non-existent StorageClass, as shown in the image below: Follow these steps to set up RBAC for error message visibility: Create a new clusterrole named harvesterhci.io:csi-driver using the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: apiserver app.kubernetes.io/name: harvester app.kubernetes.io/part-of: harvester name: harvesterhci.io:csi-driver rules: - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - get - list - watch Create a new clusterrolebinding associated with the clusterrole above with the relevant serviceaccount using the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: &lt;namespace&gt;-&lt;serviceaccount name&gt; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: harvesterhci.io:csi-driver subjects: - kind: ServiceAccount name: &lt;serviceaccount name&gt; namespace: &lt;namespace&gt; Make sure the serviceaccount name and namespace match your cloud provider settings. Perform the following steps to retrieve these details. Find the rolebinding associated with your cloud provider: $ kubectl get rolebinding -A |grep harvesterhci.io:cloudprovider default default-rke2-guest-01 ClusterRole/harvesterhci.io:cloudprovider 7d1h Extract the subjects information from this rolebinding: $ kubectl get rolebinding default-rke2-guest-01 -n default -o yaml |yq -e '.subjects' Identify the ServiceAccount information: - kind: ServiceAccount name: rke2-guest-01 namespace: default Deploying​ Now you can create a new StorageClass that you intend to use in your guest Kubernetes cluster. For administrators, you can create a desired StorageClass (e.g., named replica-2) in your bare-metal Harvester cluster. Then, on the guest Kubernetes cluster, create a new StorageClass associated with the StorageClass named replica-2 from the Harvester Cluster: note When choosing a Provisioner, select Harvester (CSI). The Host StorageClass parameter should match the StorageClass name created on the Harvester Cluster.For guest Kubernetes owners, you may request that the Harvester cluster administrator create a new StorageClass.If you leave the Host StorageClass field empty, the default StorageClass of the Harvester cluster will be used. You can now create a PVC based on this new StorageClass, which utilizes the Host StorageClass to provision volumes on the bare-metal Harvester cluster.","keywords":"Harvester harvester Rancher Integration","version":"v1.3 (latest)"},{"title":"IP Pool","type":0,"sectionRef":"#","url":"/v1.3/networking/ippool","content":"IP Pool Available as of v1.2.0 Harvester IP Pool is a built-in IP address management (IPAM) solution exclusively available to Harvester load balancers (LBs). Features​ Multiple IP ranges: Each IP pool can contain multiple IP ranges or CIDRs.Allocation history: The IP pool keeps track of the allocation history of every IP address and prioritizes assigning previously allocated addresses by load balancer name. status: allocatedHistory: 192.168.178.8: default/rke2-default-lb-pool-2fab9ac0 Scope: IP pools can be confined to a particular network, project, namespace, or guest cluster. How to create​ To create a new IP pool: Go to the Networks &gt; IP Pools page and select Create.Specify the Name of the IP pool.Go to the Range tab to specify the IP ranges for the IP pool. You can add multiple IP ranges.Go to the Selector tab to specify the Scope and Priority of the IP pool. Selection policy​ Each IP pool will have a specific range, and you can specify the corresponding requirements in the LB annotations. IP pools that meet the specified requirements will automatically assign IP addresses to LBs. LBs utilize the following annotations to express requirements (all annotations are optional): loadbalancer.harvesterhci.io/network specifies the VM network the guest cluster nodes use.loadbalancer.harvesterhci.io/project and loadbalancer.harvesterhci.io/namespace identify the project and namespace of the VMs that comprise the guest cluster.loadbalancer.harvesterhci.io/cluster denotes the name of the guest cluster. The IP pool has a selector, including network and scope, to match the requirements of the LB. Network is a hard condition. The optional IP pool must match the value of the LB annotation loadbalancer.harvesterhci.io/network.Every IP pool, except the global IP pool, has a unique scope different from others if its priority is 0. The project, namespace, or cluster name of LBs should be in the scope of the IP pool if they want to get an IP from this pool. spec.selector.priority specifies the priority of the IP Pool. The larger the number, the higher the priority. If the priority is not 0, the value should differ. The priority helps you to migrate the old IP pool to the new one.If the IP Pool has a scope that matches all projects, namespaces, and guest clusters, it's called a global IP pool, and only one global IP pool is allowed. If there is no IP pool matching the requirements of the LB, the IPAM will allocate an IP address from the global IP pool if it exists. Examples​ Example 1: You wish to set up an IP pool within the range 192.168.100.0/24 for the default namespace. In this scenario, all load balancers within the default namespace will receive an IP address from this designated IP pool: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: default-ip-pool spec: ranges: - subnet: 192.168.100.0/24 selector: scope: namespace: default Example 2: You have a guest cluster rke2 deployed within the network default/vlan1, and its project/namespace name is product/default. If you want to configure an exclusive IP pool range 192.168.10.10-192.168.10.20 for it. Refer to the following YAML config: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: rke2-ip-pool spec: ranges: - subnet: 192.168.10.0/24 rangeStart: 192.168.10.10 rangeEnd: 192.168.10.20 selector: network: default/vlan1 scope: - project: product namespace: default guestCluster: rke2 Example 3: If you have specified the IP pool default-ip-pool for the default namespace, you want to migrate the IP pool default-ip-pool to a different IP pool default-ip-pool-2 with range 192.168.200.0/24. It's not allowed to specify over one IP pool for the same scope, but you can give the IP pool default-ip-pool-2 a higher priority than default-ip-pool. Refer to the following YAML config: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: default-ip-pool-2 spec: ranges: - subnet: 192.168.200.0/24 selector: priority: 1 # The priority is higher than default-ip-pool scope: namespace: default Example 4: You want to configure a global IP pool with a CIDR range of 192.168.20.0/24: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: global-ip-pool spec: ranges: - subnet: 192.168.20.0/24 selector: scope: - project: &quot;*&quot; namespace: &quot;*&quot; guestCluster: &quot;*&quot; Allocation policy​ The IP pool prioritizes the allocation of previously assigned IP addresses based on their allocation history.IP addresses are assigned in ascending order. note Starting with Harvester v1.2.0, the vip-pools setting is deprecated. Following the upgrade, this setting will be automatically migrated to the Harvester IP pools.","keywords":"IP Pool","version":"v1.3 (latest)"},{"title":"Creating an K3s Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.3/rancher/node/k3s-cluster","content":"Creating an K3s Kubernetes Cluster You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. note Harvester K3s node driver is in Tech Preview.VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For the port requirements of the guest clusters deployed within Harvester, please refer to the port requirements for guest clusters. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create K3s Kubernetes cluster​ You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules. Using Harvester K3s node driver in air gapped environment​ K3s provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with the required packages (e.g., iptables, qemu-guest-agent).Option 2. Go to Show Advanced &gt; User Data to allow VMs to install the required packages via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.3 (latest)"},{"title":"Harvester Node Driver","type":0,"sectionRef":"#","url":"/v1.3/rancher/node/node-driver","content":"Harvester Node Driver The Harvester node driver, similar to the Docker Machine driver, is used to provision VMs in the Harvester cluster, and Rancher uses it to launch and manage Kubernetes clusters. One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected. Additionally, the Harvester node driver is integrated with the Harvester cloud provider by default, providing built-in load balancer support as well as storage passthrough from the bare-metal cluster to the guest Kubernetes clusters to gain native storage performance. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. note The Harvester node driver only supports cloud images. This is because ISO images usually require additional setup that interferes with a clean deployment (without requiring user intervention), and they are not typically used in cloud environments. Harvester node driver​ Starting from Rancher v2.6.3, the Harvester node driver is enabled by default. You can go to the Cluster Management &gt; Drivers &gt; Node Drivers page to check the Harvester node driver status. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. note Refer to the Rancher downstream cluster support matrix for its supported RKE2 versions and guest OS versions. Changes made to the node driver configuration is not persisted. Any modifications applied will be reset upon restarting the Rancher container. Starting with Harvester node driver v0.6.3, the automatic injection of the qemu-guest-agent has been removed from the backend. If the image you are using does not contain the qemu-guest-agent package, you can still install it via the userdata config. Otherwise, the cluster will not be provisioned successfully. #cloud-config package_update: true packages: - qemu-guest-agent runcmd: - - systemctl - enable - '--now' - qemu-guest-agent.service RKE1 Kubernetes cluster​ Click to learn how to create RKE1 Kubernetes Clusters. RKE2 Kubernetes cluster​ Click to learn how to create RKE2 Kubernetes Clusters. K3s Kubernetes cluster​ Click to learn how to create k3s Kubernetes Clusters. Topology spread constraints​ Available as of v1.0.3 Within your guest Kubernetes cluster, you can use topology spread constraints to manage how workloads are distributed across nodes, accounting for factors such as failure domains like regions and zones. This helps achieve high availability and efficient resource utilization of the Harvester cluster resources. For RKE2 versions before v1.25.x, the minimum required versions to support the topology label sync feature are as follows: Minimum Required RKE2 Version&gt;= v1.24.3+rke2r1 &gt;= v1.23.9+rke2r1 &gt;= v1.22.12+rke2r1 Furthermore, for custom installation, the Harvester cloud provider version should be &gt;= v0.1.4. Sync topology labels to the guest cluster node​ During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone topology labels are supported. Configure topology labels on the Harvester nodes on the Hosts &gt; Edit Config &gt; Labels page. For example, add the topology labels as follows: topology.kubernetes.io/region: us-east-1 topology.kubernetes.io/zone: us-east-1a Create a downstream RKE2 cluster using the Harvester node driver with Harvester cloud provider enabled. We recommend adding the node affinity rules, which prevents nodes from drifting to other zones after VM rebuilding. After the cluster is ready, confirm that those topology labels are successfully synchronized to the nodes on the guest Kubernetes cluster. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints. note For Harvester cloud provider &gt;= v0.2.0, topology labels on the Harvester node will be automatically resynchronized when a VM (corresponding to the guest node) undergoes migration or update. For Harvester cloud provider &lt; v0.2.0, label synchronization will only occur during the initialization of guest nodes. To prevent nodes from drifting to different regions or zones, we recommend adding node affinity rules during cluster provisioning. This will allow you to schedule VMs in the same zone even after rebuilding.","keywords":"Harvester harvester Rancher rancher Harvester Node Driver","version":"v1.3 (latest)"},{"title":"Creating an RKE2 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.3/rancher/node/rke2-cluster","content":"Creating an RKE2 Kubernetes Cluster You can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For the port requirements of the guest clusters deployed within Harvester, please refer to the doc here.For RKE2 with Harvester cloud provider support matrix, please refer to the website here. Backward Compatibility Notice​ note Please note a known backward compatibility issue if you're using the Harvester cloud provider version v0.2.2 or higher. If your Harvester version is below v1.2.0 and you intend to use newer RKE2 versions (i.e., &gt;= v1.26.6+rke2r1, v1.25.11+rke2r1, v1.24.15+rke2r1), it is essential to upgrade your Harvester cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester cloud provider. For a detailed support matrix, please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section of the official website. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create RKE2 kubernetes cluster​ Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).(optional) Configure the Show Advanced &gt; User Data to install the required packages of VM. #cloud-config packages: - iptables note Calico and Canal networks require the iptables or xtables-nft package to be installed on the node, for more details, please refer to the RKE2 known issues. Click Create. note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration.Only imported Harvester clusters are supported by the Harvester node driver. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Add workload affinity​ Available as of v1.2.0 + Rancher v2.7.6 The workload affinity rules allow you to constrain which nodes your machines can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels. Workload affinity rules can be added to the machine pools during the cluster creation: Select Show Advanced and choose Add Workload Selector.Select Type, Affinity or Anti-Affinity.Select Priority. Prefered means it's an optional rule, and Required means a mandatory rule.Select the namespaces for the target workloads.Select Add Rule to specify the workload affinity rules.Set Topology Key to specify the label key that divides Harvester hosts into different topologies. See the Kubernetes Pod Affinity and Anti-Affinity Documentation for more details. Update RKE2 Kubernetes cluster​ The fields highlighted below of the RKE2 machine pool represent the Harvester VM configurations. Any modifications to these fields will trigger node reprovisioning. Using Harvester RKE2 node driver in air gapped environment​ RKE2 provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. Calico and Canal require the iptables or xtables-nft package to be installed on the node. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with required packages (e.g., iptables, qemu-guest-agent).Option 2. Go to Show Advanced &gt; User Data to allow VMs to install the required packages via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.3 (latest)"},{"title":"Rancher Integration","type":0,"sectionRef":"#","url":"/v1.3/rancher/rancher-integration","content":"Rancher Integration Rancher is an open-source multi-cluster management platform. Starting with Rancher v2.6.1, Rancher has integrated Harvester by default to centrally manage VMs and containers. Users can import and manage multiple Harvester clusters using the Rancher Virtualization Management feature. Leveraging the Rancher's authentication feature and RBAC control for multi-tenancy support. For a comprehensive overview of the support matrix, please refer to the Harvester &amp; Rancher Support Matrix. For the network requirements, please refer to the doc here. Deploying Rancher server​ To use Rancher with Harvester, please install Rancher on a separate server. If you want to try out the integration features, you can create a VM in Harvester and install the Rancher server by following the Helm CLI quick start. For production setup, please use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix MetalOutscale (uses Terraform) If you prefer, the following guide will take you through the same process in individual steps. Use this if you want to run Rancher with a different provider, on prem, or if you want to see how easy it is. Manual Install caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged, and your cluster may not be able to be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. Virtualization management​ With Rancher's virtualization management feature, you can import and manage your Harvester cluster. By clicking one of the imported clusters, you can easily access and manage a range of Harvester cluster resources, including hosts, VMs, images, volumes, and more. Additionally, the virtualization management feature leverages Rancher's existing capabilities, such as authentication with various auth providers and multi-tenancy support. For in-depth insights, please refer to the virtualization management page. Creating Kubernetes clusters using the Harvester node driver​ You can launch a Kubernetes cluster from Rancher using the Harvester node driver. When Rancher deploys Kubernetes onto these nodes, you can choose between Rancher Kubernetes Engine (RKE) or RKE2 distributions. One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected. Starting from Rancher version v2.6.1, the Harvester node driver is included by default. You can refer to the node-driver page for more details. Harvester baremetal container workload support (experimental)​ Available as of Harvester v1.2.0 + Rancher v2.7.6 Starting with Rancher v2.7.6, Harvester introduces a new feature that enables you to deploy and manage container workloads directly to the underlying Harvester cluster. With this feature, you can seamlessly combine the power of virtual machines with the flexibility of containerization, allowing for a more versatile and efficient infrastructure setup. This guide will walk you through enabling and using this experimental feature, highlighting its capabilities and best practices. To enable this new feature flag, follow these steps: Click the hamburger menu and choose the Global Settings tab.Click Feature Flags and locate the new feature flag harvester-baremetal-container-workload.Click the drop-down menu and select Activate to enable this feature.If the feature state changes to Active, the feature is successfully enabled. Key Features​ Unified Dashboard View:Once you've enabled the feature, you can explore the dashboard view of the Harvester cluster, just like you would with other standard Kubernetes clusters. This unified experience simplifies the management and monitoring of both your virtual machines and container workloads from a single, user-friendly interface. Deploy Custom Workloads:This feature lets you deploy custom container workloads directly to the bare-metal Harvester cluster. While this functionality is experimental, it introduces exciting possibilities for optimizing your infrastructure. However, we recommend deploying container and VM workloads in separate namespaces to ensure clarity and separation. note Critical system components such as monitoring, logging, Rancher, KubeVirt, and Longhorn are all managed by the Harvester cluster itself. You can't upgrade or modify these components. Therefore, exercise caution and avoid making changes to these critical system components.It is essential not to deploy any workloads to the system namespaces cattle-system, harvester-system, or longhorn-system. Keeping your workloads in separate namespaces is crucial to maintaining clarity and preserving the integrity of the system components.For best practices, we recommend deploying container and VM workloads in separate namespaces. note With this feature enabled, your Harvester cluster does not appear on the Continuous Delivery page in the Rancher UI. Please check the issue #4482 for further updates.","keywords":"Harvester harvester Rancher rancher Rancher Integration","version":"v1.3 (latest)"},{"title":"Resource Quotas","type":0,"sectionRef":"#","url":"/v1.3/rancher/resource-quota","content":"Resource Quotas ResourceQuota is used to limit the usage of resources within a namespace. It helps administrators control and restrict the allocation of cluster resources to ensure fairness and controlled resource distribution among namespaces. In Harvester, ResourceQuota can define usage limits for the following resources: CPU: Limits compute resource usage, including CPU cores and CPU time.Memory: Limits the usage of memory resources in bytes or other recognizable memory units. Set ResourceQuota via Rancher​ In the Rancher UI, administrators can configure resource quotas for namespaces through the following steps: Click the hamburger menu and choose the Virtualization Management tab.Choose one of the clusters and go to Projects/Namespaces &gt; Create Project.Specify the desired project Name. Next, go to the Resource Quotas tab and select the Add Resource option. Within the Resource Type field, select either CPU Limit or Memory Limit and define the Project Limit and Namespace Default Limit values. You can configure the Namespace limits as follows: Find the newly created project, and select Create Namespace.Specify the desired namespace Name, and adjust the limits.Complete the process by selecting Create. Overhead memory of virtual machine​ Upon creating a virtual machine (VM), the VM controller seamlessly incorporates overhead resources into the VM's configuration. These additional resources intend to guarantee the consistent and uninterrupted functioning of the VM. It's important to note that configuring memory limits requires a higher memory reservation due to the inclusion of these overhead resources. For example, consider the creation of a new VM with the following configuration: CPU: 8 coresMemory: 16Gi note The operating system, either Linux or Windows, does not affect overhead calculations. Memory Overhead is calculated in the following sections: Memory PageTables Overhead: This accounts for one bit for every 512b RAM size. For instance, a memory of 16Gi requires an overhead of 32Mi.VM Fixed Overhead: This consists of several components: VirtLauncherMonitorOverhead: 25Mi (the ps RSS for virt-launcher-monitor)VirtLauncherOverhead: 75Mi (the ps RSS for the virt-launcher process)VirtlogdOverhead: 17Mi (the ps RSS for virtlogd)LibvirtdOverhead: 33Mi (the ps RSS for libvirtd)QemuOverhead : 30Mi (the ps RSS for qemu, minus the RAM of its (stressed) guest, minus the virtual page table) 8Mi per CPU (vCPU) Overhead: Additionally, 8Mi of overhead per vCPU is added, along with a fixed 8Mi overhead for IOThread.Extra Added Overhead: This encompasses various factors like video RAM overhead and architecture overhead. Refer to Additional Overhead for further details. This calculation demonstrates that the VM instance necessitates an additional memory overhead of approximately 276Mi. For more information, see Memory Overhead. For more information on how the memory overhead is calculated in Kubevirt, refer to kubevirt/pkg/virt-controller/services/template.go. Automatic adjustment of ResourceQuota during migration​ When the allocated resource quota controlled by the ResourceQuota object reaches its limit, migrating a VM becomes unfeasible. The migration process automatically creates a new pod mirroring the resource requirements of the source VM. If these pod creation prerequisites surpass the defined quota, the migration operation cannot proceed. Available as of v1.2.0 In Harvester, the ResourceQuota values will dynamically expand ahead of migration to accommodate the resource needs of the target virtual machine. After migration, the ResourceQuotas will be reinstated to their prior configurations. Please be aware of the following constrains of the automatic resizing of ResourceQuota: ResourceQuota cannot be changed during VM migration.When raising the ResourceQuota value, if you create, start, or restore other VMs, Harvester will verify if the resources are sufficient based on the original ResourceQuota. If the conditions are not met, the system will alert that the migration process is not feasible.After expanding ResourceQuota, potential resource contention may occur between non-VM pods and VM pods, leading to migration failures. Therefore, deploying custom container workloads and VMs to the same namespace is not recommended.Due to the concurrent limitation of the webhook validator, the VM controller will execute a secondary validation to confirm resource sufficiency. If the resource is insufficient, it will auto config the VM's RunStrategy to Halted, and a new annotation harvesterhci.io/insufficient-resource-quota will be added to the VM object, informing you that the VM was shut down due to insufficient resources.","keywords":"Harvester harvester Rancher rancher Resource Quota","version":"v1.3 (latest)"},{"title":"Virtualization Management","type":0,"sectionRef":"#","url":"/v1.3/rancher/virtualization-management","content":"Virtualization Management With Rancher's virtualization management capabilities, you can import and manage multiple Harvester clusters. It provides a solution that unifies virtualization and container management from a single pane of glass. Additionally, Harvester leverages Rancher's existing capabilities, such as authentication and RBAC control, to provide full multi-tenancy support. Importing Harvester cluster​ Please refer to the Harvester &amp; Rancher Support Matrix to find a desired Rancher version. You can use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix MetalOutscale (uses Terraform)Manual Install Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server.Specify the Cluster Name and click Create. You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly.Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly.From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy​ In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication, users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions: Define user authorization outside the scope of any particular cluster. Cluster and Project Roles: Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings.A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example​ The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users &amp; Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project.A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab.Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save.Open an incognito browser and log in as project-owner.After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster and project to which you have been assigned.Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed.Create a VM with one of the images that you have uploaded.Log in with another user, e.g., project-readonly, and this user will only have the read permission of the assigned project. note The harvester-public namespace is a predefined namespace accessible to all users assigned to this cluster. Delete Imported Harvester Cluster​ Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management &gt; Harvester Clusters. Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. You will also need to reset the cluster-registration-url setting on the associated Harvester cluster to clean up the Rancher cluster agent. caution Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","keywords":"Harvester Rancher","version":"v1.3 (latest)"},{"title":"Creating an RKE1 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.3/rancher/node/rke1-cluster","content":"Creating an RKE1 Kubernetes Cluster You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. RKE1 and RKE2 have several slight behavioral differences. Refer to the differences between RKE1 and RKE2 to get some high-level insights. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For port requirements of guest clusters deployed within Harvester, please refer to the port requirements for guest clusters. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials, you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster.In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential name.Select &quot;Imported Harvester Cluster&quot;.Click Create. Create node templates​ You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials.Configure Instance Options: Configure the CPU, memory, and diskSelect an OS image that is compatible with the cloud-init config.Select a network that the node driver is able to connect to; currently, only VLAN is supported.Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu. (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs:Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration &gt; Node Templates: Check the Advanced Options tab and click Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Create an RKE1 Kubernetes cluster​ Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE1.Select Harvester node driver.Enter Cluster Name (required).Enter Name Prefix (required).Enter Template (required).Select etcd and Control Plane (required).On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver.Click Create. Using Harvester RKE1 node driver in air-gapped environments​ RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with both qemu-guest-agent and docker.Option 2. Configure the cloud-init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=&quot;http://192.168.0.1:3128&quot; HTTPS_PROXY=&quot;http://192.168.0.1:3128&quot; append: true ","keywords":"","version":"v1.3 (latest)"},{"title":"Harvester Terraform Provider","type":0,"sectionRef":"#","url":"/v1.3/terraform/terraform-provider","content":"Harvester Terraform Provider Support Matrix​ Harvester Version\tSupported Terraform Provider Harvester\tSupported Terraformer Harvesterv1.2.0\tv0.6.3\tv1.1.1-harvester v1.1.2\tv0.6.3\tv1.1.1-harvester v1.1.1\tv0.6.3\tv1.1.1-harvester v1.1.0\tv0.6.3\tv1.1.1-harvester Requirements​ Terraform &gt;= 0.13.xGo 1.18 to build the provider plugin Install The Provider​ copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = &quot;harvester/harvester&quot; version = &quot;&lt;replace to the latest release version&gt;&quot; } } } provider &quot;harvester&quot; { # Configuration options } Using the provider​ More details about the provider-specific configurations can be found in the docs. Github Repo: https://github.com/harvester/terraform-provider-harvester","keywords":"","version":"v1.3 (latest)"},{"title":"Installation","type":0,"sectionRef":"#","url":"/v1.3/troubleshooting/index","content":"Installation The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS)​ Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancherPassword: rancher Meeting hardware requirements​ Check that your hardware meets the minimum requirements to complete installation. Stuck in Loading images. This may take a few minutes...​ Because the system doesn't have a default route, your installer may become &quot;stuck&quot; in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev mgmt-br proto dhcp &lt;-- Does a default route exist? 10.10.0.0/24 dev mgmt-br proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. For more information, see DHCP Server Configuration. Modifying cluster token on agent nodes​ When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg=&quot;Bootstrapping Rancher (v2.7.5/v1.25.9+rke2r1)&quot; msg=&quot;failed to bootstrap system, will retry: generating plan: response 502: 502 Bad Gateway getting cacerts: &lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;\\r\\n&lt;body&gt;\\r\\n&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;\\r\\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\\r\\n&lt;/body&gt;\\r\\n&lt;/html&gt;\\r\\n&quot; Note that the Rancher version and IP address depend on your environment and might differ from the message above. To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml. For example, if the cluster token setup in the server node is ThisIsTheCorrectOne, you will update the token value as follow: token: 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/90_custom.yaml: name: Harvester Configuration stages: ... initramfs: - commands: - rm -f /etc/sysconfig/network/ifroute-mgmt-br files: - path: /etc/rancher/rancherd/config.yaml permissions: 384 owner: 0 group: 0 content: | server: https://$cluster-vip:443 role: agent token: &quot;ThisIsTheCorrectOne&quot; kubernetesVersion: v1.25.9+rke2r1 rancherVersion: v2.7.5 rancherInstallerImage: rancher/system-agent-installer-rancher:v2.7.5 labels: - harvesterhci.io/managed=true extraConfig: disable: - rke2-snapshot-controller - rke2-snapshot-controller-crd - rke2-snapshot-validation-webhook encoding: &quot;&quot; ownerstring: &quot;&quot; note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml. For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting troubleshooting information​ Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file.","keywords":"","version":"v1.3 (latest)"},{"title":"Harvester","type":0,"sectionRef":"#","url":"/v1.3/troubleshooting/harvester","content":"Harvester Fail to Deploy a Multi-node Cluster Due to Incorrect HTTP Proxy Setting​ ISO Installation Without a Harvester Configuration File​ Configure HTTP Proxy During Harvester Installation​ In some environments, you configure http-proxy of OS Environment during Harvester installation. Configure HTTP Proxy After First Node is Ready​ After the first node is installed successfully, you login into the Harvester GUI to configure http-proxy of Harvester System Settings. Then you continue to add more nodes to the cluster. One Node Becomes Unavailable​ The issue you may encounter: The first node is installed successfully. The second node is installed successfully. The third node is installed successfully. Then the second node changes to Unavialable state and cannot recover automatically. Solution​ When the nodes in the cluster do not use the HTTP Proxy to communicate with each other, after the first node is installed successfully, you need to configure http-proxy.noProxy against the CIDR used by those nodes. For example, your cluster assigns IPs from CIDR 172.26.50.128/27 to nodes via DHCP/static setting, please add this CIDR to noProxy. After setting this, you can continue to add new nodes to the cluster. For more details, please refer to Harvester issue 3091. ISO Installation With a Harvester Configuration File​ When a Harvester configuration file is used in ISO installation, please configure proper http-proxy in Harvester System Settings. PXE Boot Installation​ When PXE Boot Installation is adopted, please configure proper http-proxy in OS Environment and Harvester System Settings. Generate a Support Bundle​ Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Manually Download and Retain a Support Bundle File​ By default, a support bundle file is automatically generated, downloaded, and deleted after you click Create on the Harvester UI. However, you may want to retain a file for various reasons, including the following: You are unable to download the file because of network connectivity errors and other issues. You must use a previously generated file to troubleshoot issues (because generating a support bundle file takes time). You want to view information that only exists in a previously generated file. Even if the file remains in the cluster, the Harvester UI does not provide a download link. Use the following workaround to generate, manually download, and retain a support bundle file: Generate the File and Prevent Automatic Downloading​ On the Harvester UI, click Generate Support Bundle. When the progress indicator reaches 20% to 80%, close the browser tab to prevent automatic downloading of the generated file. Retrieve a list of all support bundles in all namespaces using kubectl. Example: $ kubectl get supportbundle -A NAMESPACE NAME ISSUE_URL DESCRIPTION AGE harvester-system bundle-htl5f sp1 3h43m Retrieve the details of all existing support bundles using the command kubectl get supportbundle -A -o yaml. Example: $ kubectl get supportbundle -A -oyaml apiVersion: v1 items: - apiVersion: harvesterhci.io/v1beta1 kind: SupportBundle metadata: creationTimestamp: &quot;2024-02-02T11:18:09Z&quot; generation: 5 name: bundle-htl5f // resource name namespace: harvester-system resourceVersion: &quot;1218311&quot; uid: a3776373-05fe-4584-8a9a-baac3fa91bbf spec: description: sp1 issueURL: &quot;&quot; status: conditions: - lastUpdateTime: &quot;2024-02-02T11:18:38Z&quot; status: &quot;True&quot; type: Initialized filename: supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-02T11-18-10Z.zip // support bundle file name filesize: 8868712 progress: 100 // 100 means successfully generated state: ready The file is ready for downloading when the value of progress is &quot;100&quot; and the value of state is &quot;ready&quot;. Download the File​ Create a download URL that includes the following information: VIP or DNS nameResource name of the fileParameter ?retain=true: If you do not include this parameter, resources related to the support bundle are automatically deleted after the file is successfully downloaded. Example: https://{vip/dns-name}/v1/harvester/supportbundles/bundle-htl5f/download?retain=true Download the file using either a command-line tool (for example, curl and wget) or a web browser. Example: curl -k https://{vip/dns-name}/v1/harvester/supportbundles/bundle-htl5f/download?retain=true -o sb2.zip Verify that resources related to the support bundle were not deleted. Example: $ kubectl get supportbundle -A NAMESPACE NAME ISSUE_URL DESCRIPTION AGE harvester-system bundle-htl5f sp1 3h43m (Optional) Delete the Related Resources​ Retained support bundle files consume memory and storage resources. Each file is backed by a supportbundle-manager-bundle* pod in the harvester-system namespace, and the generated ZIP file is stored in the /tmp folder of the pod's memory-based filesystem. Example: $ kubectl get pods -n harvester-system NAME READY STATUS RESTARTS AGE supportbundle-manager-bundle-dtl2k-69dcc69b59-w64vl 1/1 Running 0 8m18s You can delete the related resources using the following methods: Manual: Run the command kubectl delete supportbundle -n {namespace} {resource-name}. Deleting a support bundle object automatically deletes the pod that backs it. Example: $ kubectl delete supportbundle -n harvester-system bundle-htl5f supportbundle.harvesterhci.io &quot;bundle-htl5f&quot; deleted $ kubectl get supportbundle -A No resources found Automatic: Harvester deletes the related resources based on how the following settings are configured: support-bundle-expiration: Defines the time allowed for retaining a support bundle file support-bundle-timeout: Defines the time allowed for generating a support bundle file Manually Copy the Support Bundle File​ You can run the command kubectl cp to copy the generated file from the backing pod. Example: kubectl cp harvester-system/supportbundle-manager-bundle-dtl2k-69dcc69b59-w64vl:/tmp/support-bundle-kit/supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-02T11-18-10Z.zip bundle.zip Known Limitations​ Replacing the backing pod prevents the support bundle file from being downloaded. The support bundle file is stored in the /tmp folder of the pod's memory-based filesystem so it is removed when the pod is replaced during cluster and node rebooting, Kubernetes pod rescheduling, and other processes. After starting, the new pod regenerates the file but assigns a name that is different from the file name in the support bundle object. Example: A support bundle file is generated and retained. $ kubectl get supportbundle -A -oyaml apiVersion: v1 items: - apiVersion: harvesterhci.io/v1beta1 kind: SupportBundle metadata: creationTimestamp: &quot;2024-02-06T11:01:19Z&quot; generation: 5 name: bundle-yr2vq namespace: harvester-system resourceVersion: &quot;1583252&quot; uid: eb8538cf-886b-4791-a7b0-dbc34dcee524 spec: description: sp2 issueURL: &quot;&quot; status: conditions: - lastUpdateTime: &quot;2024-02-06T11:01:47Z&quot; status: &quot;True&quot; type: Initialized filename: supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-06T11-01-20Z.zip // file is ready to download filesize: 7832010 progress: 100 state: ready kind: List metadata: resourceVersion: &quot;&quot; The backing pod restarts. $ kubectl get pods -n harvester-system supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d -oyaml apiVersion: v1 kind: Pod metadata: ... labels: app: support-bundle-manager pod-template-hash: c5484fbdf rancher/supportbundle: bundle-yr2vq name: supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d namespace: harvester-system containerStatuses: - containerID: containerd://ea82b63875c18a2b5b36afea6a47a99a5efd26464f94d401cde1727d175ef740 ... name: manager ready: true restartCount: 1 started: true state: running: startedAt: &quot;2024-02-06T11:05:33Z&quot; // pod's latest starting timestamp, newer than the timestamp in support bundle's file name The backing pod regenerates the file after it starts. The name of the regenerated file is different from the file name recorded in the support bundle object. $ kubectl exec -i -t -n harvester-system supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d -- ls /tmp/support-bundle-kit -alth total 2.2M drwxr-xr-x 3 root root 4.0K Feb 6 11:05 . -rw-r--r-- 1 root root 2.2M Feb 6 11:05 supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-06T11-05-34Z.zip // different with above file name Attempts to download the regenerated file fail. The following download URL cannot be used to access the regenerated file. https://{vip/dns-name}/v1/harvester/supportbundles/bundle-yr2vq/download?retain=true. Retained support bundle files may affect system and node rebooting, node draining, and system upgrades. Retained support bundle files are backed by pods in the harvester-system namespace. These pods are replaced during system and node rebooting, node draining, and system upgrades, consuming CPU and memory resources. Moreover, the regenerated files are very similar in content to the retained files, which means that storage resources are also unnecessarily consumed. For more information, see Issue 3383. Access Embedded Rancher and Longhorn Dashboards​ Available as of v1.1.0 You can now access the embedded Rancher and Longhorn dashboards directly on the Support page, but you must first go to the Preferences page and check the Enable Extension developer features box under Advanced Features. note We only support using the embedded Rancher and Longhorn dashboards for debugging and validation purposes. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here. I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers​ If you changedSSL/TLS enabled protocols and ciphers settingsand you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl: # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{&quot;protocols&quot;:&quot;TLS99&quot;,&quot;ciphers&quot;:&quot;WRONG_CIPHER&quot;}' # &lt;- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly. Network interfaces are not showing up​ You may need help finding the correct interface with a 10G uplink since the interface is not showing up. The uplink doesn't show up when the ixgbe module fails to load because an unsupported SFP+ module type is detected. How to identify the issue with the unsupported SFP?​ Execute the command lspci | grep -i net to see the number of NIC ports connected to the motherboard. By running the command ip a, you can gather information about the detected interfaces. If the number of detected interfaces is less than the number of identified NIC ports, then it's likely that the problem arises from using an unsupported SFP+ module. Testing​ You can perform a simple test to verify whether the unsupported SFP+ is the cause. Follow these steps on a running node: Create the file /etc/modprobe.d/ixgbe.conf manually with the content: options ixgbe allow_unsupported_sfp=1 Then run following command: rmmod ixgbe &amp;&amp; modprobe ixgbe If the above steps are successful and the missing interface shows, we can confirm that the issue is an unsupported SFP+. However, the above test is not permanent and will be flushed out once rebooted. Solution​ Due to support issues, Intel restricts the types of SFPs used on their NICs. To make the above changes persistent, adding the following content to a config.yaml during installation is recommended. os: write_files: - content: | options ixgbe allow_unsupported_sfp=1 path: /etc/modprobe.d/ixgbe.conf - content: | name: &quot;reload ixgbe module&quot; stages: boot: - commands: - rmmod ixgbe &amp;&amp; modprobe ixgbe path: /oem/99_ixgbe.yaml ","keywords":"","version":"v1.3 (latest)"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/v1.3/troubleshooting/os","content":"Operating System Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the elemental-toolkit. The following sections contain information and tips to help users troubleshoot OS-related issues. How to log in to a Harvester node​ Users can log in to a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~&gt; sudo blkid # Or become root rancher@node1:~&gt; sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only?​ The OS file system, like a container image, is image-based and immutable except in some directories. We recommend using a toolbox container to run programs not packaged in the Harvester OS for debugging purposes. Please see this article to learn how to build and run a toolbox container. The Harvester OS also provides a way to enable the read-write mode temporarily. Please follow the following steps: caution Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0, we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat &gt; /oem/91_hack.yaml &lt;&lt;'EOF' name: &quot;Rootfs Layout Settings for debugrw&quot; stages: rootfs: - if: 'grep -q root=LABEL=COS_STATE /proc/cmdline &amp;&amp; grep -q rd.cos.debugrw /proc/cmdline' name: &quot;Layout configuration for debugrw&quot; environment_file: /run/cos/cos-layout.env environment: RW_PATHS: &quot; &quot; EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters​ note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry &quot;${display_name}&quot; --id cos { # label is kept around for backward compatibility set label=${active_label} set img=/cOS/active.img loopback $loopdev /$img source ($loopdev)/etc/cos/bootargs.cfg linux ($loopdev)$kernel $kernelcmd ${extra_cmdline} ${extra_active_cmdline} nomodeset initrd ($loopdev)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry​ To change the default entry, first check the --id attribute of a menu entry. Grub menu entries are located in the following files: /run/initramfs/cos-state/grub2/grub.cfg: Contains the default, fallback, and recovery entries/run/initramfs/cos-state/grubcustom: Contains the debug entry In the following example, the id of the entry is debug. # cat \\ /run/initramfs/cos-state/grub2/grub.cfg \\ /run/initramfs/cos-state/grubcustom &lt;...&gt; menuentry &quot;${display_name} (debug)&quot; --id debug { search --no-floppy --set=root --label COS_STATE set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd ${extra_cmdline} ${extra_passive_cmdline} ${crash_kernel_params} initrd (loop0)$initramfs } You can configure the default entry by running the following commands: # mount -o remount,rw /run/initramfs/cos-state # grub2-editenv /run/initramfs/cos-state/grub_oem_env set saved_entry=debug If necessary, you can undo the change by running the command grub2-editenv /run/initramfs/cos-state/grub_oem_env unset saved_entry. How to debug a system crash or hang​ Collect crash log​ If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps​ For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/&lt;time&gt; directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","keywords":"","version":"v1.3 (latest)"},{"title":"VM","type":0,"sectionRef":"#","url":"/v1.3/troubleshooting/vm","content":"VM The following sections contain information useful in troubleshooting issues related to Harvester VM management. VM Start Button is Not Visible​ Issue Description​ On rare occasions, the Start button is unavailable on the Harvester UI for VMs that are Off. Without that button, users are unable to start the VMs. VM General Operations​ On the Harvester UI, the Stop button is visible after a VM is created and started. The Start button is visible after the VM is stopped. When the VM is powered off from inside the VM, both the Start and Restart buttons are visible. General VM Related Objects​ A Running VM​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of all three objects is Running. # kubectl get vm NAME AGE STATUS READY vm8 7m25s Running True # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 78s Running 10.52.0.199 harv41 True # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 1/1 Running 0 80s A VM Stopped Using the Harvester UI​ Only the object vm exists and its status is Stopped. Both vmi and pod disappear. # kubectl get vm NAME AGE STATUS READY vm8 123m Stopped False # kubectl get vmi No resources found in default namespace. # kubectl get pod No resources found in default namespace. # A VM Stopped Using the VM's Poweroff Command​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of vm is Stopped, while the status of pod is Completed. # kubectl get vm NAME AGE STATUS READY vm8 134m Stopped False # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 2m49s Succeeded 10.52.0.199 harv41 False # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 0/1 Completed 0 2m54s Issue Analysis​ When the issue occurs, the objects vm, vmi, and pod exist. The status of the objects is similar to that of A VM Stopped Using the VM's Poweroff Command. Example: The VM ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachine ... status: conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready Similarly, the VMI (virtual machine instance) ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance ... name: ocffm031v000 ... status: activePods: ec36a1eb-84a5-4421-b57b-2c14c1975018: aibfredg02 conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready On the other hand, the pod virt-launcher-ocffm031v000-rrkss is not ready (status: &quot;False&quot;) because the pod has run to completion (reason: &quot;PodCompleted&quot;). The underlying container 0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb is terminated, and the exitCode is 0. - apiVersion: v1 kind: Pod ... name: virt-launcher-ocffm031v000-rrkss ... ownerReferences: - apiVersion: kubevirt.io/v1 ... kind: VirtualMachineInstance name: ocffm031v000 uid: 8d2cf524-7e73-4713-86f7-89e7399f25db uid: ec36a1eb-84a5-4421-b57b-2c14c1975018 ... status: conditions: - lastProbeTime: &quot;2023-07-18T13:48:56Z&quot; lastTransitionTime: &quot;2023-07-18T13:48:56Z&quot; message: the virtual machine is not paused reason: NotPaused status: &quot;True&quot; type: kubevirt.io/virtual-machine-unpaused - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-18T13:48:55Z&quot; reason: PodCompleted status: &quot;True&quot; type: Initialized - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: Ready - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: ContainersReady ... containerStatuses: - containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb image: registry.suse.com/suse/sles/15.4/virt-launcher:0.54.0-150400.3.3.2 imageID: sha256:43bb08efdabb90913534b70ec7868a2126fc128887fb5c3c1b505ee6644453a2 lastState: {} name: compute ready: false restartCount: 0 started: false state: terminated: containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb exitCode: 0 finishedAt: &quot;2023-07-20T08:38:55Z&quot; reason: Completed startedAt: &quot;2023-07-18T13:50:17Z&quot; A critical difference is that the Stop and Start actions appear in the stateChangeRequests property of vm. status: conditions: ... printableStatus: Stopped stateChangeRequests: - action: Stop uid: 8d2cf524-7e73-4713-86f7-89e7399f25db - action: Start Root Cause​ The root cause of this issue is under investigation. It is notable that the source code checks the status of vm and assumes that the object is starting. No Start and Restart operations are added to the object. func (vf *vmformatter) canStart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } .. } func (vf *vmformatter) canRestart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } ... } func (vf *vmformatter) isVMStarting(vm *kubevirtv1.VirtualMachine) bool { for _, req := range vm.Status.StateChangeRequests { if req.Action == kubevirtv1.StartRequest { return true } } return false } Workaround​ To address the issue, you can force delete the pod using the command kubectl delete pod virt-launcher-ocffm031v000-rrkss -n namespace --force. After the pod is successfully deleted, the Start button becomes visible again on the Harvester UI. Related Issue​ https://github.com/harvester/harvester/issues/4659 VM Stuck in Starting State with Error Messsage not a device node​ Impacted versions: v1.3.0 Issue Description​ Some VMs may fail to start and then become unresponsive after the cluster or some nodes are restarted. On the Dashboard screen of the Harvester UI, the status of the affected VMs is stuck at Starting. Issue Analysis​ The status of the pod related to the affected VM is CreateContainerError. $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vm1-w9bqs 0/2 CreateContainerError 0 9m39s The phrase failed to generate spec: not a device node can be found in the following: $kubectl get pods -oyaml apiVersion: v1 items: apiVersion: v1 kind: Pod metadata: ... containerStatuses: - image: registry.suse.com/suse/sles/15.5/virt-launcher:1.1.0-150500.8.6.1 imageID: &quot;&quot; lastState: {} name: compute ready: false restartCount: 0 started: false state: waiting: message: 'failed to generate container &quot;50f0ec402f6e266870eafb06611850a5a03b2a0a86fdd6e562959719ccc003b5&quot; spec: failed to generate spec: not a device node' reason: CreateContainerError kubelet.log file: file path: /var/lib/rancher/rke2/agent/logs/kubelet.log E0205 20:44:31.683371 2837 pod_workers.go:1294] &quot;Error syncing pod, skipping&quot; err=&quot;failed to \\&quot;StartContainer\\&quot; for \\&quot;compute\\&quot; with CreateContainerError: \\&quot;failed t o generate container \\\\\\&quot;255d42ec2e01d45b4e2480d538ecc21865cf461dc7056bc159a80ee68c411349\\\\\\&quot; spec: failed to generate spec: not a device node\\&quot;&quot; pod=&quot;default/virt-laun cher-caddytest-9tjzj&quot; podUID=d512bf3e-f215-4128-960a-0658f7e63c7c containerd.log file: file path: /var/lib/rancher/rke2/agent/containerd/containerd.log time=&quot;2024-02-21T11:24:00.140298800Z&quot; level=error msg=&quot;CreateContainer within sandbox \\&quot;850958f388e63f14a683380b3c52e57db35f21c059c0d93666f4fdaafe337e56\\&quot; for &amp;ContainerMetadata{Name:compute,Attempt:0,} failed&quot; error=&quot;failed to generate container \\&quot;5ddad240be2731d5ea5210565729cca20e20694e364e72ba14b58127e231bc79\\&quot; spec: failed to generate spec: not a device node&quot; After adding debug information to containerd, it identifies the error message not a device node is upon the file pvc-3c1b28fb-*. time=&quot;2024-02-22T15:15:08.557487376Z&quot; level=error msg=&quot;CreateContainer within sandbox \\&quot;d23af3219cb27228623cf8168ec27e64e836ed44f2b2f9cf784f0529a7f92e1e\\&quot; for &amp;ContainerMetadata{Name:compute,Attempt:0,} failed&quot; error=&quot;failed to generate container \\&quot;e4ed94fb5e9145e8716bcb87aae448300799f345197d52a617918d634d9ca3e1\\&quot; spec: failed to generate spec: get device path: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-3c1b28fb-683e-4bf5-9869-c9107a0f1732/20291c6b-62c3-4456-be8a-fbeac118ec19 containerPath: /dev/disk-0 error: not a device node&quot; This is a CSI related file, but it is an empty file instead of the expected device file. Then the containerd denied the CreateContainer request. $ ls /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-3c1b28fb-683e-4bf5-9869-c9107a0f1732/ -alth total 8.0K drwxr-x--- 2 root root 4.0K Feb 22 15:10 . -rw-r--r-- 1 root root 0 Feb 22 14:28 aa851da3-cee1-45be-a585-26ae766c16ca -rw-r--r-- 1 root root 0 Feb 22 14:07 20291c6b-62c3-4456-be8a-fbeac118ec19 drwxr-x--- 4 root root 4.0K Feb 22 14:06 .. -rw-r--r-- 1 root root 0 Feb 21 15:48 4333c9fd-c2c8-4da2-9b5a-1a310f80d9fd -rw-r--r-- 1 root root 0 Feb 21 09:18 becc0687-b6f5-433e-bfb7-756b00deb61b $file /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-3c1b28fb-683e-4bf5-9869-c9107a0f1732/20291c6b-62c3-4456-be8a-fbeac118ec19 : empty The output listed above directly contrasts with the following example, which shows the expected device file of a running VM. $ ls /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-732f8496-103b-4a08-83af-8325e1c314b7/ -alth total 8.0K drwxr-x--- 2 root root 4.0K Feb 21 10:53 . drwxr-x--- 4 root root 4.0K Feb 21 10:53 .. brw-rw---- 1 root root 8, 16 Feb 21 10:53 4883af80-c202-4529-a2c6-4e7f15fe5a9b Root Cause​ After the cluster or specific nodes are rebooted, the kubelet calls NodePublishVolume for the new pod without first calling NodeStageVolume. Moreover, the Longhorn CSI plugin bind mounts the regular file at the staging target path (previously used by the deleted pod) to the target path, and the operation is considered successful. Workaround​ Cluster level operation: Find the backing pods of the affected VMs and the related Longhorn volumes. $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vm1-nxfm4 0/2 CreateContainerError 0 7m11s $ kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default vm1-disk-0-9gc6h Bound pvc-f1798969-5b72-4d76-9f0e-64854af7b59c 1Gi RWX longhorn-image-fxsqr 7d22h Stop the affected VMs from Harvester UI. The VM may stuck in Stopping, continue the next step. Delete the backing pods forcely. $ kubectl delete pod virt-launcher-vm1-nxfm4 --force Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod &quot;virt-launcher-vm1-nxfm4&quot; force deleted The VM is off now. Node level operation, node by node: Cordon a node. Unmout all the affected Longhorn volumes in this node. You need to ssh to this node and execute the sudo -i umount path command. $ umount /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/* umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/4b2ab666-27bd-4e3c-a218-fb3d48a72e69: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/6aaf2bbe-f688-4dcd-855a-f9e2afa18862: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/91488f09-ff22-45f4-afc0-ca97f67555e7: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/bb4d0a15-737d-41c0-946c-85f4a56f072f: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/d2a54e32-4edc-4ad8-a748-f7ef7a2cacab: not mounted. Uncordon this node. Start the affected VMs from harvester UI. Wait some time, the VM will run successfully. The newly generated csi file is an expected device file. $ ls /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/ -alth ... brw-rw---- 1 root root 8, 64 Mar 6 11:47 7beb531d-a781-4775-ba5e-8773773d77f1 Related Issue​ https://github.com/harvester/harvester/issues/5109 https://github.com/longhorn/longhorn/issues/8009","keywords":"","version":"v1.3 (latest)"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/v1.3/upgrade/troubleshooting","content":"Troubleshooting Overview​ Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes. You can click the version in the support matrix table to see if there are any known issues.Dive into the upgrade design proposal. The following section briefly describes phases within an upgrade and possible diagnostic methods. Diagnose the upgrade flow​ A Harvester upgrade process contains several phases. Phase 1: Provision upgrade repository VM.​ The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s Phase 2: Preload container images​ The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase. Phase 3: Upgrade system services​ In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ... Phase 4: Upgrade nodes​ The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node.post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-&lt;hostname&gt;). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... caution Please do not start over an upgrade if the upgrade fails at this phase. Phase 5: Clean-up​ The Harvester controller deletes the upgrade repository VM and all files that are no longer needed. Common operations​ Start over an upgrade​ Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again. Download upgrade logs​ We have designed and implemented a mechanism to automatically collect all the upgrade-related logs and display the upgrade procedure. By default, this is enabled. You can also choose to opt out of such behavior. You can click the Download Log button to download the log archive during an upgrade. Log entries will be collected as files for each upgrade-related Pod, even for intermediate Pods. The support bundle provides a snapshot of the current state of the cluster, including logs and resource manifests, while the upgrade log preserves any logs generated during an upgrade. By combining these two, you can further investigate the issues during upgrades. After the upgrade ended, Harvester stops collecting the upgrade logs to avoid occupying the disk space. In addition, you can click the Dismiss it button to purge the upgrade logs. For more details, please refer to the upgrade log HEP. caution The storage volume for storing upgrade-related logs is 1GB by default. If an upgrade went into issues, the logs may consume all the available space of the volume. To work around such kind of incidents, try the following steps: Detach the log-archive Volume by scaling down the fluentd StatefulSet and downloader Deployment. # Locate the StatefulSet and Deployment $ kubectl -n harvester-system get statefulsets -l harvesterhci.io/upgradeLogComponent=aggregator NAME READY AGE hvst-upgrade-xxxxx-upgradelog-infra-fluentd 1/1 43s $ kubectl -n harvester-system get deployments -l harvesterhci.io/upgradeLogComponent=downloader NAME READY UP-TO-DATE AVAILABLE AGE hvst-upgrade-xxxxx-upgradelog-downloader 1/1 1 1 38s # Scale down the resources to terminate any Pods using the volume $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=0 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=0 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled Expand the volume size via Longhorn dashboard. For more details, please refer to the volume expansion guide. # Here's how to find out the actual name of the target volume $ kubectl -n harvester-system get pvc -l harvesterhci.io/upgradeLogComponent=log-archive -o jsonpath='{.items[].spec.volumeName}' pvc-63355afb-ce61-46c4-8781-377cf962278a Recover the fluentd StatefulSet and downloader Deployment. $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=1 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=1 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled ","keywords":"","version":"v1.3 (latest)"},{"title":"Upgrading Harvester","type":0,"sectionRef":"#","url":"/v1.3/upgrade/index","content":"Upgrading Harvester Upgrade support matrix​ The following table shows the upgrade path of all supported versions. Upgrade from version\tSupported new version(s)v1.1.2/v1.2.0\tv1.2.1 v1.1.1/v1.1.2\tv1.1.3 v1.1.0/v1.1.1\tv1.1.2 Rancher upgrade​ If you are using Rancher to manage your Harvester cluster, we recommend upgrading your Rancher server first. For more information, please refer to the Rancher upgrade guide. For the Harvester &amp; Rancher support matrix, please visit our website here. note Upgrading Rancher will not automatically upgrade your Harvester cluster. You still need to upgrade your Harvester cluster after upgrading Rancher.Upgrading Rancher will not bring your Harvester cluster down. You can still access your Harvester cluster using its virtual IP. Start an upgrade​ caution Before you upgrade your Harvester cluster, we highly recommend: Back up your VMs if needed. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 30 GiB of free system partition space (df -h /usr/local/). If any node in the cluster has less than 30 GiB of free system partition space, the upgrade will be denied. Check free system partition space requirement for more information.Run the pre-check script on a Harvester control-plane node. Please pick a script according to your cluster's version: https://github.com/harvester/upgrade-helpers/tree/main/pre-check. caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Prepare an air-gapped upgrade​ caution Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages. Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso. Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml. Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~&gt; sudo -i rancher@node1:~&gt; kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page. Free system partition space requirement​ Available as of v1.2.0 The minimum free system partition space requirement in Harvester v1.2.0 is 30 GiB, which will be revised in each release. Harvester will check the amount of free system partition space on each node when you select Upgrade. If any node does not meet the requirement, the upgrade will be denied as follows If some nodes do not have enough free system partition space, but you still want to try upgrading, you can customize the upgrade by updating the harvesterhci.io/minFreeDiskSpaceGB annotation of Version object. apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: annotations: harvesterhci.io/minFreeDiskSpaceGB: &quot;30&quot; # the value is pre-defined and may be customized name: 1.2.0 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://192.168.0.181:8000/harvester-master-amd64.iso minUpgradableVersion: 1.1.2 releaseDate: &quot;20230609&quot; caution Setting a smaller value than the pre-defined value may cause the upgrade to fail and is not recommended in a production environment.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v1.3 (latest)"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.3/troubleshooting/monitoring","content":"Monitoring The following sections contain tips to troubleshoot Harvester Monitoring. Monitoring is unusable​ When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons. Monitoring is unusable due to Pod being stuck in Terminating status​ Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0/3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1/1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod &quot;prometheus-rancher-monitoring-prometheus-0&quot; force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ... Expand PV/Volume Size​ Harvester integrates Longhorn as the default storage provider. Harvester Monitoring uses Persistent Volume (PV) to store running data. When a cluster has been running for a certain time, the Persistent Volume may need to expand its size. Based on the Longhorn Volume expansion guide, Harvester illustrates how to expand the volume size. View Volume​ From Embedded Longhorn WebUI​ Access the embedded Longhorn WebUI according to this document. The Longhorn dashboard default view. Click Volume to list all existing volumes. From CLI​ You can also use kubectl to get all Volumes. # kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 5Gi RWO harvester-longhorn 43h cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 50Gi RWO harvester-longhorn 43h cattle-monitoring-system rancher-monitoring-grafana Bound pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 2Gi RWO harvester-longhorn 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 attached degraded 2147483648 harv31 43h Scale Down a Deployment​ To detach the Volume, you need to scale down the deployment that uses the Volume. The example below is against the PVC claimed by rancher-monitoring-grafana. Find the deployment in the namespace cattle-monitoring-system. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // target deployment rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h Scale down the deployment rancher-monitoring-grafana to 0. # kubectl scale --replicas=0 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment and the volume. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 0/0 0 0 43h // scaled down rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 detached unknown 2147483648 43h // volume is detached Expand Volume​ In the Longhorn WebUI, the related volume becomes Detached. Click the icon in the Operation column, and select Expand Volume. Input a new size, and Longhorn will expand the volume to this size. Scale Up a Deployment​ After the Volume is expanded to target size, you need to scale up the aforementioned deployment to its original replicas. For the above example of rancher-monitoring-grafana, the original replicas is 1. # kubectl scale --replicas=1 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment again. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // scaled up rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h The Volume is attached to the new POD. To now, the Volume is expanded to the new size and the POD is using it smoothly. Fail to Enable rancher-monitoring Addon​ You may encounter this when you install the Harvester v1.3.0 or higher version cluster with the minimal 250 GB disk per hardware requirements. Reproduce Steps​ Install the Harvester v1.3.0 cluster. Enable the rancher-monitoring addon, you will observe: The POD prometheus-rancher-monitoring-prometheus-0 in cattle-monitoring-system namespace fails to start due to PVC attached failed. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE alertmanager-rancher-monitoring-alertmanager-0 2/2 Running 0 3m22s helm-install-rancher-monitoring-4b5mx 0/1 Completed 0 3m41s prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 3m21s // stuck in this status rancher-monitoring-grafana-d6f466988-hgpkb 4/4 Running 0 3m26s rancher-monitoring-kube-state-metrics-7659b76cc4-66sr7 1/1 Running 0 3m26s rancher-monitoring-operator-595476bc84-7hdxj 1/1 Running 0 3m25s rancher-monitoring-prometheus-adapter-55dc9ccd5d-pcrpk 1/1 Running 0 3m26s rancher-monitoring-prometheus-node-exporter-pbzv4 1/1 Running 0 3m26s $ kubectl describe pod -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 Name: prometheus-rancher-monitoring-prometheus-0 Namespace: cattle-monitoring-system Priority: 0 Service Account: rancher-monitoring-prometheus ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 3m48s (x3 over 4m15s) default-scheduler 0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.. Normal Scheduled 3m44s default-scheduler Successfully assigned cattle-monitoring-system/prometheus-rancher-monitoring-prometheus-0 to harv41 Warning FailedMount 101s kubelet Unable to attach or mount volumes: unmounted volumes=[prometheus-rancher-monitoring-prometheus-db], unattached volumes=[prometheus-rancher-monitoring-prometheus-db], failed to process volumes=[]: timed out waiting for the condition Warning FailedAttachVolume 90s (x9 over 3m42s) attachdetach-controller AttachVolume.Attach failed for volume &quot;pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0&quot; : rpc error: code = Aborted desc = volume pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 is not ready for workloads $ kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 50Gi RWO harvester-longhorn 7m12s $ kubectl get volume -A NAMESPACE NAME DATA ENGINE STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 v1 detached unknown 53687091200 6m55s The Longhorn manager is unable to schedule the replica. $ kubectl logs -n longhorn-system longhorn-manager-bf65b | grep &quot;pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0&quot; time=&quot;2024-02-19T10:12:56Z&quot; level=error msg=&quot;There's no available disk for replica pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0-r-dcb129fd, size 53687091200&quot; func=&quot;schedule r.(*ReplicaScheduler).ScheduleReplica&quot; file=&quot;replica_scheduler.go:95&quot; time=&quot;2024-02-19T10:12:56Z&quot; level=warning msg=&quot;Failed to schedule replica&quot; func=&quot;controller.(*VolumeController).reconcileVolumeCondition&quot; file=&quot;volume_controller.go:169 4&quot; accessMode=rwo controller=longhorn-volume frontend=blockdev migratable=false node=harv41 owner=harv41 replica=pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0-r-dcb129fd sta te= volume=pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 ... Workaround​ Disable the rancher-monitoring addon if you have alreay enabled it. All pods in cattle-monitoring-system are deleted but the PVCs are retained. For more information, see [Addons]. $ kubectl get pods -n cattle-monitoring-system No resources found in cattle-monitoring-system namespace. $ kubectl get pvc -n cattle-monitoring-system NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-cea6316e-f74f-4771-870b-49edb5442819 5Gi RWO harvester-longhorn 14m prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 50Gi RWO harvester-longhorn 14m Delete the PVC named prometheus, but retain the PVC named alertmanager. $ kubectl delete pvc -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 persistentvolumeclaim &quot;prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0&quot; deleted $ kubectl get pvc -n cattle-monitoring-system NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-cea6316e-f74f-4771-870b-49edb5442819 5Gi RWO harvester-longhorn 16m On the Addons screen of the Harvester UI, select ⋮ (menu icon) and then select Edit YAML. As indicated below, change the two occurrences of the number 50 to 30 under prometheusSpec, and then save. The prometheus feature will use a 30GiB disk to store data. Alternatively, you can use kubectl to edit the object. kubectl edit addons.harvesterhci.io -n cattle-monitoring-system rancher-monitoring retentionSize: 50GiB // Change 50 to 30 storageSpec: volumeClaimTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi // Change 50 to 30 storageClassName: harvester-longhorn Enable the rancher-monitoring addon and wait for a few minutes.. All pods are successfully deployed, and the rancher-monitoring feature is available. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE alertmanager-rancher-monitoring-alertmanager-0 2/2 Running 0 3m52s helm-install-rancher-monitoring-s55tq 0/1 Completed 0 4m17s prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 3m51s rancher-monitoring-grafana-d6f466988-hkv6f 4/4 Running 0 3m55s rancher-monitoring-kube-state-metrics-7659b76cc4-ght8x 1/1 Running 0 3m55s rancher-monitoring-operator-595476bc84-r96bp 1/1 Running 0 3m55s rancher-monitoring-prometheus-adapter-55dc9ccd5d-vtssc 1/1 Running 0 3m55s rancher-monitoring-prometheus-node-exporter-lgb88 1/1 Running 0 3m55s ","keywords":"","version":"v1.3 (latest)"},{"title":"Upgrade from v1.1.1/v1.1.2 to v1.1.3","type":0,"sectionRef":"#","url":"/v1.3/upgrade/v1-1-1-to-v1-1-3","content":"Upgrade from v1.1.1/v1.1.2 to v1.1.3 General information​ An Upgrade button appears on the Dashboard screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see Start an upgrade. For air-gapped environments, see Prepare an air-gapped upgrade. Known Issues​ 1. The upgrade process is stuck when pre-draining a node. (Case 1)​ Starting from v1.1.0, Harvester waits for all volumes to become healthy before upgrading a node (for clusters with three or more nodes). When this issue occurs, you can check the health of the affected volumes on the embedded Longhorn UI. You can also check the pre-drain job logs. For more troubleshooting information, see Phase 4: Upgrade nodes. 2. The upgrade process is stuck when pre-draining a node. (Case 2)​ An upgrade is stuck, as shown in the screenshot below: Harvester is unable to proceed with the upgrade and the status of two or more nodes is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. The upgrade process is stuck on the first node.​ Harvester attempts to upgrade the first node but is unable to proceed. The upgrade eventually fails because the job is not completed by the expected end time. Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. The status of a Fleet bundle after the upgrade indicates that deployment errors occurred.​ After an upgrade is completed, the status of a bundle managed by Fleet may be ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]. The errors that occurred while deploying the bundle may block the next Harvester upgrade or managedChart update if not addressed. To check the status of bundles, run the following command: kubectl get bundles -A The following output indicates that the issue exists in your cluster. NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. The upgrade process stops after the upgrade repository is created.​ Harvester is unable to retrieve the harvester-release.yaml file and proceed with the upgrade. The following error message is displayed: Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)context deadline exceeded (Client.Timeout exceeded while awaiting headers)` message: This issue was fixed in v1.1.2. For v1.1.0 and v1.1.1 users, however, the workaround is to restart the upgrade process. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. The upgrade is stuck in the &quot;Pre-drained&quot; state.​ This issue could be caused by a misconfigured pod disruption budget (PDB). You can perform the following steps to confirm the cause and use the current workaround. In this example, the affected node is harvester-node-1. Check the name of the instance-manager-e or instance-manager-r pod on the node. $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output shows that the instance-manager-r-d4ed2788 pod is on the node. Check the Rancher logs and verify that the instance-manager-e or instance-manager-r pod cannot be drained. $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check if a PDB is associated with the node. $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager for the associated PDB. $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output does not show the affected node, the issue exists in your cluster. In this example, the output shows harvester-node-2 instead of harvester-node-1. Check the health of all volumes. kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should show that all volumes are marked healthy. If not, consider uncordoning nodes to improve volume health. Remove the misconfigured PDB. kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.3 (latest)"},{"title":"Upgrade from v1.1.0/v1.1.1 to v1.1.2","type":0,"sectionRef":"#","url":"/v1.3/upgrade/v1-1-to-v1-1-2","content":"Upgrade from v1.1.0/v1.1.1 to v1.1.2 danger Please do not upgrade a running cluster to v1.1.2 if your machine has an Intel E810 NIC card. We saw some reports that the NIC card has a problem when added to a bonding device. Please check this issue for more info: https://github.com/harvester/harvester/issues/3860. General information​ Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known Issues​ 1. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 2. An upgrade is stuck when pre-draining a node (case 2)​ An upgrade is stuck, as shown in the screenshot below: And you can also observe that multiple nodes' status is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. After an upgrade, a fleet bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]​ There is a chance fleet-managed bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] after an upgrade. To check if this happened, run the following command: kubectl get bundles -A If you see the following output, it's possible that your cluster has hit the issue: NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. An upgrade stops because it can't retrieve the harvester-release.yaml file​ An upgrade is stopped with the Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) message: We have fixed this issue in v1.1.2. But for v1.1.0 and v1.1.1 users, the workaround is to start over an upgrade. Please refer to Start over an upgrade. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: This could be caused by a misconfigured PDB. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.3 (latest)"},{"title":"Upgrade from v1.1.2/v1.2.0 to v1.2.1","type":0,"sectionRef":"#","url":"/v1.3/upgrade/v1-2-0-to-v1-2-1","content":"Upgrade from v1.1.2/v1.2.0 to v1.2.1 Important changes to this version​ Harvester v1.2.1 fixes upgrade known issues found in v1.2.0, we suggest upgrading v1.1.2 and v1.2.0 clusters to v1.2.1. The known issues found in v1.2.0 are: An Upgrade is stuck in the Post-draining stateAn upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent For clusters already upgraded to v1.2.0, please refer to the release note for new changes. General information​ tip Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this URL for the script. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ Please check v1.2.0 known issues.","keywords":"","version":"v1.3 (latest)"},{"title":"Upload Images","type":0,"sectionRef":"#","url":"/v1.3/upload-image","content":"Upload Images Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL​ To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File​ Currently, qcow2, raw, and ISO images are supported. note Please do not refresh the page until the file upload is finished. HTTP 413 Error in Rancher Multi-Cluster Management​ You can upload images from the Multi-Cluster Management screen on the Rancher UI. When the status of an image is Uploading but the progress indicator displays 0% for an extended period, check the HTTP response status code. 413 indicates that the size of the request body exceeds the limit. The maximum request body size should be specific to the cluster that is hosting Rancher (for example, RKE2 clusters have a default limit of 1 MB but no such limit exists in K3s clusters). The current workaround is to upload images from the Harvester UI. If you choose to upload images from the Rancher UI, you may need to configure related settings on the ingress server (for example, proxy-body-size in NGINX). If Rancher is deployed on an RKE2 cluster, perform the following steps: Edit the Rancher ingress. $ kubectl -n cattle-system edit ingress rancher Specify a value for nginx.ingress.kubernetes.io/proxy-body-size. Example: Delete the stuck image, and then restart the upload process. Create Images via Volumes​ On the Volumes page, click Export Image. Enter the image name and select a StorageClass to create an image. Image StorageClass​ When creating an image, you can select a StorageClass and use its pre-defined parameters like replicas, node selectors and disk selectors . note The image will not use the StorageClass selected here directly. It's just a StorageClass template. Instead, it will create a special StorageClass under the hood with a prefix name of longhorn-. This is automatically done by the Harvester backend, but it will inherit the parameters from the StorageClass you have selected. Image Labels​ You can add labels to the image, which will help identify the OS type more accurately. Also, you can add any custom labels for filtering if needed. If your image name or URL contains any valid information, the UI will automatically recognize the OS type and image category for you. If not, you can also manually specify those corresponding labels on the UI.","keywords":"Harvester harvester Rancher rancher Import Images","version":"v1.3 (latest)"},{"title":"Access to the Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/vm/access-to-the-vm","content":"Access to the Virtual Machine Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI​ VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, e.g., the Ubuntu-Minimal-Cloud image, the VM can only be accessed with the serial console. SSH Access​ Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection, which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection, which allows keys or basic auth to be updated dynamically at runtime. Static SSH Key Injection via cloud-init​ You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place. Example of SSH key cloud-init configuration:​ #cloud-config ssh_authorized_keys: - &gt;- ssh-rsa #replace with your public key Dynamic SSH Key Injection via Qemu guest agent​ Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the qemuGuestAgent propagation method. note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click ⋮ button.Click the Edit Config button and go to the Access Credentials tab.Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE).Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI. Access with the SSH Client​ Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@&lt;ip-address-or-hostname&gt; ","keywords":"Harvester harvester Rancher rancher Access to the VM","version":"v1.3 (latest)"},{"title":"Clone VM","type":0,"sectionRef":"#","url":"/v1.3/vm/clone-vm","content":"Clone VM Available as of v1.1.0 VM can be cloned with/without data. This function doesn't need to take a VM snapshot or set up a backup target first. Clone VM with volume data​ On the Virtual Machines page, click Clone of the VM actions.Set a new VM name and click Create to create a new VM. Clone VM without volume data​ Cloning a VM without volume data creates a new VM with the same configuration as the source VM. On the Virtual Machines page, click Clone of the VM actions.Unclick the clone volume data checkbox.Set a new VM name and click Create to create a new VM.","keywords":"Harvester harvester Rancher rancher Clone VM","version":"v1.3 (latest)"},{"title":"VM Backup, Snapshot & Restore","type":0,"sectionRef":"#","url":"/v1.3/vm/backup-restore","content":"VM Backup, Snapshot &amp; Restore VM Backup &amp; Restore​ Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. note A backup target must be set up. For more information, see Configure Backup Target. If the backup target has not been set, you’ll be prompted with a message to do so. Configure Backup Target​ A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings &gt; backup-target. Parameter\tType\tDescriptionType\tstring\tChoose S3 or NFS Endpoint\tstring\tA hostname or an IP address. It can be left empty for AWS S3. BucketName\tstring\tName of the bucket BucketRegion\tstring\tRegion of the bucket AccessKeyID\tstring\tA user-id that uniquely identifies your account SecretAccessKey\tstring\tThe password to your account Certificate\tstring\tPaste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle\tbool\tUse VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup​ Once the backup target is set, go to the Virtual Machines page.Click Take Backup of the VM actions to create a new VM backup.Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Backup &amp; Snapshot &gt; VM Backups page to view all VM backups. The State will be set to Ready once the backup is complete. Users can either restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup​ To restore a new VM from a backup, follow these steps: Go to the VM Backups page.Click the Restore Backup button at the top right.Specify the new VM name and click Create.A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a backup​ You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the VM Backups page.Click the Restore Backup button at the top right.Click Replace Existing.You can view the restore process from the Virtual Machines page. Restore a new VM on another Harvester cluster​ Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata &amp; content backup feature. prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster​ Check the existing image name (normally starts with image-) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: &quot;&quot; pvcNamespace: &quot;&quot; sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster​ Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster.Go to the VM Backups page.Select the synced VM backup metadata and choose to restore a new VM with a specified VM name.A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page. VM Snapshot &amp; Restore​ Available as of v1.1.0 VM snapshots are created from the Virtual Machines page. The VM snapshot volumes will be stored in the cluster, and they can be used to either restore a new VM or replace an existing VM. Create a VM snapshot​ Go to the Virtual Machines page.Click Take VM Snapshot of the VM actions to create a new VM snapshot.Set a custom snapshot name and click Create to create a new VM snapshot. Result: The snapshot is created. You can also go to the Backup &amp; Snapshot &gt; VM Snapshots page to view all VM snapshots. The State will be set to Ready once the snapshot is complete. Users can either restore a new VM or replace an existing VM using this snapshot. Restore a new VM using a snapshot​ To restore a new VM from a snapshot, follow these steps: Go to the VM Snapshots page.Click the Restore Snapshot button at the top right.Specify the new VM name and click Create.A new VM will be restored using the snapshot volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a snapshot​ You can replace an existing VM using the snapshot. note You can only choose to retain the previous volumes. Go to the VM Snapshots page.Click the Restore Snapshot button at the top right.Click Replace Existing.You can view the restore process from the Virtual Machines page.","keywords":"Harvester harvester Rancher rancher VM Backup  Snapshot & Restore","version":"v1.3 (latest)"},{"title":"Hot-Plug Volumes","type":0,"sectionRef":"#","url":"/v1.3/vm/hotplug-volume","content":"Hot-Plug Volumes Harvester supports adding hot-plug volumes to a running VM. info Currently, KubeVirt only supports disk bus scsi for hot-plug volumes. For more information, see this issue. Adding Hot-Plug Volumes to a Running VM​ The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select ⋮ &gt; Add Volume. Enter the Name and select the Volume. Click Apply.","keywords":"Harvester Hot-plug Volume","version":"v1.3 (latest)"},{"title":"Edit a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/vm/edit-vm","content":"Edit a Virtual Machine How to Edit a VM​ After creating a virtual machine, you can edit your virtual machine by clicking the ⋮ button and selecting the Edit Config button. note In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect. Basics​ On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS. Networks​ You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server. For more details about the network implementation, please refer to the Networking page. Volumes​ You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, edit the size of the expanded volume. After restarting the VM and waiting for the resize to complete, your disk will automatically finish expanding. Access Credentials​ Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has QEMU guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Edit a VM","version":"v1.3 (latest)"},{"title":"Upgrade from v1.1.2 to v1.2.0 (not recommended)","type":0,"sectionRef":"#","url":"/v1.3/upgrade/v1-1-2-to-v1-2-0","content":"Upgrade from v1.1.2 to v1.2.0 (not recommended) caution Due to the known issues found v1.2.0: An Upgrade is stuck in the Post-draining stateAn upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent We don't recommend upgrading to v1.2.0. Please upgrade your v1.1.x cluster to v1.2.1. General information​ tip Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this URL for the script. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. An upgrade can't start and reports &quot;validator.harvesterhci.io&quot; denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready​ If a cluster is configured with a storage network, an upgrade can't start with the following message. Related issue: [Doc] upgrade stuck while upgrading system service with alertmanager and prometheus Workaround: https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192 2. An upgrade is stuck in Creating Upgrade Repository​ During an upgrade, Creating Upgrade Repository is stuck in the Pending state: Please perform the following steps to check if the cluster runs into the issue: Check the upgrade repository pod: If the virt-launcher-upgrade-repo-hvst-&lt;upgrade-name&gt; pod stays in ContainerCreating, your cluster might have run into this issue. In this case, proceed with step 2. Check the upgrade repository volume in the Longhorn GUI. Go to Longhorn GUI. Navigate to the Volume page. Check the upgrade repository VM volume. It should be attached to a pod called virt-launcher-upgrade-repo-hvst-&lt;upgrade-name&gt;. If one of the volume's replicas stays in Stopped (gray color), the cluster is running into the issue. Related issue: [BUG] upgrade stuck on create upgrade VM Workaround: Delete the Stopped replica from Longhorn GUI. Or,Start over the upgrade. 3. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 5. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: In this stage, Kubernetes is supposed to drain the workload on the node, but some reasons might cause the process to stall. 5.1 The node contains a Longhorn instance-manager-r pod that serves single-replica volume(s)​ Longhorn doesn't allow draining a node if the node contains the last surviving replica of a volume. To check if a node is running into this situation, follow these steps: List single-replica volumes with the command: kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + &quot;/&quot; + .metadata.name' For example: $ kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + &quot;/&quot; + .metadata.name' longhorn-system/pvc-d1f19bab-200e-483b-b348-c87cfbba85ab Check if the replica resides on the stuck node: List the NodeID of the volume's replica with the command: kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == &quot;&lt;volume&gt;&quot;) | .spec.nodeID' For example: $ kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == &quot;pvc-d1f19bab-200e-483b-b348-c87cfbba85ab&quot;) | .spec.nodeID' node1 If the result shows that the replica resides on the node where the upgrade is stuck (in this example, node1), your cluster is hitting this issue. There are a couple of ways to address this situation. Choose the most appropriate method for your VM: Shut down the VM that uses the single-replica volume to detach the volume, allowing the upgrade to continue.Adjust the volumes's replicas to more than one. Go to Longhorn GUI.Go to the Volume page.Locate the problematic volume and click the icon on the right side, then select Update Replicas Count:Increase the Number of Replicas and select OK. 5.2 Misconfigured Longhorn instance-manager-r Pod Disruption Budgets (PDB)​ A misconfigured PDB could cause this issue. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4 5.3 The instance-manager-e pod could not be drained​ During an upgrade, you might encounter an issue where you can't drain the instance-manager-e pod. When this situation occurs, you will see error messages in the Rancher logs like the ones shown below: $ kubectl logs deployment/rancher -n cattle-system | grep &quot;evicting pod&quot; evicting pod longhorn-system/instance-manager-r-a06a43f3437ab4f643eea7053b915a80 evicting pod longhorn-system/instance-manager-e-452e87d2 error when evicting pods/&quot;instance-manager-r-a06a43f3437ab4f643eea7053b915a80&quot; -n &quot;Longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. error when evicting pods/&quot;instance-manager-e-452e87d2&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check the instance-manager-e to see if any engine instances remain. $ kubectl get instancemanager instance-manager-e-452e87d2 -n longhorn-system -o yaml | yq -e &quot;.status.instances&quot; pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57: spec: name: pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57 status: endpoint: &quot;&quot; errorMsg: &quot;&quot; listen: &quot;&quot; portEnd: 10001 portStart: 10001 resourceVersion: 0 state: running type: &quot;&quot; In this example, the instance-manager-e-452e87d2 still has an engine instance, so you can't drain the pod. You need to check the engine numbers to see if any engine number is redundant. Each PVC should only have one engine. # kubectl get engines -n longhorn-system -l longhornvolume=pvc-7b120d60-1577-4716-be5a-62348271025a NAME STATE NODE INSTANCEMANAGER IMAGE AGE pvc-76120d60-1577-4716-be5a-62348271025a-e-08220662 running harvester-qv4hd instance-manager-e-625d715e2f2e7065d64339f9b31407c2 longhornio/longhorn-engine:v1.4.3 2d12h pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 running harvester-lhlkv instance-manager-e-452e87d2 longhornio/longhorn-engine:v1.4.3 4d10h The example above shows that two engines exist for the same PVC, which is a known issue in Longhorn #6642. To resolve this, delete the redundant engine to allow the upgrade to continue. To determine which engine is the correct one, use the following command: $ kubectl get volumes pvc-7b120d60-1577-4716-be5a-62348271025a -n longhorn-system NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-7b120d60-1577-4716-be5a-62348271025a attached healthy 42949672960 harvester-q4vhd 4d10h In this example, the volume pvc-7b120d60-1577-4716-be5a-62348271025a is active on the node harvester-q4vhd, indicating that the engine not running on this node is redundant. To make the engine inactive and trigger its automatic deletion by Longhorn, run the following command: $ kubectl patch engine pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 -n longhorn-system --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/active&quot;, &quot;value&quot;: false}]' engine.longhorn.io/pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 patched After a few seconds, you can verify the engine's status: $ kubectl get engine -n longhorn-system|grep pvc-7b120d60-1577-4716-be5a-62348271025a pvc-7b120d60-1577-4716-be5a-62348271025a-e-08220b62 running harvester-q4vhd instance-manager-e-625d715e2f2e7065d64339f9631407c2 longhornio/longhorn-engine:v1.4.3 2d13h The instance-manager-e pod should now drain successfully, allowing the upgrade to proceed. Related issue: [BUG] Upgrade (v1.1.2 -&gt; v1.2.0-rc6) stuck in pre-drained 6. An upgrade is stuck in the Upgrading System Service state​ If you notice the upgrade is stuck in the Upgrading System Service state for a long period of time, you might need to investigate if the upgrade is stuck in the apply-manifests phase. POD prometheus-rancher-monitoring-prometheus-0 is to be deleted​ Check the log of the apply-manifests pod to see if the following messages repeat. $ kubectl -n harvester-system logs hvst-upgrade-md6wr-apply-manifests-wqslg --tail=10 Tue Sep 5 10:20:39 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:45 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:50 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:55 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:21:00 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Check if the prometheus-rancher-monitoring-prometheus-0 pod is stuck with the status Terminating. $ kubectl -n cattle-monitoring-system get pods NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Terminating 0 19d Find the UID of the terminating pod with the following command: $ kubectl -n cattle-monitoring-system get pod prometheus-rancher-monitoring-prometheus-0 -o jsonpath='{.metadata.uid}' 33f43165-6faa-4648-927d-69097901471c Get access to any node of the cluster via the console or SSH. Search for the related log messages in /var/lib/rancher/rke2/agent/logs/kubelet.log using the pod's UID. E0905 10:26:18.769199 17399 reconciler.go:208] &quot;operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : UnmountVolume.NewUnmounter failed for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory&quot; err=&quot;UnmountVolume.NewUnmounter failed for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory&quot; If kubelet continues to complain about the volume failing to unmount, apply the following workaround to allow the upgrade to proceed. Forcibly remove the pod stuck with the status Terminating with the following command: kubectl delete pod prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system --force Related issue [BUG] The rancher-monitoring Pod stuck at terminating status when upgrading from v1.1.2 to v1.2.0-rc6 Multiple PODs in cattle-monitoring-system namespace are to be deleted​ Check the log of the apply-manifests pod to see if the following messages repeat. there are still 10 pods in cattle-monitoring-system to be deleted Fri Dec 8 19:06:56 UTC 2023 there are still 10 pods in cattle-monitoring-system to be deleted Fri Dec 8 19:07:01 UTC 2023 When it continues to show 10 (or other number) pods, it encounters below issue. The monitoring feature is deployed from the rancher-monitoring ManagedChart, in Harvester v1.2.0,v1.2.1, this ManagedChart is converted to Harvester Addon feature when upgrading. The ManagedChart rancher-monitoring is deleted, normally, all the generated resources including deployment, daemonset etc. will be deleted automatically. But in this case, those resources are not deleted. The above log reflects the result. Following instructions will guide to delete them manually. Locate the affected resources in the cattle-monitoring-system namespace. Root level resources in cattle-monitoring-system Customized CRD: Prometheus Object: rancher-monitoring-prometheus Sub-object: statefulset.apps/prometheus-rancher-monitoring-prometheus Customized CRD: Alertmanager object: rancher-monitoring-alertmanager Sub-object: statefulset.apps/alertmanager-rancher-monitoring-alertmanager Deployment: rancher-monitoring-grafana rancher-monitoring-kube-state-metrics rancher-monitoring-operator rancher-monitoring-prometheus-adapter Daemonset: rancher-monitoring-prometheus-node-exporter Delete the affected resources. Use below commands to delete them, meanwhile check the log of the `apply-manifests` until it does not report `there are still x pods in cattle-monitoring-system to be deleted`. kubectl delete prometheus rancher-monitoring-prometheus -n cattle-monitoring-system kubectl delete alertmanager rancher-monitoring-alertmanager -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-grafana -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-kube-state-metrics -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-operator -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-prometheus-adapter -n cattle-monitoring-system kubectl delete daemonset rancher-monitoring-prometheus-node-exporter -n cattle-monitoring-system note You may need to run some of the commands more than once to completely delete the resources. Related issue [BUG] upgrade hung on apply-manifests 7. Upgrade stuck in the Upgrading System Service state​ If an upgrade is stuck in an Upgrading System Service state for an extended period, some system services' certificates may have expired. To investigate and resolve this issue, follow these steps: Find the apply-manifest job's name with the command: kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest Example output: NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s Check the job's log with the command: kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system If the following messages appear in the log, continue to the next step: Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Check CAPI cluster's state with the command: kubectl get clusters.provisioning.cattle.io local -n fleet-local -o yaml If you see a condition similar to the one below, it's likely that the cluster has encountered the issue: - lastUpdateTime: &quot;2023-01-17T16:26:48Z&quot; message: 'configuring bootstrap node(s) custom-24cb32ce8387: waiting for probes: kube-controller-manager, kube-scheduler' reason: Waiting status: Unknown type: Updated Find the machine's hostname with the following command, and follow the workaround to see if service certificates expire on a node: kubectl get machines.cluster.x-k8s.io -n fleet-local &lt;machine_name&gt; -o yaml | yq .status.nodeRef.name Replace &lt;machine_name&gt; with the machine's name from the output in the previous step. note If multiple nodes joined the cluster around the same time, you should perform the workaround on all those nodes. Related issue: [DOC/ENHANCEMENT] need to add cert-rotate feature, otherwise upgrade may stuck on Waiting for CAPI cluster fleet-local/local to be provisioned Workaround: https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311 8. The registry.suse.com/harvester-beta/vmdp:latest image is not available in air-gapped environment​ Harvester does not package the registry.suse.com/harvester-beta/vmdp:latest image in the ISO file as of v1.1.0. For Windows VMs before v1.1.0, they used this image as a container disk. However, kubelet may remove old images to free up bytes. Windows VMs can't access an air-gapped environment when this image is removed. You can fix this issue by changing the image to registry.suse.com/suse/vmdp/vmdp:2.5.4.2 and restarting the Windows VMs. Related issue: [BUG] VMDP Image wrong after upgrade to Harvester 1.2.0 9. An Upgrade is stuck in the Post-draining state​ note This known issue is fixed in v1.2.1. The node might be stuck in the OS upgrade process if you encounter the Post-draining state, as shown below. Harvester uses elemental upgrade to help us upgrade the OS. Check the elemental upgrade logs to see if there are any errors. You can check the elemental upgrade logs with the following commands: # View the post-drain job, which should be named `hvst-upgrade-xxx-post-drain-xxx` $ kubectl get pod --selector=harvesterhci.io/upgradeJobType=post-drain -n harvester-system # Check the logs with the following command $ kubectl logs -n harvester-system pods/hvst-upgrade-xxx-post-drain-xxx Suppose you see the following error in the logs. An incomplete state.yaml causes this issue. Flag --directory has been deprecated, 'directory' is deprecated please use 'system' instead INFO[2023-09-13T12:02:42Z] Starting elemental version 0.3.1 INFO[2023-09-13T12:02:42Z] reading configuration form '/tmp/tmp.N6rn4F6mKM' ERRO[2023-09-13T12:02:42Z] Invalid upgrade command setup undefined state partition elemental upgrade failed with return code: 33 + ret=33 + '[' 33 '!=' 0 ']' + echo 'elemental upgrade failed with return code: 33' + cat /host/usr/local/upgrade_tmp/elemental-upgrade-20230913120242.log In this case, Harvester upgrades the elemental-cli to the latest version. It will try to find the state partition from the state.yaml. If the state.yaml is incomplete, there is a chance it will fail to find the state partition. The incomplete state.yaml will look like the following. # Autogenerated file by elemental client, do not edit date: &quot;2023-09-13T08:31:42Z&quot; state: # we are missing `label` here. active: source: dir:///tmp/tmp.01deNrXNEC label: COS_ACTIVE fs: ext2 passive: null Remove this incomplete state.yaml file to work around this issue. (The post-draining will retry every 10 minutes). Remount the state partition to RW. $ mount -o remount,rw /run/initramfs/cos-state Remove the state.yaml. $ rm -f /run/initramfs/cos-state/state.yaml Remount the state partition to RO. $ mount -o remount,ro /run/initramfs/cos-state After performing the steps above, you should pass post-draining with the next retry. Related issues: [BUG] Upgrade stuck with first node in Post-draining stateA potential bug in NewElementalPartitionsFromList which caused upgrade error code 33 Workaround: https://github.com/harvester/harvester/issues/4526#issuecomment-1732853216 10. An upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent​ note This known issue is fixed in v1.2.1. If an upgrade is stuck in an Upgrading System Service state for an extended period, follow these steps to investigate this issue: Find the pods related to the upgrade: kubectl get pods -A | grep upgrade Example output: # kubectl get pods -A | grep upgrade cattle-system system-upgrade-controller-5685d568ff-tkvxb 1/1 Running 0 85m harvester-system hvst-upgrade-vq4hl-apply-manifests-65vv8 1/1 Running 0 87m // waiting for managedchart to be ready .. The pod hvst-upgrade-vq4hl-apply-manifests-65vv8 has the following loop log: Current version: 102.0.0+up40.1.2, Current state: WaitApplied, Current generation: 23 Sleep for 5 seconds to retry Check the status for all bundles. Note thata couple of bundles are OutOfSync: # kubectl get bundle -A NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS ... fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-logging-crd 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-monitoring 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-monitoring-crd 0/1 WaitApplied(1) [Cluster fleet-local/local] The pod fleet-agent-* has following error log: fleet-agent pod log: time=&quot;2023-09-19T12:18:10Z&quot; level=error msg=&quot;Failed to register agent: looking up secret cattle-fleet-local-system/fleet-agent-bootstrap: Post \\&quot;https://192.168.122.199/apis/fleet.cattle.io/ v1alpha1/namespaces/fleet-local/clusterregistrations\\&quot;: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.122.199 because it doesn't contain any IP SANs&quot; Check the ssl-certificates settings in Harvester: From the command line: # kubectl get settings.harvesterhci.io ssl-certificates NAME VALUE ssl-certificates {&quot;publicCertificate&quot;:&quot;-----BEGIN CERTIFICATE-----\\nMIIFNDCCAxygAwIBAgIUS7DoHthR/IR30+H/P0pv6HlfOZUwDQYJKoZIhvcNAQEL\\nBQAwFjEUMBIGA1UEAwwLZXhhbXBsZS5j....&quot;} From the Harvester Web UI: Check the server-url setting, it is the value of VIP: # kubectl get settings.management.cattle.io -n cattle-system server-url NAME VALUE server-url https://192.168.122.199 The root cause: User sets the self-signed ssl-certificates with FQDN in the Harvester settings, but the server-url points to the VIP, the fleet-agent pod fails to register. For example: create self-signed certificate for (*).example.com openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \\ -keyout example.key -out example.crt -subj &quot;/CN=example.com&quot; \\ -addext &quot;subjectAltName=DNS:example.com,DNS:*.example.com&quot; The general outputs are: example.crt, example.key The workaround: Update server-url with the value of https://harv31.example.com # kubectl edit settings.management.cattle.io -n cattle-system server-url setting.management.cattle.io/server-url edited ... # kubectl get settings.management.cattle.io -n cattle-system server-url NAME VALUE server-url https://harv31.example.com After the workaround is applied, the fleet-agent pod is replaced by Rancher automatically and registers successfully, the upgrade continues. Related issue: [BUG] Upgrade to Harvester 1.2.0 fails in fleet-agent due to customer provided SSL certificate without IP SAN Workaround: https://github.com/harvester/harvester/issues/4519#issuecomment-1727132383","keywords":"","version":"v1.3 (latest)"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/vm/index","content":"Create a Virtual Machine How to Create a VM​ You can create one or more virtual machines from the Virtual Machines page. note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances.Select the namespace of your VMs, only the harvester-public namespace is visible to all users.The VM Name is a required field.(Optional) VM template is optional, you can choose iso-image, raw-image or windows-iso-image template to speed up your VM instance creation.Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision).Select SSH keys or upload new keys.Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured.You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced &gt; Networks first. (Optional) Set node affinity rules on the Node Scheduling tab.(Optional) Set workload affinity rules on the VM Scheduling tab.Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable. Volumes​ You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type\tdescriptiondisk\tThis type will expose the volume as an ordinary disk to the VM. cd-rom\tThis type will expose the volume as a cd-rom drive to the VM. It is read-only by default. A volume's StorageClass can be specified when adding a new empty volume; for other volumes (such as VM images), the StorageClass is defined during image creation. Adding a container disk​ A container disk is an ephemeral storage volume that can be assigned to any number of VMs and provides the ability to store and distribute VM disks in the container image registry. A container disk is: An ideal tool if you want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Ephemeral volumes are designed for VMs that need more storage but don't care whether that data is stored persistently across VM restarts or only expect some read-only input data to be present in files, like configuration data or secret keys.Not a good solution for any workload that requires persistent root disks across VM restarts. A container disk is added when creating a VM by providing a Docker image. When creating a VM, follow these steps: Go to the Volumes tab.Select Add Container.Enter a Name for the container disk.Choose a disk Type.Add a Docker Image. A disk image, with the format qcow2 or raw, must be placed into the /disk directory.Raw and qcow2 formats are supported, but qcow2 is recommended in order to reduce the container image's size. If you use an unsupported image format, the VM will get stuck in a Running state.A container disk also allows you to store disk images in the /disk directory. An example of creating such a container image can be found here. Choose a Bus type. Networks​ You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type\tdescriptionbridge\tConnect using a Linux bridge masquerade\tConnect using iptables rules to NAT the traffic Management Network​ A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes. Secondary Network​ It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks. In bridge VLAN, virtual machines are connected to the host network through a linux bridge. The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Node Scheduling​ Node Scheduling allows you to constrain which nodes your VMs can be scheduled on based on node labels. See the Kubernetes Node Affinity Documentation for more details. VM Scheduling​ VM Scheduling allows you to constrain which nodes your VMs can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels. For instance, you can combine Required with Affinity to instruct the scheduler to place VMs from two services in the same zone, enhancing communication efficiency. Likewise, the use of Preferred with Anti-Affinity can help distribute VMs of a particular service across multiple zones for increased availability. See the Kubernetes Pod Affinity and Anti-Affinity Documentation for more details. Advanced Options​ Run Strategy​ Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true. RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false. Cloud Configuration​ Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Cloud-init​ Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password: password chpasswd: { expire: False } ssh_pwauth: True Example of network-data configuration using DHCP: network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp You can also use the Advanced &gt; Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM. Installing the QEMU guest agent​ The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service. TPM Device​ Available as of v1.2.0 Trusted Platform Module (TPM) is a cryptoprocessor that secures hardware using cryptographic keys. According to Windows 11 Requirements, the TPM 2.0 device is a hard requirement of Windows 11. In the Harvester UI, you can add an emulated TPM 2.0 device to a VM by checking the Enable TPM box in the Advanced Options tab. note Currently, only non-persistent vTPMs are supported, and their state is erased after each VM shutdown. Therefore, Bitlocker should not be enabled. One-time Boot For ISO Installation​ When creating a VM to boot from cd-rom, you can use the bootOrder option so that the OS can boot from cd-rom during image installation, and boot from the disk when the installation is complete without unmounting the cd-rom. The following example describes how to install an ISO image using openSUSE Leap 15.4: Click Images in the left sidebar and download the openSUSE Leap 15.4 ISO image.Click Virtual Machines in the left sidebar, then create a VM. You need to fill up those VM basic configurations.Click the Volumes tab, In the Image field, select the image downloaded in step 1 and ensure Type is cd-romClick Add Volume and select an existing StorageClass.Drag Volume to the top of Image Volume as follows. In this way, the bootOrder of Volume will become 1. Click Create.Open the VM web-vnc you just created and follow the instructions given by the installer.After the installation is complete, reboot the VM as instructed by the operating system (you can remove the installation media after booting the system).After the VM reboots, it will automatically boot from the disk volume and start the operating system.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Create a VM","version":"v1.3 (latest)"},{"title":"Live Migration","type":0,"sectionRef":"#","url":"/v1.3/vm/live-migration","content":"Live Migration Live migration means moving a virtual machine to a different host without downtime. note Live migration is not allowed when the virtual machine is using a management network of bridge interface type.Live migration is not allowed when the virtual machine has any volume of the CD-ROM type. Such volumes should be ejected before live migration.Live migration is not allowed when the virtual machine has any volume of the Container Disk type. Such volumes should be removed before live migration.Live migration is not allowed when the virtual machine has any PCIDevice passthrough enabled. Such devices need to be removed before live migration. Starting a Migration​ Go to the Virtual Machines page.Find the virtual machine that you want to migrate and select ⋮ &gt; Migrate.Choose the node to which you want to migrate the virtual machine. Click Apply. When you have node scheduling rules configured for a VM, you must ensure that the target nodes you are migrating to meet the VM's runtime requirements. The list of nodes you get to search and select from will be generated based on: VM scheduling rules.Possibly node rules from the network configuration. Aborting a Migration​ Go to the Virtual Machines page.Find the virtual machine in migrating status that you want to abort. Select ⋮ &gt; Abort Migration. Migration Timeouts​ Completion Timeout​ The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout​ Live migration will also be aborted when copying memory doesn't make any progress in 150s.","keywords":"Harvester harvester Rancher rancher Live Migration","version":"v1.3 (latest)"},{"title":"Create a Windows Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/vm/create-windows-vm","content":"Create a Windows Virtual Machine Create one or more virtual machines from the Virtual Machines page. note For creating Linux virtual machines, please refer to this page. How to Create a Windows VM​ Header Section​ Create a single VM instance or multiple VM instances.Set the VM name.(Optional) Provide a description for the VM.(Optional) Select the VM template windows-iso-image-base-template. This template will add a volume with the virtio drivers for Windows. Basics Tab​ Configure the number of CPU cores assigned to the VM.Configure the amount of Memory assigned to the VM. note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. caution The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab​ The first volume is an Image Volume with the following values: Name: The value cdrom-disk is set by default. You can keep it or change it.Type: Select cd-rom.Image: Select the Windows image to be installed. See Upload Images for the full description on how to create new images.Size: The value 20 is set by default. You can change it if your image has a bigger size.Bus: The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name: The value rootdisk is set by default. You can keep it or change it.Type: Select disk.StorageClass: You can use the default StorageClass harvester-longhorn or specify a custom one.Size: The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value.Bus: The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI. The third volume is a Container with the following values: Name: The value virtio-container-disk is set by default. You can keep it or change it.Type: Select cd-rom.Docker Image: The value registry.suse.com/suse/vmdp/vmdp:2.5.4.2 is set by default. We recommend not changing this value.Bus: The value SATA is set by default. We recommend not changing this value. You can add additional disks using the buttons Add Volume, Add Existing Volume, Add VM Image, or Add Container. Networks Tab​ The Management Network is added by default with the following values: Name: The value default is set by default. You can keep it or change it.Model: The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown.Network: The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks.Type: The value masquerade is set by default. You can keep it or change it to the other available option, bridge. You can add additional networks by clicking Add Network. caution Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration. Node Scheduling Tab​ Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab​ OS Type: The value Windows is set by default. It's recommended you don't change it.Machine Type: The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value.(Optional) Hostname: Set the VM hostname.(Optional) Cloud Config: Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs.(Optional) Enable TPM, Booting in EFI mode, Secure Boot: Both the TPM 2.0 device and UEFI firmware with Secure Boot are hard requirements for Windows 11. note Currently, only non-persistent vTPMs are supported, and their state is erased after each VM shutdown. Therefore, Bitlocker should not be enabled. Footer Section​ Once all the settings are in place, click on Create to create the VM. note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML. And if you want to cancel all changes made, click Cancel. Installation of Windows​ Select the VM you just created, and click Start to boot up the VM. Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template, the instruction is as follows: Click on Load driver, and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside.Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows, and click Next to load the driver.Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version\tSupported\tDriver pathWindows 7\tNo\tN/A Windows Server 2008\tNo\tN/A Windows Server 2008r2\tNo\tN/A Windows 8 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly. note For full instructions on how to install the VMDP guest driver and tools see the documentation at https://documentation.suse.com/sle-vmdp/2.5/html/vmdp/index.html Known Issues​ Windows ISO unable to boot when using EFI mode​ When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO. VM crashes when reserved memory not enough​ There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. If 256MiB doesn't work, try 512MiB. BSoD (Blue Screen of Death) at first boot time of Windows​ There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name: Harvester Configuration stages: initramfs: - commands: # ... files: - path: /etc/modprobe.d/kvm.conf permissions: 384 owner: 0 group: 0 content: | options kvm ignore_msrs=1 encoding: &quot;&quot; ownerstring: &quot;&quot; # ... note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","keywords":"Harvester harvester Rancher rancher Windows windows Virtual Machine virtual machine Create a Windows VM","version":"v1.3 (latest)"},{"title":"Resource Overcommit","type":0,"sectionRef":"#","url":"/v1.3/vm/resource-overcommit","content":"Resource Overcommit Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config, this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600%RAM allocation ratio: 150%Storage allocation ratio: 200% note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated. Configure the global setting overcommit-config​ Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced &gt; Settings page. Find the overcommit-config setting. Configure the desired CPU, Memory, and Storage ratio. Configure overcommit for a single virtual machine​ In situations where you require specific configurations for individual virtual machines without affecting the global settings, you can easily achieve this by modifying the spec.template.spec.domain.resources.limits.&lt;memory|cpu&gt; value on the corresponding virtual machine spec directly. Reserve more memory for the system overhead​ By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ... Why my virtual machines are scheduled unevenly?​ The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling.","keywords":"Harvester Overcommit Overprovision ballooning","version":"v1.3 (latest)"},{"title":"Clone a Volume","type":0,"sectionRef":"#","url":"/v1.3/volume/clone-volume","content":"Clone a Volume How to Clone a Volume​ After creating a volume, you can clone the volume by following the steps below: Click the ⋮ button and select the Clone option. Select clone volume data. Configure the Name of the new volume and click Create. (Optional) A cloned volume can be added to a VM using Add Existing Volume.","keywords":"Volume","version":"v1.3 (latest)"},{"title":"Edit a Volume","type":0,"sectionRef":"#","url":"/v1.3/volume/edit-volume","content":"Edit a Volume After creating a volume, you can edit your volume by clicking the ⋮ button and selecting the Edit Config option. Expand a Volume​ You can expand a volume by increasing the value of the Size parameter directly. To prevent the expansion from interference by unexpected data R/W, Harvester supports offline expansion only. You must shut down the VM or detach the volume first if it is attached to a VM, and the detached volume will automatically attach to a random node with maintenance mode to expand automatically. Cancel a Failed Volume Expansion​ If you specify a size larger than Longhorn's capacity during the expansion, the status of the volume expansion will be stuck in Resizing. You can cancel the failed volume expansion by clicking the ⋮ button and selecting the Cancel Expand option. Change the StorageClass of an Existing Volume​ The StorageClass of an existing volume cannot be changed. However, you can change the StorageClass while restoring a new volume from the snapshot by following the steps below: Take a volume snapshot.Select StorageClass when restoring the volume using snapshot.","keywords":"Volume","version":"v1.3 (latest)"},{"title":"Export a Volume to Image","type":0,"sectionRef":"#","url":"/v1.3/volume/export-volume","content":"Export a Volume to Image You can select and export an existing volume to an image by following the steps below: Click the ⋮ button and select the Export Image option. Select the Namespace of the new image. Configure the Name of the new image. Select an existing StorageClass. (Optional) You can download the exported image from the Images page by clicking the ⋮ button and selecting the Download option.","keywords":"Volume","version":"v1.3 (latest)"},{"title":"Create a Volume","type":0,"sectionRef":"#","url":"/v1.3/volume/index","content":"Create a Volume Create an Empty Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose New in Source.Select an existing StorageClass.Configure the Size of the volume. Create an Image Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose VM Image in Source.Select an existing Image.Configure the Size of the volume.","keywords":"Volume","version":"v1.3 (latest)"},{"title":"Volume Snapshots","type":0,"sectionRef":"#","url":"/v1.3/volume/volume-snapshots","content":"Volume Snapshots A volume snapshot represents a snapshot of a volume on a storage system. After creating a volume, you can create a volume snapshot and restore a volume to the snapshot's state. With volume snapshots, you can easily copy or restore a volume's configuration. Create Volume Snapshots​ You can create a volume snapshot from an existing volume by following these steps: Go to the Volumes page. Choose the volume that you want to take a snapshot of and select ⋮ &gt; Take Snapshot. Enter a Name for the snapshot. Select Create to finish creating a new volume snapshot. Check the status of this operation and view all volume snapshots by going to the Volumes page and selecting the Snapshots tab. When the Ready To Use becomes √, the volume snapshot is ready to use. note A recurring snapshot is currently not supported and is tracked via harvester/harvester#572. Restore a new volume from a volume snapshot​ You can restore a new volume from an existing volume snapshot by following these steps: Go to the Backup &amp; Snapshot &gt; Volume Snapshots page or select a Volume from the Volumes page and go to the Snapshots tab. Select ⋮ &gt; Restore. Specify the Name of the new volume. If the source volume is not an image volume, you can select a different StorageClass. You can not change the StorageClass if the source volume is an image volume. Select Create to finish restoring a new volume.","keywords":"Volume Snapshot Volume Snapshots","version":"v1.3 (latest)"},{"title":"Addons","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons","content":"Addons Harvester makes optional functionality available as Addons. One of the key reasons for the same is to ensure that Harvester installation footprint can be kept low while allowing users to enable/disable functionality based on their use case or requirements. Some level of customization is allowed for each addon, which depends on the underlying addon. Available as of v1.1.0 Harvester v1.3.0 ships with six Addons: pcidevices-controllervm-import-controllerrancher-monitoringrancher-loggingharvester-seedernvidia-driver-toolkit note harvester-seeder is released as an experimental feature in Harvester v1.2.0 and has an Experimental label added to the Name. You can enable a Disabled by choosing an addon and selecting ⋮ &gt; Enable from the Basic tab. When the addon is enabled successfully, the State will be DeploySuccessful. You can disable an Enabled by choosing an addon and selecting ⋮ &gt; Disable or from the Basic tab. When the addon is disabled successfully, the State will be Disabled. note When an addon is disabled, the configuration data is stored to reuse when the addon is enabled again.","keywords":"","version":"v1.4 (dev)"},{"title":"Nvidia Driver Toolkit","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons/nvidiadrivertoolkit","content":"Nvidia Driver Toolkit Available as of v1.3.0 nvidia-driver-toolkit is an add-on that allows you to deploy out-of-band NVIDIA GRID KVM drivers to your existing Harvester clusters. note The toolkit only includes the correct Harvester OS image, build utilities, and kernel headers that allow NVIDIA drivers to be compiled and loaded from the container. You must download the NVIDIA KVM drivers using a valid NVIDIA subscription. For guidance on identifying the correct driver for your NVIDIA GPU, see the NVIDIA documentation. The Harvester ISO does not include the nvidia-driver-toolkit container image. Because of its size, the image is pulled from Docker Hub by default. If you have an air-gapped environment, you can download and push the image to your private registry. The Image Repository and Image Tag fields on the nvidia-driver-toolkit screen provide information about the image that you must download. note Each new Harvester version will be released with the correct nvidia-driver-toolkit image to ensure that all dependencies required to install the NVIDIA vGPU KVM drivers are available in the image. To enable the addon, users need to perform the following: Provide the Driver Location: which is an http location where nvidia vgpu kvm driver file is located (as shown in the example)update the Image Repository and Image Tag if needed Once the addon is enabled, a nvidia-driver-toolkit daemonset is deployed to the cluster. On pod startup, the entrypoint script will download the nvidia driver from the speificied Driver Location, install the driver and load the kernel drivers. The PCIDevices addon can now leverage this addon to manage the lifecycle of the vGPU devices on nodes containing supported GPU devices.","keywords":"","version":"v1.4 (dev)"},{"title":"Harvester Overview","type":0,"sectionRef":"#","url":"/v1.4/","content":"Harvester Overview Harvester is a modern, open, interoperable, hyperconverged infrastructure (HCI) solution built on Kubernetes. It is an open-source alternative designed for operators seeking a cloud-native HCI solution. Harvester runs on bare metal servers and provides integrated virtualization and distributed storage capabilities. In addition to traditional virtual machines (VMs), Harvester supports containerized environments automatically through integration with Rancher. It offers a solution that unifies legacy virtualized infrastructure while enabling the adoption of containers from core to edge locations. Harvester Architecture​ The Harvester architecture consists of cutting-edge open-source technologies: Linux OS. Elemental for SLE-Micro 5.3 is at the core of Harvester and is an immutable Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Built on top of Kubernetes. Kubernetes has become the predominant infrastructure language across all form factors, and Harvester is an HCI solution with Kubernetes under the hood.Virtualization management with KubeVirt. KubeVirt provides virtualization management using KVM on top of Kubernetes.Storage management with Longhorn. Longhorn provides distributed block storage and tiering.Observability with Grafana and Prometheus. Grafana and Prometheus provide robust monitoring and logging. Harvester Features​ Harvester is an enterprise-ready, easy-to-use infrastructure platform that leverages local, direct attached storage instead of complex external SANs. It utilizes Kubernetes API as a unified automation language across container and VM workloads. Some key features of Harvester include: Easy to get started. Since Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image or automatically install it using iPXE scripts.VM lifecycle management. Easily create, edit, clone, and delete VMs, including SSH-Key injection, cloud-init, and graphic and serial port console.VM live migration. Move a VM to a different host or node with zero downtime.VM backup, snapshot, and restore. Back up your VMs from NFS, S3 servers, or NAS devices. Use your backup to restore a failed VM or create a new VM on a different cluster.Storage management. Harvester supports distributed block storage and tiering. Volumes represent storage; you can easily create, edit, clone, or export a volume.Network management. Supports using a virtual IP (VIP) and multiple Network Interface Cards (NICs). If your VMs need to connect to the external network, create a VLAN or untagged network.Integration with Rancher. Access Harvester directly within Rancher through Rancher’s Virtualization Management page and manage your VM workloads alongside your Kubernetes clusters. Harvester Dashboard​ Harvester provides a powerful and easy-to-use web-based dashboard for visualizing and managing your infrastructure. Once you install Harvester, you can access the IP address for the Harvester Dashboard from the node's terminal.","keywords":"Harvester harvester Rancher rancher Harvester Intro","version":"v1.4 (dev)"},{"title":"Seeder","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons/seeder","content":"Seeder Available as of v1.2.0 The harvester-seeder addon lets you perform out-of-band operations on underlying nodes. This addon can also discover hardware and hardware events for bare-metal nodes that support redfish-based access and then associate the hardware with the corresponding Harvester nodes. You must enable the harvester-seeder addon from the Addons page to get started. Once the addon is enabled, find the desired host and select Edit Config and go to the Out-Of-Band Access tab. seeder leverages ipmi to manage the underlying node hardware. Hardware discovery and event detection require redfish support. Power operations​ Once you've defined the out-of-band config for a node, you can put the node into Maintenance mode, which allows you to shut down or reboot the node as needed. If a node is shut down, you can also select Power On to power it on again: Hardware event aggregation​ If you've enabled Event in Out-of-Band Access, seeder will leverage redfish to query the underlying hardware for information about component failures and fan temperatures. This information is associated with Harvester nodes and can be used as Kubernetes events. info Sometimes, the Out-Of-Band Access section may be stuck with the message Waiting for &quot;inventories.metal.harvesterhci.io&quot; to be ready. In this case, you need to refresh the page. For more information, see this issue.","keywords":"","version":"v1.4 (dev)"},{"title":"PCI Devices","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons/pcidevices","content":"PCI Devices Available as of v1.1.0 A PCIDevice in Harvester represents a host device with a PCI address. The devices can be passed through the hypervisor to a VM by creating a PCIDeviceClaim resource, or by using the UI to enable passthrough. Passing a device through the hypervisor means that the VM can directly access the device, and effectively owns the device. A VM can even install its own drivers for that device. This is accomplished by using the pcidevices-controller addon. To use the PCI devices feature, users need to enable the pcidevices-controller addon first. Once the pcidevices-controller addon is deployed successfully, it can take a few minutes for it to scan and the PCIDevice CRDs to become available. Enabling Passthrough on a PCI Device​ Now go to the Advanced -&gt; PCI Devices page: Search for your device by vendor name (e.g. NVIDIA, Intel, etc.) or device name. Select the devices you want to enable for passthrough: Then click Enable Passthrough and read the warning message. If you still want to enable these devices, click Enable and wait for all devices to be Enabled. caution Please do not use host-owned PCI devices (e.g., management and VLAN NICs). Incorrect device allocation may cause damage to your cluster, including node failure. Attaching PCI Devices to a VM​ After enabling these PCI devices, you can navigate to the Virtual Machines page and select Edit Config to pass these devices. Select PCI Devices and use the Available PCI Devices drop-down. Select the devices you want to attach from the list displayed and then click Save. Using a passed-through PCI Device inside the VM​ Boot the VM up, and run lspci inside the VM, the attached PCI devices will show up, although the PCI address in the VM won't necessarily match the PCI address in the host. Installing drivers for your PCI device inside the VM​ This is just like installing drivers in the host. The PCI passthrough feature will bind the host device to the vfio-pci driver, which gives VMs the ability to use their own drivers. Here is a screenshot of NVIDIA drivers being installed in a VM. It includes a CUDA example that proves that the device drivers work. SRIOV Network Devices​ Available as of v1.2.0 The pcidevices-controller addon can now scan network interfaces on the underlying hosts and check if they support SRIOV Virtual Functions (VFs). If a valid device is found, pcidevices-controller will generate a new SRIOVNetworkDevice object. To create VFs on a SriovNetworkDevice, you can click ⋮ &gt; Enable and then define the Number of Virtual Functions. The pcidevices-controller will define the VFs on the network interface and report the new PCI device status for the newly created VFs. On the next re-scan, the pcidevices-controller will create the PCIDevices for VFs. This can take up to 1 minute. You can now navigate to the PCI Devices page to view the new devices. We have also introduced a new filter to help you filter PCI devices by the underlying network interface. The newly created PCI device can be passed through to virtual machines like any other PCI device.","keywords":"","version":"v1.4 (dev)"},{"title":"Rancher Manager (Experimental)","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons/rancher-vcluster","content":"Rancher Manager (Experimental) Available as of v1.2.0 The rancher-vcluster addon allows you to run Rancher Manager as a workload on the underlying Harvester cluster and is implemented using vcluster. The addon runs a nested K3s cluster in the rancher-vcluster namespace and deploys Rancher to this cluster. During the installation, the ingress for Rancher is synced to the Harvester cluster, allowing end users to access Rancher. Installing rancher-vcluster​ The rancher-vcluster addon is not packaged with Harvester, but you can find it in the experimental-addons repo. Assuming you are using the Harvester kubeconfig, you can run the following commands to install the addon: kubectl apply -f https://raw.githubusercontent.com/harvester/experimental-addons/main/rancher-vcluster/rancher-vcluster.yaml Configuring rancher-vcluster​ After installing the addon, you need to configure it from the Harvester UI as follows: Select Advanced &gt; Addons.Find the rancher-vcluster addon and select ⋮ &gt; Edit Config. In the Hostname field, enter a valid DNS record pointing to the Harvester VIP. This is essential as the vcluster ingress is synced to the parent Harvester cluster. A valid hostname is used to filter ingress traffic to the vcluster workload.In the Bootstrap Password field, enter the bootstrap password for the new Rancher deployed on the vcluster. Once the addon is deployed, Rancher can take a few minutes to become available. You can then access Rancher via the hostname DNS record that you provided. See Rancher Integration for more information. Disabling rancher-vcluster The rancher-vcluster addon is deployed on a vcluster Statefulset that uses a Longhorn PVC. When rancher-vcluster is disabled, the PVC data-rancher-vcluster-0 will remain in the rancher-vcluster namespace. If you enable the addon again, the PVC is re-used, and Rancher will have the old state available again. If you want to wipe the data, ensure that the PVC is deleted.","keywords":"","version":"v1.4 (dev)"},{"title":"Third-Party Storage Support","type":0,"sectionRef":"#","url":"/v1.4/advanced/csidriver","content":"Third-Party Storage Support Available as of v1.2.0 Harvester now offers the capability to install a Container Storage Interface (CSI) in your Harvester cluster. This allows you to leverage external storage for the Virtual Machine's non-system data disk, allowing you to use different drivers tailored for specific needs, whether for performance optimization or seamless integration with your existing in-house storage solutions. note The Virtual Machine (VM) image provisioner in Harvester still relies on Longhorn. Before version 1.2.0, Harvester exclusively supported Longhorn for storing VM data and did not offer support for external storage as a destination for VM data. Prerequisites​ For the Harvester functions to work well, the third-party CSI driver needs to have the following capabilities: Support expansionSupport snapshotSupport cloneSupport block deviceSupport Read-Write-Many (RWX), for Live Migration Create Harvester cluster​ Harvester's operating system follows an immutable design, meaning that most OS files revert to their pre-configured state after a reboot. Therefore, you might need to perform additional configurations before installing the Harvester cluster for third-party CSI drivers. Some CSI drivers require additional persistent paths on the host. You can add these paths to os.persistent_state_paths. Some CSI drivers require additional software packages on the host. You can install these packages with os.after_install_chroot_commands. note Upgrading Harvester causes the changes to the OS in the after-install-chroot stage to be lost. You must also configure the after-upgrade-chroot to make your changes persistent across an upgrade. Refer to Runtime persistent changes before upgrading Harvester. Install the CSI driver​ After installing the Harvester cluster is complete, refer to How can I access the kubeconfig file of the Harvester cluster? to get the kubeconfig of the cluster. With the kubeconfig of the Harvester cluster, you can install the third-party CSI drivers into the cluster by following the installation instructions for each CSI driver. You must also refer to the CSI driver documentation to create the StorageClass and VolumeSnapshotClass in the Harvester cluster. Configure Harvester Cluster​ Before you can make use of Harvester's Backup &amp; Snapshot features, you need to set up some essential configurations through the Harvester csi-driver-config setting. Follow these steps to make these configurations: Login to the Harvester UI, then navigate to Advanced &gt; Settings.Find and select csi-driver-config, and then select ⋮ &gt; Edit Setting to access the configuration options.Set the Provisioner to the third-party CSI driver in the settings.Next, Configure the Volume Snapshot Class Name. This setting points to the name of the VolumeSnapshotClass used for creating volume snapshots or VM snapshots.Similarly, Configure the Backup Volume Snapshot Class Name. This corresponds to the name of the VolumeSnapshotClass responsible for creating VM backups. Use the CSI driver​ After successfully configuring these settings, you can utilize the third-party StorageClass. You can apply the third-party StorageClass when creating an empty volume or adding a new block volume to a VM, enhancing your Harvester cluster's storage capabilities. With these configurations in place, your Harvester cluster is ready to make the most of the third-party storage integration. References​ Use Rook Ceph External Storage with HarvesterUsing NetApp Storage on Harvester","keywords":"","version":"v1.4 (dev)"},{"title":"Managed DHCP","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons/managed-dhcp","content":"Managed DHCP Available as of v1.3.0 Beginning with v1.3.0, you can configure IP pool information and serve IP addresses to VMs running on Harvester clusters using the embedded Managed DHCP feature. This feature, which is an alternative to the standalone DHCP server, leverages the vm-dhcp-controller add-on to simplify guest cluster deployment. note Harvester uses the planned infrastructure network so you must ensure that network connectivity is available and plan the IP pools in advance. Install and Enable the vm-dhcp-controller Add-On​ The vm-dhcp-controller add-on is not packed into the Harvester ISO, but you can download it from the expreimental-addons repository. You can install the add-on by running the following command: kubectl apply -f https://raw.githubusercontent.com/harvester/experimental-addons/main/harvester-vm-dhcp-controller/harvester-vm-dhcp-controller.yaml After installation, enable the add-on on the Dashboard screen of the Harvester UI or using the command-line tool kubectl. Usage​ On the Dashboard screen of the Harvester UI, create a VM Network. Create an IPPool object using the command-line tool kubectl. cat &lt;&lt;EOF | kubectl apply -f - apiVersion: network.harvesterhci.io/v1alpha1 kind: IPPool metadata: name: net-48 namespace: default spec: ipv4Config: serverIP: 192.168.48.77 cidr: 192.168.48.0/24 pool: start: 192.168.48.81 end: 192.168.48.90 exclude: - 192.168.48.81 - 192.168.48.90 router: 192.168.48.1 dns: - 1.1.1.1 leaseTime: 300 networkName: default/net-48 EOF Create a VM that is connected to the VM Network you previously created. Wait for the corresponding VirtualMachineNetworkConfig object to be created and for the MAC address of the VM's network interface to be applied to the object. Check the .status field of the IPPool and VirtualMachineNetworkConfig objects, and verify that the IP address is allocated and assigned to the MAC address. $ kubectl get ippools.network net-48 -o yaml apiVersion: network.harvesterhci.io/v1alpha1 kind: IPPool metadata: creationTimestamp: &quot;2024-02-15T13:17:21Z&quot; finalizers: - wrangler.cattle.io/vm-dhcp-ippool-controller generation: 1 name: net-48 namespace: default resourceVersion: &quot;826813&quot; uid: 5efd44b7-3796-4f02-947e-3949cb4c8e3d spec: ipv4Config: cidr: 192.168.48.0/24 dns: - 1.1.1.1 leaseTime: 300 pool: end: 192.168.48.90 exclude: - 192.168.48.81 - 192.168.48.90 start: 192.168.48.81 router: 192.168.48.1 serverIP: 192.168.48.77 networkName: default/net-48 status: agentPodRef: name: default-net-48-agent namespace: harvester-system conditions: - lastUpdateTime: &quot;2024-02-15T13:17:21Z&quot; status: &quot;True&quot; type: Registered - lastUpdateTime: &quot;2024-02-15T13:17:21Z&quot; status: &quot;True&quot; type: CacheReady - lastUpdateTime: &quot;2024-02-15T13:17:30Z&quot; status: &quot;True&quot; type: AgentReady - lastUpdateTime: &quot;2024-02-15T13:17:21Z&quot; status: &quot;False&quot; type: Stopped ipv4: allocated: 192.168.48.81: EXCLUDED 192.168.48.84: ca:70:82:e6:84:6e 192.168.48.90: EXCLUDED available: 7 used: 1 lastUpdate: &quot;2024-02-15T13:48:20Z&quot; $ kubectl get virtualmachinenetworkconfigs.network test-vm -o yaml apiVersion: network.harvesterhci.io/v1alpha1 kind: VirtualMachineNetworkConfig metadata: creationTimestamp: &quot;2024-02-15T13:48:02Z&quot; finalizers: - wrangler.cattle.io/vm-dhcp-vmnetcfg-controller generation: 2 labels: harvesterhci.io/vmName: test-vm name: test-vm namespace: default ownerReferences: - apiVersion: kubevirt.io/v1 kind: VirtualMachine name: test-vm uid: a9f8ce12-fd6c-4bd2-b266-245d8e77dae3 resourceVersion: &quot;826809&quot; uid: 556440c7-eeeb-4daf-9c98-60ab39688ba8 spec: networkConfig: - macAddress: ca:70:82:e6:84:6e networkName: default/net-48 vmName: test-vm status: conditions: - lastUpdateTime: &quot;2024-02-15T13:48:20Z&quot; status: &quot;True&quot; type: Allocated - lastUpdateTime: &quot;2024-02-15T13:48:02Z&quot; status: &quot;False&quot; type: Disabled networkConfig: - allocatedIPAddress: 192.168.48.84 macAddress: ca:70:82:e6:84:6e networkName: default/net-48 state: Allocated Check the VM's serial console and verify that the IP address is correctly configured on the network interface (via DHCP). vm-dhcp-controller Pods and CRDs​ When the vm-dhcp-controller add-on is enabled, the following types of pods run: Controller: Reconciles CRD objects to determine allocation and mapping between IP and MAC addresses. The results are persisted in the IPPool objects.Webhook: Validates and mutates CRD objects when receiving requests (creation, updating, and deletion)Agent: Serves DHCP requests and ensures that the internal DHCP lease store is up to date. This is accomplished by syncing the specific IPPool object that the agent is associated with. Agents are spawned on-demand whenever you create new IPPool objects. The vm-dhcp-controller introduces the following new CRDs. IPPool (ippl)VirtualMachineNetworkConfig (vmnetcfg) IPPool CRD​ The IPPool CRD allows you to define IP pool information. You must map each IPPool object to a specific NetworkAttachmentDefinition (NAD) object, which must be created beforehand. note Multiple CRDs named &quot;IPPool&quot; are used in the Harvester ecosystem, including a similarly-named CRD in the loadbalancer.harvesterhci.io API group. To avoid issues, ensure that you are working with the IPPool CRD in the network.harvesterhci.io API group. For more information about IPPool CRD operations in relation to load balancers, see IP Pool. Example: apiVersion: network.harvesterhci.io/v1alpha1 kind: IPPool metadata: name: example namespace: default spec: ipv4Config: serverIP: 192.168.100.2 # The DHCP server's IP address cidr: 192.168.100.0/24 # The subnet information, must be in the CIDR form pool: start: 192.168.100.101 end: 192.168.100.200 exclude: - 192.168.100.151 - 192.168.100.187 router: 192.168.100.1 # The default gateway, if any dns: - 1.1.1.1 domainName: example.com domainSearch: - example.com ntp: - pool.ntp.org leaseTime: 300 networkName: default/example # The namespaced name of the NAD object After the IPPool object is created, the controller reconciliation process initializes the IP allocation module and spawns the agent pod for the network. $ kubectl get ippools.network example NAME NETWORK AVAILABLE USED REGISTERED CACHEREADY AGENTREADY example default/example 98 0 True True True VirtualMachineNetworkConfig CRD​ The VirtualMachineNetworkConfig CRD resembles a request for IP address issuance and is associated with NetworkAttachmentDefinition (NAD) objects. A sample VirtualMachineNetworkConfig object looks like the following: apiVersion: network.harvesterhci.io/v1alpha1 kind: VirtualMachineNetworkConfig metadata: name: test-vm namespace: default spec: networkConfig: - macAddress: 22:37:37:82:93:7d networkName: default/example vmName: test-vm After the VirtualMachineNetworkConfig object is created, the controller attempts to retrieve a list of unused IP addresses from the IP allocation module for each recorded MAC address. The IP-MAC mapping is then updated in the VirtualMachineNetworkConfig object and the corresponding IPPool objects. note Manual creation of VirtualMachineNetworkConfig objects for VMs is unnecessary in most cases because vm-dhcp-controller handles that task during the VirtualMachine reconciliation process. Automatically-created VirtualMachineNetworkConfig objects are deleted when VirtualMachine objects are removed.","keywords":"","version":"v1.4 (dev)"},{"title":"Custom SUSE VM Images","type":0,"sectionRef":"#","url":"/v1.4/advanced/customsuseimages","content":"Custom SUSE VM Images SUSE provides SUSE Linux Enterprise (SLE) and openSUSE Leap virtual machine (VM) images suitable for use in Harvester. These images are built on the openSUSE Build Service (OBS) using the Kiwi image building tool, and can be used immediately after downloading. For most cases, you can use the Minimal VM Cloud qcow2 images because these include the cloud-init tool necessary for automatic VM configuration. Other image variants require you to log onto the VM console and then perform initial configuration. info The Minimal VM Cloud images were named Minimal VM OpenStack Cloud in releases earlier than SLES 15 SP5 and openSUSE 15.5. Using the openSUSE Build Service (OBS)​ You can create custom images based on what SUSE provides using OBS image templates, which are pre-configured Kiwi image configurations. For example, if you want use other packages with SLE 15 SP5, you can create an image using the SLE 15 SP5 Minimal template. OBS provides an interface for adding packages and automatically builds the image, which you can download and then upload to Harvester. For more information, see the OBS User Guide. 1. Create a custom image based on an existing template.​ Go to https://build.opensuse.org/image_templates. You must sign in to your openSUSE account to access the resources. Select the template that you want to use. Specify a name for the image, and then select Create appliance. OBS automatically builds the image. By default, the interface shows the Overview tab, which contains information such as the number of included packages and the build status. 2. Select image profiles and add packages.​ Go to the Software tab. Select the image profiles that you want OBS to build. info For most cases, you can use the Minimal VM Cloud qcow2 images because these include the cloud-init tool necessary for automatic VM configuration. Other image variants require you to log onto the VM console and then perform initial configuration. (Optional) Add and remove packages. 3. (Optional) Switch to View Package mode.​ View Package mode provides more granular control over configuration. To switch, click the View Package icon in the navigation bar. The Source Files section of the Overview tab shows all the files that comprise your Kiwi template. You can edit any of the files by selecting the corresponding file name. 4. (Optional) Edit the configuration file Minimal.kiwi.​ Select the file name to open the text editor. The &lt;packages type=&quot;image&quot;&gt; section lists the packages to be installed. You can specify additional packages for each image profile. By default, the Cloud image profile (&lt;package type=&quot;image&quot; profiles=&quot;Cloud&quot;&gt;) installs the kernel-default-base package. In the following example, that package is replaced with kernel-default, which includes modules necessary for iSCSI support. 5. Wait for OBS to finish building the image.​ Once the process is completed, the Build Results section on the Overview tab shows the status succeeded. The Build Results section also contains a download link for the new image. 6. Enable publishing to share the image.​ To allow the public to download your custom image, go to the Repositories tab of your OBS project and enable the Publish flag. Your image is published to https://download.opensuse.org/ (under repositories/home:/YOUR_USER_NAME:/branches:/SUSE:/Templates:/Images:/). Using the Kiwi Command-line Tool​ As an alternative to the openSUSE Build Service, you can create images locally using the Kiwi command-line tool. For more information about the tool, see Building Linux System Appliances with KIWI Next Generation (KIWI NG). To create custom images, you must first download the file Minimal.kiwi, and the scripts config.sh and editbootinstall_rpi.sh from the corresponding project on OBS. OS\topenSUSE Build Service ProjectSLE 15 SP5\thttps://build.opensuse.org/package/show/SUSE:SLE-15-SP5:GA/kiwi-templates-Minimal openSUSE Leap 15.5\thttps://build.opensuse.org/package/show/openSUSE:Leap:15.5/kiwi-templates-Minimal SLE 15 SP4\thttps://build.opensuse.org/package/show/SUSE:SLE-15-SP4:GA/kiwi-templates-Minimal openSUSE Leap 15.4\thttps://build.opensuse.org/package/show/openSUSE:Leap:15.4/kiwi-templates-Minimal","keywords":"Custom Images","version":"v1.4 (dev)"},{"title":"VM Import","type":0,"sectionRef":"#","url":"/v1.4/advanced/addons/vmimport","content":"VM Import Available as of v1.1.0 Beginning with v1.1.0, users can import their virtual machines from VMWare and OpenStack into Harvester. This is accomplished using the vm-import-controller addon. To use the VM Import feature, users need to enable the vm-import-controller addon. By default, vm-import-controller leverages ephemeral storage, which is mounted from /var/lib/kubelet. During the migration, a large VM's node could run out of space on this mount, resulting in subsequent scheduling failures. To avoid this, users are advised to enable PVC-backed storage and customize the amount of storage needed. According to the best practice, the PVC size should be twice the size of the largest VM being migrated. This is essential as the PVC is used as scratch space to download the VM, and convert the disks into raw image files. vm-import-controller​ Currently, the following source providers are supported: VMWareOpenStack API​ The vm-import-controller introduces two CRDs. Sources​ Sources allow users to define valid source clusters. For example: apiVersion: migration.harvesterhci.io/v1beta1 kind: VmwareSource metadata: name: vcsim namespace: default spec: endpoint: &quot;https://vscim/sdk&quot; dc: &quot;DCO&quot; credentials: name: vsphere-credentials namespace: default The secret contains the credentials for the vCenter endpoint: apiVersion: v1 kind: Secret metadata: name: vsphere-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; As part of the reconciliation process, the controller will log into vCenter and verify whether the dc specified in the source spec is valid. Once this check is passed, the source is marked as ready and can be used for VM migrations. $ kubectl get vmwaresource.migration NAME STATUS vcsim clusterReady For OpenStack-based source clusters, an example definition is as follows: apiVersion: migration.harvesterhci.io/v1beta1 kind: OpenstackSource metadata: name: devstack namespace: default spec: endpoint: &quot;https://devstack/identity&quot; region: &quot;RegionOne&quot; credentials: name: devstack-credentials namespace: default The secret contains the credentials for the OpenStack endpoint: apiVersion: v1 kind: Secret metadata: name: devstack-credentials namespace: default stringData: &quot;username&quot;: &quot;user&quot; &quot;password&quot;: &quot;password&quot; &quot;project_name&quot;: &quot;admin&quot; &quot;domain_name&quot;: &quot;default&quot; &quot;ca_cert&quot;: &quot;pem-encoded-ca-cert&quot; The OpenStack source reconciliation process attempts to list VMs in the project and marks the source as ready. $ kubectl get opestacksource.migration NAME STATUS devstack clusterReady VirtualMachineImport​ The VirtualMachineImport CRD provides a way for users to define a source VM and map to the actual source cluster to perform VM export/import. A sample VirtualMachineImport looks like this: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: alpine-export-test namespace: default spec: virtualMachineName: &quot;alpine-export-test&quot; networkMapping: - sourceNetwork: &quot;dvSwitch 1&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;dvSwitch 2&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: vcsim namespace: default kind: VmwareSource apiVersion: migration.harvesterhci.io/v1beta1 This will trigger the controller to export the VM named &quot;alpine-export-test&quot; on the VMWare source cluster to be exported, processed and recreated into the harvester cluster This can take a while based on the size of the virtual machine, but users should see VirtualMachineImages created for each disk in the defined virtual machine. The list of items in networkMapping will define how the source network interfaces are mapped to the Harvester Networks. If a match is not found, each unmatched network interface is attached to the default managementNetwork. Once the virtual machine has been imported successfully, the object will reflect the status: $ kubectl get virtualmachineimport.migration NAME STATUS alpine-export-test virtualMachineRunning openstack-cirros-test virtualMachineRunning Similarly, users can define a VirtualMachineImport for an OpenStack source as well: apiVersion: migration.harvesterhci.io/v1beta1 kind: VirtualMachineImport metadata: name: openstack-demo namespace: default spec: virtualMachineName: &quot;openstack-demo&quot; #Name or UUID for instance networkMapping: - sourceNetwork: &quot;shared&quot; destinationNetwork: &quot;default/vlan1&quot; - sourceNetwork: &quot;public&quot; destinationNetwork: &quot;default/vlan2&quot; sourceCluster: name: devstack namespace: default kind: OpenstackSource apiVersion: migration.harvesterhci.io/v1beta1 note OpenStack allows users to have multiple instances with the same name. In such a scenario, users are advised to use the Instance ID. The reconciliation logic tries to perform a name-to-ID lookup when a name is used.","keywords":"","version":"v1.4 (dev)"},{"title":"Single-Node Clusters","type":0,"sectionRef":"#","url":"/v1.4/advanced/singlenodeclusters","content":"Single-Node Clusters As of Harvester release v1.2.0, single-node clusters are supported for implementations that require minimal initial deployment resources or that can tolerate lower resiliency. You can create single-node clusters using the standard installation methods (ISO, USB, and PXE boot). Single-node clusters support most Harvester features, including the creation of RKE2 clusters and node upgrades (with some limitations). However, this deployment type has the following key disadvantages: No high availability: Errors and updates that require rebooting of the node cause downtime to running VMs.No multi-replica support: Only one replica is created for each volume in Longhorn.No live migration and zero-downtime support during upgrades. Prerequisites​ Before you begin deploying your single-node cluster, ensure that the following requirements are addressed. Hardware: Use server-class hardware with sufficient resources to run Harvester and a production workload. Laptops and nested virtualization are not supported. Network: Configure ports based on the type of traffic to be transmitted among VMs. StorageClass: Create a new default StorageClass with the Number of Replicas parameter set to &quot;1&quot;. This ensures that only one replica is created for each volume in Longhorn. important The default StorageClass harvester-longhorn has a replica count value of 3 for high availability. If you use this StorageClass to create volumes for your single-node cluster, Longhorn is unable to create the configured number of replicas. This results in volumes being marked as &quot;Degraded&quot; on the Longhorn UI.","keywords":"Single Node","version":"v1.4 (dev)"},{"title":"StorageClass","type":0,"sectionRef":"#","url":"/v1.4/advanced/storageclass","content":"StorageClass A StorageClass allows administrators to describe the classes of storage they offer. Different Longhorn StorageClasses might map to replica policies, or to node schedule policies, or disk schedule policies determined by the cluster administrators. This concept is sometimes called profiles in other storage systems. note For support with other storage, please refer to Third-Party Storage Support Creating a StorageClass​ You can create one or more StorageClasses from the Advanced &gt; StorageClasses page. note After a StorageClass is created, nothing can be changed except Description. Header Section​ Name: name of the StorageClassDescription (optional): description of the StorageClass Parameters Tab​ Number of Replicas​ The number of replicas created for each volume in Longhorn. Defaults to 3. Stale Replica Timeout​ Determines when Longhorn would clean up an error replica after the replica's status is ERROR. The unit is minute. Defaults to 30 minutes in Harvester. Node Selector (Optional)​ Select the node tags to be matched in the volume scheduling stage. You can add node tags by going to Host &gt; Edit Config. Disk Selector (Optional)​ Select the disk tags to be matched in the volume scheduling stage. You can add disk tags by going to Host &gt; Edit Config. Migratable​ Whether Live Migration is supported. Defaults to Yes. Customize Tab​ Reclaim Policy​ Volumes dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class. The Delete mode is used by default. Delete: Deletes volumes and the underlying devices when the volume claim is deleted.Retain: Retains the volume for manual cleanup. Allow Volume Expansion​ Volumes can be configured to be expandable. This feature is Enabled by default, which allows users to resize the volume by editing the corresponding PVC object. note You can only use the volume expansion feature to grow a Volume, not to shrink it. Volume Binding Mode​ The volumeBindingMode field controls when volume binding and dynamic provisioning should occur. The Immediate mode is used by default. Immediate: Binds and provisions a persistent volume once the PersistentVolumeClaim is created.WaitForFirstConsumer: Binds and provisions a persistent volume once a VM using the PersistentVolumeClaim is created. Data Locality Settings​ You can use the dataLocality parameter when at least one replica of a Longhorn volume must be scheduled on the same node as the pod that uses the volume (whenever possible). Harvester officially supports data locality as of v1.3.0. This applies even to volumes created from images. To configure data locality, create a new StorageClass on the Harvester UI (Storage Classess &gt; Create &gt; Parameters) and then add the following parameter: Key: dataLocalityValue: disabled or best-effort Data Locality Options​ Harvester currently supports the following options: disabled: When applied, Longhorn may or may not schedule a replica on the same node as the pod that uses the volume. This is the default option. best-effort: When applied, Longhorn always attempts to schedule a replica on the same node as the pod that uses the volume. Longhorn does not stop the volume even when a local replica is unavailable because of an environmental limitation (for example, insufficient disk space or incompatible disk tags). note Longhorn provides a third option called strict-local, which forces Longhorn to keep only one replica on the same node as the pod that uses the volume. Harvester does not support this option because it can affect certain operations such as VM Live Migration For more information, see Data Locality in the Longhorn documentation. Appendix - Use Case​ HDD Scenario​ With the introduction of StorageClass, users can now use HDDs for tiered or archived cold storage. caution HDD is not recommended for guest RKE2 clusters or VMs with good performance disk requirements. Recommended Practice​ First, add your HDD on the Host page and specify the disk tags as needed, such asHDD or ColdStorage. For more information on how to add extra disks and disk tags, see Multi-disk Management for details. Then, create a new StorageClass for the HDD (use the above disk tags). For hard drives with large capacity but slow performance, the number of replicas can be reduced to improve performance. You can now create a volume using the above StorageClass with HDDs mostly for cold storage or archiving purpose.","keywords":"","version":"v1.4 (dev)"},{"title":"Witness Node","type":0,"sectionRef":"#","url":"/v1.4/advanced/witness","content":"Witness Node Available as of v1.3.0 Harvester clusters deployed in production environments require a control plane for node and pod management. A typical three-node cluster has three management nodes that each contain the complete set of control plane components. One key component is etcd, which Kubernetes uses to store its data (configuration, state, and metadata). The etcd node count must always be an odd number (for example, 3 is the default count in Harvester) to ensure that a quorum is maintained. Some situations may require you to avoid deploying workloads and user data to management nodes. In these situations, one cluster node can be assigned the witness role, which limits it to functioning as an etcd cluster member. The witness node is responsible for establishing a member quorum (a majority of nodes), which must agree on updates to the cluster state. Witness nodes do not store any data, but the hardware recommendations for etcd nodes must still be considered. Using hardware with limited resources significantly affects cluster performance, as described in the article Slow etcd performance (performance testing and optimization). Harvester v1.3.0 supports clusters with two management nodes and one witness node (and optionally, one or more worker nodes). For more information about node roles in Harvester, see Role Management. important A node can be assigned the witness role only at the time it joins a cluster. Each cluster can have only one witness node. Creating a Harvester Cluster with a Witness Node​ You can assign the witness role to a node when it joins a newly created cluster. In the following example, a cluster with three nodes was created and the node harvester-node-1 was assigned the witness role. harvester-node-1 consumes less resources and only has etcd capabilities. NAME↑ STATUS ROLE VERSION PODS CPU MEM %CPU %MEM CPU/A MEM/A AGE harvester-node-0 Ready control-plane,etcd,master v1.27.10+rke2r1 70 1095 10143 10 63 10000 15976 4d13h harvester-node-1 Ready etcd v1.27.10+rke2r1 7 258 2258 2 14 10000 15976 4d13h harvester-node-2 Ready control-plane,etcd,master v1.27.10+rke2r1 36 840 6905 8 43 10000 15976 4d13h Because the cluster must have three nodes, the promote controller will promote the other two nodes. After that, the cluster will have two control-plane nodes and one witness node. Workloads on the Witness Node​ The witness node only runs the following essential workloads: harvester-node-managercloud-controller-manageretcdkube-proxyrke2-canalrke2-multus Upgrade a Cluster with a Witness Node​ The general upgrade requirements and procedures apply to clusters with a witness node. However, the existence of degraded volumes in such clusters may cause upgrade operations to fail. Longhorn Replicas in Clusters with a Witness Node​ Harvester uses Longhorn, a distributed block storage system, for management of block device volumes. Longhorn is provisioned to management and worker nodes but not to witness nodes, which do not store any data. Longhorn creates replicas of each volume to increase availability. Replicas contain a chain of snapshots of the volume, with each snapshot storing the change from a previous snapshot. In Harvester, the default StorageClass harvester-longhorn has a replica count value of 3. Limitations​ Witness nodes do not store any data. This means that in three-node clusters (no worker nodes), only two replicas are created for each Longhorn volume. However, the default StorageClass harvester-longhorn has a replica count value of 3 for high availability. If you use this StorageClass to create volumes, Longhorn is unable to create the configured number of replicas. This results in volumes being marked as Degraded on the Longhorn UI. In summary, you must use a StorageClass that matches the cluster configuration. 2 management nodes + 1 witness node: Create a new default StorageClass with the Number of Replicas parameter set to 2. This ensures that only two replicas are created for each Longhorn volume.2 management nodes + 1 witness node + 1 or more worker nodes: You can use the existing default StorageClass. If you already created volumes using the original default StorageClass, you can modify the replica count on the Volume screen of the embedded Longhorn UI. Known Issues​ 1. When creating a cluster with a witness node, the Network Config: Create screen on the Harvester UI is unable to identify any NICs that can be used with all nodes.​ The workaround is to select a non-witness node and then select a NIC that can be used with that specific node. You must repeat this procedure for every non-witness node in the cluster. The same uplink settings can be used across nodes. Related issue: [BUG] Unable to select NIC to create network config when cluster contains witness node 2. When selecting a target node for VM migration, the target list includes the witness node.​ Do not select the witness node as the migration target. If you do, VM migration will fail. Related issue: [BUG] The witness node should not be selected as a migration target","keywords":"","version":"v1.4 (dev)"},{"title":"vGPU Support","type":0,"sectionRef":"#","url":"/v1.4/advanced/vgpusupport","content":"vGPU Support Available as of v1.3.0 Harvester now offers the capability to share NVIDIA GPU's supporting SRIOV based virtualisation as vGPU devices. The additional capability is provided by the pcidevices-controller addon, and leverages sriov-manage to manage the gpu. Please refer the Nvidia Documentation and your GPU documentation to identify if the GPU is supported. The nvidia-driver-toolkit addon needs to be enabled for users to be able to manage the lifecycle of vGPU's on GPU devices. Usage​ On the Harvester UI, go to Advanced &gt; SR-IOV GPU Devices and verify the following: GPU devices have been scanned. An associated sriovgpudevices.devices.harvesterhci.io object has been created. Locate the device that you want to enable, and then select : &gt; Enable. Go to the vGPU Devices screen and check the associated vgpudevices.devices.harvesterhci.io objects. Allow some time for the pcidevices-controller to scan the vGPU devices and for the Harvester UI to display the device information. Select a vGPU and configure a profile. :::note The list of profiles depends on the GPU and the underlying /sys tree of the host. For more information about the available profiles and their capabilities, see the NVIDIA documentation. After you select the first profile, the NVIDIA driver automatically configures the profiles available for the remaining vGPUs. ::: ::: Attach the vGPU to a new or existing VM. important Once a vGPU has been assigned to a VM, it may not be possible to disable the VM until the vGPU is removed. Limitations​ Attaching multiple vGPU's:​ Attaching multiple vGPUs to a VM may fail for the following reasons: Not all vGPU profiles support attachment of multiple vGPUs. The NVIDIA documentation lists the vGPU profiles that support this feature. For example, if you use NVIDIA A2 or A16 GPUs, note that only Q-series vGPUs allow you to attach multiple vGPUs. Only 1 GPU device in the VM definition can have ramFB enabled. To attach multiple vGPUs, you must edit the VM configuration (in YAML) and add virtualGPUOptions to all non-primary vGPU devices. virtualGPUOptions: display: ramFB: enabled: false Related issue: https://github.com/harvester/harvester/issues/5289 Cap on Usable vGPUs​ When vGPU support is enabled on a GPU, the NVIDIA driver creates 16 vGPU devices by default. After you select the first profile, the NVIDIA driver automatically configures the profiles available for the remaining vGPUs. The profile used also dictates the maximum number of vGPUs available for each GPU. Once the maximum is exhausted, no profiles can be selected for the remaining vGPUs and those devices cannot be configured. Example (NVIDIA A2 GPU): If you select the NVIDIA A2-4Q profile, you can only configure 4 vGPU devices. Once those devices are configured, you cannot select any profiles for the remaining vGPUs. Technical Deep dive​ pcidevices-controller introduces the following CRDs: sriovgpudevices.devices.harvesterhci.io vgpudevices.devices.harvesterhci.io On boot, pcidevices-controller scans the host for NVIDIA GPUs that support SR-IOV vGPU devices. When such devices are found, they are represented as a CRD. Example: apiVersion: devices.harvesterhci.io/v1beta1 kind: SRIOVGPUDevice metadata: creationTimestamp: &quot;2024-02-21T05:57:37Z&quot; generation: 2 labels: nodename: harvester-kgd9c name: harvester-kgd9c-000008000 resourceVersion: &quot;6641619&quot; uid: e3a97ee4-046a-48d7-820d-8c6b45cd07da spec: address: &quot;0000:08:00.0&quot; enabled: true nodeName: harvester-kgd9c status: vGPUDevices: - harvester-kgd9c-000008004 - harvester-kgd9c-000008005 - harvester-kgd9c-000008016 - harvester-kgd9c-000008017 - harvester-kgd9c-000008020 - harvester-kgd9c-000008021 - harvester-kgd9c-000008022 - harvester-kgd9c-000008023 - harvester-kgd9c-000008006 - harvester-kgd9c-000008007 - harvester-kgd9c-000008010 - harvester-kgd9c-000008011 - harvester-kgd9c-000008012 - harvester-kgd9c-000008013 - harvester-kgd9c-000008014 - harvester-kgd9c-000008015 vfAddresses: - &quot;0000:08:00.4&quot; - &quot;0000:08:00.5&quot; - &quot;0000:08:01.6&quot; - &quot;0000:08:01.7&quot; - &quot;0000:08:02.0&quot; - &quot;0000:08:02.1&quot; - &quot;0000:08:02.2&quot; - &quot;0000:08:02.3&quot; - &quot;0000:08:00.6&quot; - &quot;0000:08:00.7&quot; - &quot;0000:08:01.0&quot; - &quot;0000:08:01.1&quot; - &quot;0000:08:01.2&quot; - &quot;0000:08:01.3&quot; - &quot;0000:08:01.4&quot; - &quot;0000:08:01.5&quot; When a SRIOVGPUDevice is enabled, the pcidevices controller works with the nvidia-driver-toolkit daemonset to manage the GPU devices. On subsequent scan of the /sys tree by the pcidevices, the vGPU devices are scanned by the pcidevices controller and managed as VGPUDevices CRD NAME ADDRESS NODE NAME ENABLED UUID VGPUTYPE PARENTGPUDEVICE harvester-kgd9c-000008004 0000:08:00.4 harvester-kgd9c true dd6772a8-7db8-4e96-9a73-f94c389d9bc3 NVIDIA A2-4A 0000:08:00.0 harvester-kgd9c-000008005 0000:08:00.5 harvester-kgd9c true 9534e04b-4687-412b-833e-3ae95b97d4d1 NVIDIA A2-4Q 0000:08:00.0 harvester-kgd9c-000008006 0000:08:00.6 harvester-kgd9c true a16e5966-9f7a-48a9-bda8-0d1670e740f8 NVIDIA A2-4A 0000:08:00.0 harvester-kgd9c-000008007 0000:08:00.7 harvester-kgd9c true 041ee3ce-f95c-451e-a381-1c9fe71918b2 NVIDIA A2-4Q 0000:08:00.0 harvester-kgd9c-000008010 0000:08:01.0 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008011 0000:08:01.1 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008012 0000:08:01.2 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008013 0000:08:01.3 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008014 0000:08:01.4 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008015 0000:08:01.5 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008016 0000:08:01.6 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008017 0000:08:01.7 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008020 0000:08:02.0 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008021 0000:08:02.1 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008022 0000:08:02.2 harvester-kgd9c false 0000:08:00.0 harvester-kgd9c-000008023 0000:08:02.3 harvester-kgd9c false 0000:08:00.0 When a user enables and selects a profile for the VGPUDevice the pcidevices controller sets up the device and sets up the correct profile on the said device. apiVersion: devices.harvesterhci.io/v1beta1 kind: VGPUDevice metadata: creationTimestamp: &quot;2024-02-26T03:04:47Z&quot; generation: 8 labels: harvesterhci.io/parentSRIOVGPUDevice: harvester-kgd9c-000008000 nodename: harvester-kgd9c name: harvester-kgd9c-000008004 resourceVersion: &quot;21051017&quot; uid: b9c7af64-1e47-467f-bf3d-87b7bc3a8911 spec: address: &quot;0000:08:00.4&quot; enabled: true nodeName: harvester-kgd9c parentGPUDeviceAddress: &quot;0000:08:00.0&quot; vGPUTypeName: NVIDIA A2-4A status: configureVGPUTypeName: NVIDIA A2-4A uuid: dd6772a8-7db8-4e96-9a73-f94c389d9bc3 vGPUStatus: vGPUConfigured The pcidevices controller also runs a vGPU device plugin, which advertises the details of the various vGPU profiles to the kubelet. This is then used by the k8s scheduler to place the VM's requesting vGPU's to the correct nodes. (⎈|local:harvester-system)➜ ~ k get nodes harvester-kgd9c -o yaml | yq .status.allocatable cpu: &quot;24&quot; devices.kubevirt.io/kvm: 1k devices.kubevirt.io/tun: 1k devices.kubevirt.io/vhost-net: 1k ephemeral-storage: &quot;149527126718&quot; hugepages-1Gi: &quot;0&quot; hugepages-2Mi: &quot;0&quot; intel.com/82599_ETHERNET_CONTROLLER_VIRTUAL_FUNCTION: &quot;1&quot; memory: 131858088Ki nvidia.com/NVIDIA_A2-4A: &quot;2&quot; nvidia.com/NVIDIA_A2-4C: &quot;0&quot; nvidia.com/NVIDIA_A2-4Q: &quot;2&quot; pods: &quot;200&quot; The pcidevices controller also setups the integration with kubevirt and advertises the vGPU devices as externally managed devices in the Kubevirt CR to ensure that the VM can consume the vGPU.","keywords":"","version":"v1.4 (dev)"},{"title":"Air Gapped Environment","type":0,"sectionRef":"#","url":"/v1.4/airgap","content":"Air Gapped Environment This section describes how to use Harvester in an air gapped environment. Some use cases could be where Harvester will be installed offline, behind a firewall, or behind a proxy. The Harvester ISO image contains all the packages to make it work in an air gapped environment. Working Behind an HTTP Proxy​ In some environments, the connection to external services, from the servers or VMs, requires an HTTP(S) proxy. Configure an HTTP Proxy During Installation​ You can configure the HTTP(S) proxy during the ISO installation as shown in picture below: Configure an HTTP Proxy in Harvester Settings​ You can configure the HTTP(S) proxy in the settings page of the Harvester dashboard: Go to the settings page of the Harvester UI.Find the http-proxy setting, click ⋮ &gt; Edit settingEnter the value(s) for http-proxy, https-proxy and no-proxy. note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. When the nodes in the cluster do not use a proxy to communicate with each other, the CIDR needs to be added to http-proxy.noProxy after the first node is installed successfully. Please refer to fail to deploy a multi-node cluster. Guest Cluster Images​ All necessary images to install and run Harvester are conveniently packaged into the ISO, eliminating the need to pre-load images on bare-metal nodes. A Harvester cluster manages them independently and effectively behind the scenes. However, it's essential to understand a guest K8s cluster (e.g., RKE2 cluster) created by the Harvester node driver is a distinct entity from a Harvester cluster. A guest cluster operates within VMs and requires pulling images either from the internet or a private registry. If the Cloud Provider option is configured to Harvester in a guest K8s cluster, it deploys the Harvester cloud provider and Container Storage Interface (CSI) driver. As a result, we recommend monitoring each RKE2 release in your air gapped environment and pulling the required images into your private registry. Please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section on the Harvester support matrix page for the best Harvester cloud provider and CSI driver capability support.","keywords":"Harvester offline Air-gap HTTP proxy","version":"v1.4 (dev)"},{"title":"Settings","type":0,"sectionRef":"#","url":"/v1.4/advanced/index","content":"Settings This page contains a list of advanced settings which can be used in Harvester. You can modify the custom resource settings.harvesterhci.io from the Dashboard UI or with the kubectl command. additional-ca​ This setting allows you to configure additional trusted CA certificates for Harvester to access external services. Default: none Example​ -----BEGIN CERTIFICATE----- SOME-CA-CERTIFICATES -----END CERTIFICATE----- caution Changing this setting might cause a short downtime for single-node clusters. auto-disk-provision-paths [Experimental]​ This setting allows Harvester to automatically add disks that match the given glob pattern as VM storage. It's possible to provide multiple patterns by separating them with a comma. note This setting only adds formatted disks mounted to the system. caution This setting is applied to every Node in the cluster.All the data in these storage devices will be destroyed. Use at your own risk. Default: none Example​ The following example will add disks matching the glob pattern /dev/sd* or /dev/hd*: /dev/sd*,/dev/hd* backup-target​ This setting allows you to set a custom backup target to store VM backups. It supports NFS and S3. For further information, please refer to the Longhorn documentation. Default: none Example​ { &quot;type&quot;: &quot;s3&quot;, &quot;endpoint&quot;: &quot;https://s3.endpoint.svc&quot;, &quot;accessKeyId&quot;: &quot;test-access-key-id&quot;, &quot;secretAccessKey&quot;: &quot;test-access-key&quot;, &quot;bucketName&quot;: &quot;test-bup&quot;, &quot;bucketRegion&quot;: &quot;us‑east‑2&quot;, &quot;cert&quot;: &quot;&quot;, &quot;virtualHostedStyle&quot;: false } cluster-registration-url​ This setting allows you to import the Harvester cluster to Rancher for multi-cluster management. Default: none Example​ https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml note When you configure this setting, a new pod called cattle-cluster-agent-* is created in the namespace cattle-system for registration purposes. This pod uses the container image rancher/rancher-agent:related-version, which is not packed into the Harvester ISO and is instead determined by Rancher. The related-version is usually the same as the Rancher version. For example, when you register Harvester to Rancher v2.7.9, the image is rancher/rancher-agent:v2.7.9. For more information, see Find the required assets for your Rancher version in the Rancher documentation. Depending on your Harvester settings, the image is downloaded from either of the following locations: Harvester containerd-registry: You can configure a private registry for the Harvester cluster. Docker Hub (docker.io): This is the default option when you do not configure a private registry in Rancher. Alternatively, you can obtain a copy of the image and manually upload it to all Harvester nodes. containerd-registry​ This setting allows you to configure a private registry for the Harvester cluster. The value will be set in /etc/rancher/rke2/registries.yaml of each node. You can read RKE2 - Containerd Registry Configuration for more information. note If you set a username and password for a private registry, the system will automatically remove it to protect the credential after the system saves it in registries.yaml. Example​ { &quot;Mirrors&quot;: { &quot;docker.io&quot;: { &quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;], &quot;Rewrites&quot;: null } }, &quot;Configs&quot;: { &quot;myregistry.local:5000&quot;: { &quot;Auth&quot;: { &quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot; }, &quot;TLS&quot;: { &quot;InsecureSkipVerify&quot;: false } } } } csi-driver-config​ Available as of v1.2.0 If you install third-party CSI drivers in the Harvester cluster, you must configure some necessary information through this setting before using Backup &amp; Snapshot related features. Default: { &quot;driver.longhorn.io&quot;: { &quot;volumeSnapshotClassName&quot;: &quot;longhorn-snapshot&quot;, &quot;backupVolumeSnapshotClassName&quot;: &quot;longhorn&quot; } } Add the provisioner for the newly added CSI driver.Configure Volume Snapshot Class Name, which refers to the name of the VolumeSnapshotClass used to create volume snapshots or VM snapshots.Configure Backup Volume Snapshot Class Name, which refers to the name of the VolumeSnapshotClass used to create VM backups. default-vm-termination-grace-period-seconds​ Available as of v1.2.0 This setting allows you to specify a default termination grace period for stopping a virtual machine in seconds. Default: 120 http-proxy​ This setting allows you to configure an HTTP proxy to access external services, including the download of images and backup to s3 services. Default: {} The following options and values can be set: Proxy URL for HTTP requests: &quot;httpProxy&quot;: &quot;http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Proxy URL for HTTPS requests: &quot;httpsProxy&quot;: &quot;https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt;&quot;Comma-separated list of hostnames and/or CIDRs: &quot;noProxy&quot;: &quot;&lt;hostname | CIDR&gt;&quot; caution If you configure httpProxy and httpsProxy, you must also put Harvester node's CIDR into noProxy, otherwise the Harvester cluster will be broken. If you also configure cluster-registration-url, you usually need to add the host of cluster-registration-url to noProxy as well, otherwise you cannot access the Harvester cluster from Rancher. Example​ { &quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc,172.16.0.0/16&quot; } note Harvester appends necessary addresses to user configured no-proxy to ensure the internal traffic works. i.e., localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local. harvester-system was added into the list since v1.1.2. caution Changing this setting might cause a short downtime for single-node clusters. log-level​ This setting allows you to configure the log level for the Harvester server. Default: info The following values can be set. The list goes from the least to most verbose log level: panicfatalerrorwarn, warninginfodebugtrace Example​ debug ntp-servers​ Available as of v1.2.0 This setting allows you to configure NTP servers for time synchronization on the Harvester nodes. Using this setting, you can define NTP servers during installation or update NTP servers after installation. caution Modifying the NTP servers will replace the previous values for all nodes. Default: &quot;&quot; Example​ { &quot;ntpServers&quot;: [ &quot;0.suse.pool.ntp.org&quot;, &quot;1.suse.pool.ntp.org&quot; ] } overcommit-config​ This setting allows you to configure the percentage for resources overcommit on CPU, memory, and storage. By setting resources overcommit, this will permit to schedule additional virtual machines even if the the physical resources are already fully utilized. Default: { &quot;cpu&quot;:1600, &quot;memory&quot;:150, &quot;storage&quot;:200 } The default CPU overcommit with 1600% means, for example, if the CPU resources limit of a virtual machine is 1600m core, Harvester would only request 100mCPU for it from Kubernetes scheduler. Example​ { &quot;cpu&quot;: 1000, &quot;memory&quot;: 200, &quot;storage&quot;: 300 } release-download-url​ Available as of v1.0.1 This setting allows you to configure the upgrade release download URL address. Harvester will get the ISO URL and checksum value from the ${URL}/${VERSION}/version.yaml file hosted by the configured URL. Default: https://releases.rancher.com/harvester Example of the version.yaml​ apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: ${VERSION} namespace: harvester-system spec: isoChecksum: ${ISO_CHECKSUM} isoURL: ${ISO_URL} server-version​ This setting displays the version of Harvester server. Example​ v1.0.0-abcdef-head ssl-certificates​ This setting allows you to configure serving certificates for Harvester UI/API. Default: {} Example​ { &quot;ca&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;publicCertificate&quot;: &quot;-----BEGIN CERTIFICATE-----\\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\\n-----END CERTIFICATE-----&quot;, &quot;privateKey&quot;: &quot;-----BEGIN RSA PRIVATE KEY-----\\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\\n-----END RSA PRIVATE KEY-----&quot; } caution Changing this setting might cause a short downtime on single-node clusters. ssl-parameters​ This setting allows you to change the enabled SSL/TLS protocols and ciphers of Harvester GUI and API. The following options and values can be set: protocols: Enabled protocols. See NGINX Ingress Controller's configs ssl-protocols for supported input. ciphers: Enabled ciphers. See NGINX Ingress Controller's configs ssl-ciphers for supported input. If no value is provided, protocols is set to TLSv1.2 only and the ciphers list isECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305. Default: none note See Troubleshooting if you have misconfigured this setting and no longer have access to Harvester GUI and API. Example​ The following example sets the enabled SSL/TLS protocols to TLSv1.2 and TLSv1.3 and the ciphers list toECDHE-ECDSA-AES128-GCM-SHA256 and ECDHE-ECDSA-CHACHA20-POLY1305. { &quot;protocols&quot;: &quot;TLSv1.2 TLSv1.3&quot;, &quot;ciphers&quot;: &quot;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305&quot; } storage-network​ By default, Longhorn uses the default management network in the Harvester cluster that is limited to a single interface and shared with other workloads cluster-wide. This setting allows you to configure a segregated storage network when network isolation is preferred. For details, please refer to the Harvester Storage Network caution Any change to storage-network requires shutting down all VMs before applying this setting. IP Range should be IPv4 CIDR format and 4 times the number of your cluster nodes. Default: &quot;&quot; Example​ { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } support-bundle-image​ Available as of v1.2.0 This setting allows you to configure the support bundle image, with various versions available in rancher/support-bundle-kit. Default: { &quot;repository&quot;: &quot;rancher/support-bundle-kit&quot;, &quot;tag&quot;: &quot;v0.0.25&quot;, &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot; } support-bundle-namespaces​ Available as of v1.2.0 This setting allows you to specify additional namespaces when collecting a support bundle. The support bundle will only capture resources from pre-defined namespaces by default. Here is the pre-defined namespaces list: cattle-dashboardscattle-fleet-local-systemcattle-fleet-systemcattle-fleet-clusters-systemcattle-monitoring-systemfleet-localharvester-systemlocallonghorn-systemcattle-logging-system If you select more namespaces, it will append to the pre-defined namespaces list. Default: none support-bundle-timeout​ Available as of v1.2.0 This setting allows you to define the number of minutes Harvester allows for the completion of the support bundle generation process. The process is considered to have failed when the data collection and file packing tasks are not completed within the configured number of minutes. Harvester will not continue or retry support bundle generation processes that have timed out. When the value is &quot;0&quot;, the timeout feature is disabled. Default: 10 support-bundle-expiration​ Available as of v1.3.0 This setting allows you to define the number of minutes Harvester waits before deleting a support bundle that has been packaged but not downloaded (either deliberately or unsuccessfully) or retained. The minimum value is 30. Default: 30 upgrade-checker-enabled​ This setting allows you to automatically check if there's an upgrade available for Harvester. Default: true Example​ false upgrade-checker-url​ This setting allows you to configure the URL for the upgrade check of Harvester. Can only be used if the upgrade-checker-enabled setting is set to true. Default: https://harvester-upgrade-responder.rancher.io/v1/checkupgrade Example​ https://your.upgrade.checker-url/v99/checkupgrade vip-pools​ Deprecated as of v1.2.0, use IP Pool instead This setting allows you to configure the global or namespace IP address pools of the VIP by CIDR or IP range. Default: {} note Configuring multi-CIDR or IP range from UI is only available from Harvester v1.1.1. Example​ { &quot;default&quot;: &quot;172.16.0.0/24,172.16.1.0/24&quot;, &quot;demo&quot;: &quot;172.16.2.50-172.16.2.100,172.16.2.150-172.16.3.200&quot; } vm-force-reset-policy​ This setting allows you to force reschedule VMs when a node is unavailable. When a node turns to be Not Ready, it will force delete the VM on that node and reschedule it to another available node after a period of seconds. Default: {&quot;enable&quot;:true, &quot;period&quot;:300} note When a host is unavailable or is powered off, the VM only reboots and does not migrate. Example​ { &quot;enable&quot;: &quot;true&quot;, &quot;period&quot;: 300 } UI Settings​ branding​ Available as of v1.2.0 This setting allows you to globally re-brand the UI by customizing the Harvester product name, logos, and color scheme. Default: Harvester You can set the following options and values: Private Label: This option replaces &quot;Harvester&quot; with the value you provide in most places.Logo: Upload light and dark logos to replace the Harvester logo in the top-level navigation header.Favicon: Upload an icon to replace the Harvester favicon in the browser tab.Primary Color: You can override the primary color used throughout the UI with a custom color of your choice.Link Color: You can override the link color used throughout the UI with a custom color of your choice. ui-index​ This setting allows you to configure the HTML index location for the UI. Default: https://releases.rancher.com/harvester-ui/dashboard/latest/index.html Example​ https://your.static.dashboard-ui/index.html ui-plugin-index​ This setting allows you to configure the JS address for the Harvester plugin (when accessing Harvester from Rancher). Default: https://releases.rancher.com/harvester-ui/plugin/harvester-latest/harvester-latest.umd.min.js Example​ https://your.static.dashboard-ui/*.umd.min.js ui-source​ This setting allows you to configure how to load the UI source. You can set the following values: auto: The default. Auto-detect whether to use bundled UI or not.external: Use external UI source.bundled: Use the bundled UI source. Example​ external ","keywords":"","version":"v1.4 (dev)"},{"title":"Storage Network","type":0,"sectionRef":"#","url":"/v1.4/advanced/storagenetwork","content":"Storage Network Harvester uses Longhorn as its built-in storage system to provide block device volumes for VMs and Pods. If the user wishes to isolate Longhorn replication traffic from the Kubernetes cluster network (i.e. the management network) or other cluster-wide workloads. Users can allocate a dedicated storage network for Longhorn replication traffic to get better network bandwidth and performance. For more information, please see Longhorn Storage Network note Configuring Longhorn settings directly is not recommended, as this can lead to untested situations. Prerequisites​ There are some prerequisites before configuring the Harvester Storage Network setting. Well-configured Cluster Network and VLAN Config. Users have to ensure the Cluster Network is configured and VLAN Config will cover all nodes and ensure the network connectivity is working and expected in all nodes. All VMs should be stopped. We recommend checking the VM status with the following command and should get an empty result.kubectl get -A vmi All pods that are attached to Longhorn Volumes should be stopped. Users could skip this step with the Harvester Storage Network setting. Harvester will stop Longhorn-related pods automatically. All ongoing image uploads or downloads should be either completed or deleted. caution If the Harvester cluster was upgraded from v1.0.3, please check if Whereabouts CNI is installed properly before you move on to the next step. We will always recommend following this guide to check. Issue 3168 describes that the Harvester cluster will not always install Whereabouts CNI properly. Verify the ippools.whereabouts.cni.cncf.io CRD exists with the following command. kubectl get crd ippools.whereabouts.cni.cncf.io If the Harvester cluster doesn't have ippools.whereabouts.cni.cncf.io, please add these two CRDs before configuring storage-network setting. kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_ippools.yaml kubectl apply -f https://raw.githubusercontent.com/harvester/harvester/v1.1.0/deploy/charts/harvester/dependency_charts/whereabouts/crds/whereabouts.cni.cncf.io_overlappingrangeipreservations.yaml Configuration Example​ VLAN ID Please check with your network switch setting, and provide a dedicated VLAN ID for Storage Network. Well-configured Cluster Network and VLAN Config Please refer Networking page for more details and configure Cluster Network and VLAN Config but not Networks. IP range for Storage Network IP range should not conflict or overlap with Kubernetes cluster networks(10.42.0.0/16, 10.43.0.0/16, 10.52.0.0/16 and 10.53.0.0/16 are reserved).IP range should be in IPv4 CIDR format and Longhorn pods use Storage Network as follows: instance-manger-e and instance-manager-r pods: These require 2 IPs per node. During an upgrade, two versions of these pods will exist (old and new), and the old version will be deleted once the upgrade is successful.backing-image-ds pods: These are employed to process on-the-fly uploads and downloads of backing image data sources. These pods will be removed once the image uploads or downloads are completed.backing-image-manager pods: 1 IP per disk, similar to the instance manager pods. Two versions of these will coexist during an upgrade, and the old ones will be removed after the upgrade is completed.The required number of IPs is calculated using a simple formula: Required Number of IPs = Number of Nodes * 4 + Number of Disks * 2 + Number of Images to Download/Upload For example, if your cluster has five nodes, each node has two disks, and ten images will be uploaded simultaneously, the IP range should be greater than or equal to /26 (5 * 4 + 5 * 2 * 2 + 10 = 50). We will take the following configuration as an example to explain the details of the Storage Network VLAN ID for Storage Network: 100Cluster Network: storageIP range: 192.168.0.0/24 Configuration Process​ Harvester will create Multus NetworkAttachmentDefinition from the configuration, stop pods related to Longhorn Volume, update Longhorn setting, and restart previous pods. Before Applying Harvester Storage Network Setting​ Here we have two cases. Expect that VM VLAN traffic and Longhorn Storage Network use the same group of physical interfaces.Expect that VM VLAN traffic and Longhorn Storage Network use different physical interfaces. Longhorn will send replication traffic through the specific interfaces shown as the red line in the figure. Same Physical Interfaces​ Take eth2 and eth3 as an example for VM VLAN traffic and Longhorn Storage Network simultaneously. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth2 and eth3 and remember the ClusterNetwork name for the further step. Different Physical Interfaces​ eth2 and eth3 are for VM VLAN Traffic. eth4 and eth5 are for Longhorn Storage Network. Please refer Networking page to configure ClusterNetwork and VLAN Config with eth4 and eth5 for Storage Network and remember the ClusterNetwork name for the further step. Harvester Storage Network Setting​ Harvester Storage Network setting will need range, clusterNetwork, vlan field to construct Multus NetworkAttachmentDefinition for Storage Network usage. You could apply this setting via Web UI or CLI. Web UI​ Harvester Storage Network setting could be easily modified on the Settings &gt; storage-network page. CLI​ Users could use this command to edit Harvester Storage Network setting. kubectl edit settings.harvesterhci.io storage-network The value format is JSON string or empty string as shown in below. { &quot;vlan&quot;: 100, &quot;clusterNetwork&quot;: &quot;storage&quot;, &quot;range&quot;: &quot;192.168.0.0/24&quot; } The full configuration will be like this example. apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: name: storage-network value: '{&quot;vlan&quot;:100,&quot;clusterNetwork&quot;:&quot;storage&quot;,&quot;range&quot;:&quot;192.168.0.0/24&quot;}' caution Because of the design, Harvester will treat extra and insignificant characters in JSON string as a different configuration. After Applying Harvester Storage Network Setting​ After applying Harvester's Storage Network setting, Harvester will stop all pods that are related to Longhorn volumes. Currently, Harvester has some pods listed below that will be stopped during setting. PrometheusGrafanaAlertmanagerVM Import Controller Harvester will also create a new NetworkAttachmentDefinition and update the Longhorn Storage Network setting. Once the Longhorn setting is updated, Longhorn will restart all instance-manager-r, instance-manager-e, and backing-image-manager pods to apply the new network configuration, and Harvester will restart the pods. note Harvester will not start VM automatically. Users should check whether the configuration is completed or not in the next section and start VM manually on demand. Verify Configuration is Completed​ Step 1​ Check if Harvester Storage Network setting's status is True and the type is configured. kubectl get settings.harvesterhci.io storage-network -o yaml Completed Setting Example: apiVersion: harvesterhci.io/v1beta1 kind: Setting metadata: annotations: storage-network.settings.harvesterhci.io/hash: da39a3ee5e6b4b0d3255bfef95601890afd80709 storage-network.settings.harvesterhci.io/net-attach-def: &quot;&quot; storage-network.settings.harvesterhci.io/old-net-attach-def: &quot;&quot; creationTimestamp: &quot;2022-10-13T06:36:39Z&quot; generation: 51 name: storage-network resourceVersion: &quot;154638&quot; uid: 2233ad63-ee52-45f6-a79c-147e48fc88db status: conditions: - lastUpdateTime: &quot;2022-10-13T13:05:17Z&quot; reason: Completed status: &quot;True&quot; type: configured Step 2​ Verify the readiness of all Longhorn instance-manager-e, instance-manager-r, and backing-image-manager pods, and confirm that their networks are correctly configured. Execute the following command to inspect a pod's details: kubectl -n longhorn-system describe pod &lt;pod-name&gt; If you encounter an event resembling the following one, the Storage Network might have run out of its available IPs: Events: Type Reason Age From Message ---- ------ ---- ---- ------- .... Warning FailedCreatePodSandBox 2m58s kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox &quot;04e9bc160c4f1da612e2bb52dadc86702817ac557e641a3b07b7c4a340c9fc48&quot;: plugin type=&quot;multus&quot; name=&quot;multus-cni-network&quot; failed (add): [longhorn-system/ba cking-image-ds-default-image-lxq7r/7d6995ee-60a6-4f67-b9ea-246a73a4df54:storagenetwork-sdfg8]: error adding container to network &quot;storagenetwork-sdfg8&quot;: erro r at storage engine: Could not allocate IP in range: ip: 172.16.0.1 / - 172.16.0.6 / range: net.IPNet{IP:net.IP{0xac, 0x10, 0x0, 0x0}, Mask:net.IPMask{0xff, 0xff, 0xff, 0xf8}} .... Please reconfigure the Storage Network with a sufficient IP range. note If the Storage Network has run out of IPs, you might encounter the same error when you upload/download images. Please delete the related images and reconfigure the Storage Network with a sufficient IP range. Step 3​ Check the k8s.v1.cni.cncf.io/network-status annotations and ensure that an interface named lhnet1 exists, with an IP address within the designated IP range. Users could use the following command to show all Longhorn Instance Manager to verify. kubectl get pods -n longhorn-system -l longhorn.io/component=instance-manager -o yaml Correct Network Example: apiVersion: v1 kind: Pod metadata: annotations: cni.projectcalico.org/containerID: 2518b0696f6635896645b5546417447843e14208525d3c19d7ec6d7296cc13cd cni.projectcalico.org/podIP: 10.52.2.122/32 cni.projectcalico.org/podIPs: 10.52.2.122/32 k8s.v1.cni.cncf.io/network-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] k8s.v1.cni.cncf.io/networks: '[{&quot;namespace&quot;: &quot;harvester-system&quot;, &quot;name&quot;: &quot;storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;}]' k8s.v1.cni.cncf.io/networks-status: |- [{ &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;ips&quot;: [ &quot;10.52.2.122&quot; ], &quot;default&quot;: true, &quot;dns&quot;: {} },{ &quot;name&quot;: &quot;harvester-system/storagenetwork-95bj4&quot;, &quot;interface&quot;: &quot;lhnet1&quot;, &quot;ips&quot;: [ &quot;192.168.0.3&quot; ], &quot;mac&quot;: &quot;2e:51:e6:31:96:40&quot;, &quot;dns&quot;: {} }] kubernetes.io/psp: global-unrestricted-psp longhorn.io/last-applied-tolerations: '[{&quot;key&quot;:&quot;kubevirt.io/drain&quot;,&quot;operator&quot;:&quot;Exists&quot;,&quot;effect&quot;:&quot;NoSchedule&quot;}]' Omitted... Start VM Manually​ After verifying the configuration, users could start VM manually on demand.","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-network-attachment-definition","content":"Create a Network Attachment Definition POST /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions Create a NetworkAttachmentDefinition object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Cluster Network","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-cluster-network","content":"Create a Cluster Network POST /apis/network.harvesterhci.io/v1beta1/clusternetworks Create a ClusterNetwork object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Key Pair","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-key-pair","content":"Create a Key Pair POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs Create a KeyPair object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Support Bundle","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-support-bundle","content":"Create a Support Bundle POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles Create a SupportBundle object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create an Upgrade","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-upgrade","content":"Create an Upgrade POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades Create a Upgrade object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Node Network","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-node-network","content":"Create a Node Network POST /apis/network.harvesterhci.io/v1beta1/nodenetworks Create a NodeNetwork object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine-image","content":"Create a Virtual Machine Image POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages Create a VirtualMachineImage object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-persistent-volume-claim","content":"Create a Persistent Volume Claim POST /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims Create a PersistentVolumeClaim object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine-instance-migration","content":"Create a Virtual Machine Instance Migration POST /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations Create a VirtualMachineInstanceMigration object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine-template","content":"Create a Virtual Machine Template POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates Create a VirtualMachineTemplate object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Cluster Network","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-cluster-network","content":"Delete a Cluster Network DELETE /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Delete a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Key Pair","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-key-pair","content":"Delete a Key Pair DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Delete a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine-restore","content":"Create a Virtual Machine Restore POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores Create a VirtualMachineRestore object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine","content":"Replace a Virtual Machine PUT /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-network-attachment-definition","content":"Delete a Network Attachment Definition DELETE /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Delete a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Node Network","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-node-network","content":"Delete a Node Network DELETE /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Delete a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-persistent-volume-claim","content":"Delete a Persistent Volume Claim DELETE /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Delete a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Support Bundle","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-support-bundle","content":"Delete a Support Bundle DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Delete a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine","content":"Delete a Virtual Machine DELETE /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine-backup","content":"Delete a Virtual Machine Backup DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete an Upgrade","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-upgrade","content":"Delete an Upgrade DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Delete a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine-backup","content":"Replace a Virtual Machine Backup PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Replace a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.3/api/replace-namespaced-virtual-machine-template-version","content":"Replace a Virtual Machine Template Version PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.3 (latest)"},{"title":"Delete a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine-restore","content":"Delete a Virtual Machine Restore DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine-instance-migration","content":"Delete a Virtual Machine Instance Migration DELETE /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine-image","content":"Delete a Virtual Machine Image DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine-template","content":"Delete a Virtual Machine Template DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Harvester APIs","type":0,"sectionRef":"#","url":"/v1.4/api/harvester-apis","content":"Version: v1beta1 Harvester APIs This section introduces the APIs of the Harvester server. You can find out more about Harvester's API definitions here.","keywords":"","version":"v1.4 (dev)"},{"title":"Delete a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.4/api/delete-namespaced-virtual-machine-template-version","content":"Delete a Virtual Machine Template Version DELETE /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Delete a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters gracePeriodSeconds integer The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately. orphanDependents boolean Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the &quot;orphan&quot; finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both. propagationPolicy string Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground. application/jsonapplication/yaml Body required apiVersion stringrequired dryRun string[] gracePeriodSeconds int64 kind stringrequired orphanDependents boolean preconditions object resourceVersion string uid string propagationPolicy string Responses​ 200401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired code int32 details object causes object[] Array [ field string message string reason string ] group string kind string name string retryAfterSeconds int32 uid string kind stringrequired message string metadata object continue string remainingItemCount int64 resourceVersion string selfLink string reason string status string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Network Attachment Definitions","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-network-attachment-definition","content":"List Network Attachment Definitions GET /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions Get a list of NetworkAttachmentDefinition objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Cluster Networks","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-cluster-network","content":"List Cluster Networks GET /apis/network.harvesterhci.io/v1beta1/clusternetworks Get a list of ClusterNetwork objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Key Pairs","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-key-pair","content":"List Key Pairs GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs Get a list of KeyPair objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Key Pairs For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-key-pair-for-all-namespaces","content":"List Key Pairs For All Namespaces GET /apis/harvesterhci.io/v1beta1/keypairs Get a list of all KeyPair objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Node Networks","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-node-network","content":"List Node Networks GET /apis/network.harvesterhci.io/v1beta1/nodenetworks Get a list of NodeNetwork objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Support Bundles","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-support-bundle","content":"List Support Bundles GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles Get a list of SupportBundle objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Upgrades","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-upgrade","content":"List Upgrades GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades Get a list of Upgrade objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Persistent Volume Claims","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-persistent-volume-claim","content":"List Persistent Volume Claims GET /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims Get a list of PersistentVolumeClaim objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Images","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-image","content":"List Virtual Machine Images GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages Get a list of VirtualMachineImage objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Instance Migrations","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-instance-migration","content":"List Virtual Machine Instance Migrations GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations Get a list of VirtualMachineInstanceMigration objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Templates","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-template","content":"List Virtual Machine Templates GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates Get a list of VirtualMachineTemplate objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Restores","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-restore","content":"List Virtual Machine Restores GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores Get a list of VirtualMachineRestore objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Network Attachment Definitions For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-network-attachment-definition-for-all-namespaces","content":"List Network Attachment Definitions For All Namespaces GET /apis/k8s.cni.cncf.io/v1/network-attachment-definitions Get a list of all NetworkAttachmentDefinition objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Persistent Volume Claims For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-persistent-volume-claim-for-all-namespaces","content":"List Persistent Volume Claims For All Namespaces GET /api/v1/persistentvolumeclaims Get a list of all PersistentVolumeClaim objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Support Bundles For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-support-bundle-for-all-namespaces","content":"List Support Bundles For All Namespaces GET /apis/harvesterhci.io/v1beta1/supportbundles Get a list of all SupportBundle objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Upgrades For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-upgrade-for-all-namespaces","content":"List Upgrades For All Namespaces GET /apis/harvesterhci.io/v1beta1/upgrades Get a list of all Upgrade objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Instances","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-instance","content":"List Virtual Machine Instances GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstances Get a list of VirtualMachineInstance objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object activePods object property name* string conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] evacuationNodeName string fsFreezeStatus string guestOSInfo object id string kernelRelease string kernelVersion string machine string name string prettyName string version string versionId string interfaces object[] Array [ infoSource string interfaceName string ipAddress string ipAddresses string[] mac string name string ] launcherContainerImageVersion string migrationMethod string migrationState object abortRequested boolean abortStatus string completed boolean endTimestamp string failed boolean migrationConfiguration object allowAutoConverge boolean allowPostCopy boolean bandwidthPerMigration string completionTimeoutPerGiB int64 disableTLS boolean network string nodeDrainTaintKey string parallelMigrationsPerCluster int64 parallelOutboundMigrationsPerNode int64 progressTimeout int64 unsafeMigrationOverride boolean migrationPolicyName string migrationUid string mode string sourceNode string startTimestamp string targetAttachmentPodUID string targetCPUSet int32[] targetDirectMigrationNodePorts object property name* int32 targetNode string targetNodeAddress string targetNodeDomainDetected boolean targetNodeTopology string targetPod string migrationTransport string nodeName string phase string phaseTransitionTimestamps object[] Array [ phase string phaseTransitionTimestamp string ] qosClass string reason string runtimeUser int64 topologyHints object tscFrequency int64 virtualMachineRevisionName string volumeStatus object[] Array [ hotplugVolume object attachPodName string attachPodUID string memoryDumpVolume object claimName string endTimestamp string startTimestamp string targetFileName string message string name stringrequired persistentVolumeClaimInfo object accessModes string[] capacity object property name* string Default value: [object Object] filesystemOverhead string preallocated boolean requests object property name* string Default value: [object Object] volumeMode string phase string reason string size int64 target stringrequired ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machines","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine","content":"List Virtual Machines GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines Get a list of VirtualMachine objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Backups","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-backup","content":"List Virtual Machine Backups GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups Get a list of VirtualMachineBackup objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Images For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-image-for-all-namespaces","content":"List Virtual Machine Images For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachineimages Get a list of all VirtualMachineImage objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Instance Migrations For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-instance-migration-for-all-namespaces","content":"List Virtual Machine Instance Migrations For All Namespaces GET /apis/kubevirt.io/v1/virtualmachineinstancemigrations Get a list of all VirtualMachineInstanceMigration objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Template Versions","type":0,"sectionRef":"#","url":"/v1.4/api/list-namespaced-virtual-machine-template-version","content":"List Virtual Machine Template Versions GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions Get a list of VirtualMachineTemplateVersion objects in a namespace. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Templates For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-template-for-all-namespaces","content":"List Virtual Machine Templates For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinetemplates Get a list of all VirtualMachineTemplate objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Restores For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-restore-for-all-namespaces","content":"List Virtual Machine Restores For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinerestores Get a list of all VirtualMachineRestore objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Cluster Network","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-cluster-network","content":"Patch a Cluster Network PATCH /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Patch a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Key Pair","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-key-pair","content":"Patch a Key Pair PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Patch a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-network-attachment-definition","content":"Patch a Network Attachment Definition PATCH /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Patch a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Node Network","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-node-network","content":"Patch a Node Network PATCH /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Patch a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-persistent-volume-claim","content":"Patch a Persistent Volume Claim PATCH /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Patch a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Support Bundle","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-support-bundle","content":"Patch a Support Bundle PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Patch a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch an Upgrade","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-upgrade","content":"Patch an Upgrade PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Patch a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Backups For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-backup-for-all-namespaces","content":"List Virtual Machine Backups For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinebackups Get a list of all VirtualMachineBackup objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine","content":"Create a Virtual Machine POST /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines Create a VirtualMachine object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine-image","content":"Patch a Virtual Machine Image PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine-backup","content":"Create a Virtual Machine Backup POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups Create a VirtualMachineBackup object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine-instance-migration","content":"Patch a Virtual Machine Instance Migration PATCH /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.4/api/create-namespaced-virtual-machine-template-version","content":"Create a Virtual Machine Template Version POST /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions Create a VirtualMachineTemplateVersion object. Request​ Path Parameters namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Responses​ 200201202401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine-restore","content":"Patch a Virtual Machine Restore PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machines For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-for-all-namespaces","content":"List Virtual Machines For All Namespaces GET /apis/kubevirt.io/v1/virtualmachines Get a list of all VirtualMachine objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine-template","content":"Patch a Virtual Machine Template PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Cluster Network","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-cluster-network","content":"Read a Cluster Network GET /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Get a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-network-attachment-definition","content":"Read a Network Attachment Definition GET /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Get a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Key Pair","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-key-pair","content":"Read a Key Pair GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Get a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine","content":"Patch a Virtual Machine PATCH /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Support Bundle","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-support-bundle","content":"Read a Support Bundle GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Get a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Node Network","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-node-network","content":"Read a Node Network GET /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Get a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-persistent-volume-claim","content":"Read a Persistent Volume Claim GET /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Get a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read an Upgrade","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-upgrade","content":"Read an Upgrade GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Get a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine-backup","content":"Patch a Virtual Machine Backup PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-image","content":"Read a Virtual Machine Image GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Instances For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-instance-for-all-namespaces","content":"List Virtual Machine Instances For All Namespaces GET /apis/kubevirt.io/v1/virtualmachineinstances Get a list of all VirtualMachineInstance objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object activePods object property name* string conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] evacuationNodeName string fsFreezeStatus string guestOSInfo object id string kernelRelease string kernelVersion string machine string name string prettyName string version string versionId string interfaces object[] Array [ infoSource string interfaceName string ipAddress string ipAddresses string[] mac string name string ] launcherContainerImageVersion string migrationMethod string migrationState object abortRequested boolean abortStatus string completed boolean endTimestamp string failed boolean migrationConfiguration object allowAutoConverge boolean allowPostCopy boolean bandwidthPerMigration string completionTimeoutPerGiB int64 disableTLS boolean network string nodeDrainTaintKey string parallelMigrationsPerCluster int64 parallelOutboundMigrationsPerNode int64 progressTimeout int64 unsafeMigrationOverride boolean migrationPolicyName string migrationUid string mode string sourceNode string startTimestamp string targetAttachmentPodUID string targetCPUSet int32[] targetDirectMigrationNodePorts object property name* int32 targetNode string targetNodeAddress string targetNodeDomainDetected boolean targetNodeTopology string targetPod string migrationTransport string nodeName string phase string phaseTransitionTimestamps object[] Array [ phase string phaseTransitionTimestamp string ] qosClass string reason string runtimeUser int64 topologyHints object tscFrequency int64 virtualMachineRevisionName string volumeStatus object[] Array [ hotplugVolume object attachPodName string attachPodUID string memoryDumpVolume object claimName string endTimestamp string startTimestamp string targetFileName string message string name stringrequired persistentVolumeClaimInfo object accessModes string[] capacity object property name* string Default value: [object Object] filesystemOverhead string preallocated boolean requests object property name* string Default value: [object Object] volumeMode string phase string reason string size int64 target stringrequired ] ] kind stringrequired metadata object continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-instance-migration","content":"Read a Virtual Machine Instance Migration GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-template","content":"Read a Virtual Machine Template GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Cluster Network","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-cluster-network","content":"Replace a Cluster Network PUT /apis/network.harvesterhci.io/v1beta1/clusternetworks/{name:[a-z0-9][a-z0-9\\-]*} Update a ClusterNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Patch a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.4/api/patch-namespaced-virtual-machine-template-version","content":"Patch a Virtual Machine Template Version PATCH /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Patch a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/json-patch+jsonapplication/merge-patch+json Body required object Responses​ 200401 OK application/json SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-restore","content":"Read a Virtual Machine Restore GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Network Attachment Definition","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-network-attachment-definition","content":"Replace a Network Attachment Definition PUT /apis/k8s.cni.cncf.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/network-attachment-definitions/{name:[a-z0-9][a-z0-9\\-]*} Update a NetworkAttachmentDefinition object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired config stringrequired Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Key Pair","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-key-pair","content":"Replace a Key Pair PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/keypairs/{name:[a-z0-9][a-z0-9\\-]*} Update a KeyPair object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired publicKey stringrequired status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] fingerPrint string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Node Network","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-node-network","content":"Replace a Node Network PUT /apis/network.harvesterhci.io/v1beta1/nodenetworks/{name:[a-z0-9][a-z0-9\\-]*} Update a NodeNetwork object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object description string nic string nodeName stringrequired type string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] networkIDs int32[] networkLinkStatus object property name* object index int32 mac string masterIndex int32 name stringrequired promiscuous boolean state string type string nics object[] Array [ index int32required masterIndex int32 name stringrequired state stringrequired type stringrequired usedByManagementNetwork boolean usedByVlanNetwork boolean ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Support Bundle","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-support-bundle","content":"Replace a Support Bundle PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/supportbundles/{name:[a-z0-9][a-z0-9\\-]*} Update a SupportBundle object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description stringrequired issueURL string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] filename string filesize int64 progress int32 state string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Persistent Volume Claim","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-persistent-volume-claim","content":"Replace a Persistent Volume Claim PUT /api/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/persistentvolumeclaims/{name:[a-z0-9][a-z0-9\\-]*} Update a PersistentVolumeClaim object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object accessModes string[] allocatedResources object property name* string Default value: [object Object] capacity object property name* string Default value: [object Object] conditions object[] Array [ lastProbeTime string Default value: [object Object] lastTransitionTime string Default value: [object Object] message string reason string status stringrequired type stringrequired ] phase string Possible values: [Bound, Lost, Pending] resizeStatus string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace an Upgrade","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-upgrade","content":"Replace an Upgrade PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/upgrades/{name:[a-z0-9][a-z0-9\\-]*} Update a Upgrade object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired image string logEnabled boolean Default value: false version string status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] imageID string nodeStatuses object property name* object message string reason string state string previousVersion string repoInfo string singleNode string upgradeLog string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"List Virtual Machine Template Versions For All Namespaces","type":0,"sectionRef":"#","url":"/v1.4/api/list-virtual-machine-template-version-for-all-namespaces","content":"List Virtual Machine Template Versions For All Namespaces GET /apis/harvesterhci.io/v1beta1/virtualmachinetemplateversions Get a list of all VirtualMachineTemplateVersion objects. Request​ Query Parameters continue string The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server the server will respond with a 410 ResourceExpired error indicating the client must restart their list without the continue field. This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications. fieldSelector string A selector to restrict the list of returned objects by their fields. Defaults to everything. includeUninitialized boolean If true, partially initialized resources are included in the response. labelSelector string A selector to restrict the list of returned objects by their labels. Defaults to everything limit integer limit is a maximum number of responses to return for a list call. If more items exist, the server will set the continue field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true. The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned. resourceVersion string When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. timeoutSeconds integer TimeoutSeconds for the list/watch call. watch boolean Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired items object[]required Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 ] kind stringrequired metadata objectrequired continue string remainingItemCount int64 resourceVersion string selfLink string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Instance Migration","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine-instance-migration","content":"Replace a Virtual Machine Instance Migration PUT /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstancemigrations/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineInstanceMigration object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired vmiName string status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] phase string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Image","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine-image","content":"Replace a Virtual Machine Image PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineimages/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineImage object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checksum string description string displayName stringrequired pvcName string pvcNamespace string retry int32 sourceType stringrequired storageClassParameters object property name* string url string status object appliedUrl string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] failed int32 lastFailedTime string progress int32 size int64 storageClassName string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Template","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine-template","content":"Replace a Virtual Machine Template PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplates/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineTemplate object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec object defaultVersionId string description string status object defaultVersion int32 latestVersion int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Restore","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine-restore","content":"Replace a Virtual Machine Restore PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinerestores/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineRestore object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired deletionPolicy string newVM boolean target objectrequired apiGroup string kind stringrequired name stringrequired virtualMachineBackupName stringrequired virtualMachineBackupNamespace stringrequired status object complete boolean conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] deletedVolumes string[] restoreTime string restores object[] Array [ persistentVolumeClaimSpec object metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string volumeBackupName string volumeName string ] targetUID string Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Authentication","type":0,"sectionRef":"#","url":"/v1.4/authentication","content":"Authentication After installation, user will be prompted to set the password for the default admin user on the first-time login. note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","keywords":"Harvester harvester Rancher rancher Authentication","version":"v1.4 (dev)"},{"title":"Developer Mode","type":0,"sectionRef":"#","url":"/v1.4/developer/developer-mode-installation","content":"Developer Mode attention Developer mode is intended to be used for development and testing purposes. Usage of this mode in production environments is not supported. Prerequisites​ The node has passed the host-checkHelm 3 and Git are installed on your local machine. Installation of the First Node​ You can install Harvester on an RKE2 cluster using the Helm CLI. For more information about installing and configuring the Harvester Helm chart, see the readme. Create an RKE2 configuration file. sudo mkdir -p /etc/rancher/rke2 cat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml disable: - rke2-snapshot-controller - rke2-snapshot-controller-crd - rke2-snapshot-validation-webhook node-label: - harvesterhci.io/managed=true token: token cni: - multus - canal EOF Install RKE2. curl -sfL https://get.rke2.io | sudo sh - sudo systemctl enable rke2-server.service --now Create a kubeconfig file. mkdir -p ~/.kube sudo cp /etc/rancher/rke2/rke2.yaml ~/.kube/config sudo chown $(id -u):$(id -g) ~/.kube/config Install system-upgrade-controller. This Kubernetes-native upgrade controller for nodes installs upgrade.cattle.io/v1 CRDs. kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.13.1/system-upgrade-controller.yaml note If you are unable to locate the kubectl binary in /usr/local/bin, check /var/lib/rancher/rke2/bin. Create the cattle-system namespace. kubectl create ns cattle-system Add the Rancher chart repository. helm repo add rancher-latest https://releases.rancher.com/server-charts/latest Install the Rancher v2.7.5 chart. helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set tls=external \\ --set rancherImagePullPolicy=IfNotPresent \\ --set rancherImage=rancher/rancher \\ --set rancherImageTag=v2.7.5 \\ --set noDefaultAdmin=false \\ --set features=&quot;multi-cluster-management=false\\,multi-cluster-management-agent=false&quot; \\ --set useBundledSystemChart=true \\ --set bootstrapPassword=admin Clone the rancher/charts repository. git clone https://github.com/rancher/charts -b dev-v2.7 Install the rancher-monitoring-crd chart. helm install rancher-monitoring-crd ./charts/charts/rancher-monitoring-crd/102.0.2+up40.1.2/ Create the harvester-system namespace. kubectl create ns harvester-system Clone the harvester/harvester repository. git clone https://github.com/harvester/harvester.git Install the harvester-crd chart. helm install harvester-crd ./harvester/deploy/charts/harvester-crd --namespace harvester-system Install the Harvester chart using kube-vip running on a static IP. VIP_ADDRESS=&quot;replace with an IP which is allocated to any device, such as 192.168.5.131&quot; helm install harvester ./harvester/deploy/charts/harvester --namespace harvester-system \\ --set harvester-node-disk-manager.enabled=true \\ --set &quot;harvester-node-disk-manager.labelFilter={COS_*,HARV_*}&quot; \\ --set harvester-network-controller.enabled=true \\ --set harvester-network-controller.vipEnabled=true \\ --set harvester-load-balancer.enabled=true \\ --set kube-vip.enabled=true \\ --set kube-vip-cloud-provider.enabled=true \\ --set longhorn.enabled=true \\ --set longhorn.defaultSettings.defaultDataPath=/var/lib/harvester/defaultdisk \\ --set longhorn.defaultSettings.taintToleration=kubevirt.io/drain:NoSchedule \\ --set rancherEmbedded=true \\ --set service.vip.enabled=true \\ --set service.vip.mode=static \\ --set service.vip.ip=${VIP_ADDRESS} Access the Harvester UI at https://${VIP_ADDRESS}. The default password is admin. Installation of Other Nodes​ Create an RKE2 configuration file. sudo mkdir -p /etc/rancher/rke2 cat &lt;&lt;EOF | sudo tee /etc/rancher/rke2/config.yaml server: https://&lt;vip address&gt;:9345 token: token EOF Install the RKE2 agent. curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=&quot;agent&quot; sudo sh - sudo systemctl enable rke2-agent.service --now Uninstallation​ sudo /usr/local/bin/rke2-uninstall.sh ","keywords":"Harvester harvester Rancher rancher Developer Mode Installation","version":"v1.4 (dev)"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/v1.4/faq","content":"FAQ This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node?​ $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard?​ username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster?​ Option 1. You can download the kubeconfig file from the support page of the Harvester dashboard. Option 2. You can get the kubeconfig file from one of the Harvester management nodes. E.g., $ sudo su $ cat /etc/rancher/rke2/rke2.yaml How to install the qemu-guest-agent of a running VM?​ # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/reference/cli.html#clean How can I reset the administrator password?​ In case you forget the administrator password, you can reset it via the command line. SSH to one of the management node and run the following command: # switch to root and run $ kubectl -n cattle-system exec $(kubectl --kubeconfig $KUBECONFIG -n cattle-system get pods -l app=rancher --no-headers | head -1 | awk '{ print $1 }') -c rancher -- reset-password New password for default administrator (user-xxxxx): &lt;new_password&gt; I added an additional disk with partitions. Why is it not getting detected?​ As of Harvester v1.0.2, we no longer support adding additional partitioned disks, so be sure to delete all partitions first (e.g., using fdisk). Why are there some Harvester pods that become ErrImagePull/ImagePullBackOff?​ This is likely because your Harvester cluster is an air-gapped setup, and some pre-loaded container images are missing. Kubernetes has a mechanism that does garbage collection against bloated image stores. When the partition which stores container images is over 85% full, kubelet tries to prune the images based on the last time they were used, starting with the oldest, until the occupancy is lower than 80%. These numbers (85% and 80%) are default High/Low thresholds that come with Kubernetes. To recover from this state, do one of the following depending on the cluster's configuration: Pull the missing images from sources outside of the cluster (if it's an air-gapped environment, you might need to set up an HTTP proxy beforehand).Manually import the images from the Harvester ISO image. note Take v1.1.2 as an example, download the Harvester ISO image from the official URL. Then extract the image list from the ISO image to decide which image tarball we're going to import. For instance, we want to import the missing container image rancher/harvester-upgrade $ curl -sfL https://releases.rancher.com/harvester/v1.1.2/harvester-v1.1.2-amd64.iso -o harvester.iso $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images-lists images-lists $ grep -R &quot;rancher/harvester-upgrade&quot; images-lists/ images-lists/harvester-images-v1.1.2.txt:docker.io/rancher/harvester-upgrade:v1.1.2 Find out the location of the image tarball, and extract it from the ISO image. Decompress the extracted zstd image tarball. $ xorriso -osirrox on -indev harvester.iso -extract /bundle/harvester/images/harvester-images-v1.1.2.tar.zst harvester.tar.zst $ zstd -d --rm harvester.tar.zst Upload the image tarball to the Harvester nodes that need recover. Finally, execute the following command to import the container images on each of them. $ ctr -n k8s.io images import harvester.tar $ rm harvester.tar Find the missing images on that node from the other nodes, then export the images from the node where the images still exist and import them on the missing node. To prevent this from happening, we recommend cleaning up unused container images from the previous version after each successful Harvester upgrade if the image store disk space is stressed. We provided a harv-purge-images script that makes cleaning up disk space easy, especially for container image storage. The script has to be executed on each Harvester node. For example, if the cluster was originally in v1.1.2, and now it gets upgraded to v1.2.0, you can do the following to discard the container images that are only used in v1.1.2 but no longer needed in v1.2.0: # on each node $ ./harv-purge-images.sh v1.1.2 v1.2.0 caution The script only downloads the image lists and compares the two to calculate the difference between the two versions. It does not communicate with the cluster and, as a result, does not know what version the cluster was upgraded from.We published image lists for each version released since v1.1.0. For clusters older than v1.1.0, you have to clean up the old images manually.","keywords":"","version":"v1.4 (dev)"},{"title":"Host Management","type":0,"sectionRef":"#","url":"/v1.4/host/","content":"Host Management Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are three or more nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance​ For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node​ Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you’re done, power back on and make the node schedulable again by uncordoning it. Deleting a Node​ caution Before removing a node from a Harvester cluster, determine if the remaining nodes have enough computing and storage resources to take on the workload from the node to be removed. Check the following: Current resource usage in the cluster (on the Hosts screen of the Harvester UI)Ability of the remaining nodes to maintain enough replicas for all volumes If the remaining nodes do not have enough resources, VMs might fail to migrate and volumes might degrade when you remove a node. 1. Check if the node can be removed from the cluster.​ You can safely remove a control plane node depending on the quantity and availability of other nodes in the cluster. The cluster has three control plane nodes and one or more worker nodes. When you remove a control plane node, a worker node will be promoted to control plane node. Harvester v1.3.0 allows you to assign a role to each node that joins a cluster. In earlier Harvester versions, worker nodes were randomly selected for promotion. If you prefer to promote specific nodes, please see Role Management and Harvester Configuration for more information. The cluster has three control plane nodes and no worker nodes. You must add a new node to the cluster before removing a control plane node. This ensures that the cluster always has three control plane nodes and that a quorum can be formed even if one control plane node fails. The cluster has only two control plane nodes and no worker nodes. Removing a control plane node in this situation is not recommended because etcd data is not replicated in a single-node cluster. Failure of a single node can cause etcd to lose its quorum and shut the cluster down. 2. Check the status of volumes.​ Access the embedded Longhorn UI. Go to the Volume screen. Verify that the state of all volumes is Healthy. 3. Evict replicas from the node to be removed.​ Access the embedded Longhorn UI. Go to the Node screen. Select the node that you want to remove, click the icon in the Operation column, and then select Edit node and disks. Configure the following settings: Node Scheduling: Select Disable.Evict Requested&quot; Select True. Click Save. Go back to the Node screen and verify that Replicas value for the node to be removed is 0. important Eviction cannot be completed if the remaining nodes cannot accept replicas from the node to be removed. In this case, some volumes will remain in the Degraded state until you add more nodes to the cluster. 4. Manage non-migratable VMs.​ Live migration cannot be performed for VMs with certain properties. The VM has PCI passthrough devices or vGPU devices. A PCI device is bound to a node. You must remove the PCI device from the VM, or delete the VM and then create a new VM from a backup or snapshot. The VM has a node selector or affinity rules that bind it to the node to be removed. You must change the node selector or affinity rules. The VM is on a VM network that binds it to the node to be removed. You must select a different VM network. tip Create a backup or snapshot for each non-migratable VM before modifying the settings that bind it to the node that you want to remove. 5. Evict workloads from the node to be removed.​ If your cluster is running Harvester v1.1.2 or later, you can enable Maintenance Mode on the node to automatically live-migrate VMs and workloads. You can also manually live-migrate VMs to other nodes. All workloads have been successfully evicted if the node state is Maintenance. important If a cluster has only two control plane nodes, Harvester does not allow you to enable Maintenance Mode on any node. You can manually drain the node to be removed using the following command: kubectl drain &lt;node_name&gt; --force --ignore-daemonsets --delete-local-data --pod-selector='app!=csi-attacher,app!=csi-provisioner' Again, removing a control plane node in this situation is not recommended because etcd data is not replicated. Failure of a single node can cause etcd to lose its quorum and shut the cluster down. 6. Remove the node.​ On the Harvester UI, go to the Hosts screen. Locate the node that you want to remove, and then click ⋮ &gt; Delete. 7. Delete RKE2 services on the node.​ Log in to the node using the root account. Run the script /opt/rke2/bin/rke2-uninstall.sh. note There's a known issue about node hard delete. Once resolved, you can skip this step. Role Management​ Hardware issues may force you to replace the management node. In earlier Harvester versions, accurately promoting a specific worker node to a management node was not easy. Harvester v1.3.0 improves the process by introducing the following roles: Management: Allows a node to be prioritized when Harvester promotes nodes to management nodes.Witness: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster.Worker: Restricts a node to being a worker node (never promoted to management node) in a specific cluster. caution Harvester currently allows only one witness node in the cluster. For more information about assigning roles to nodes, see ISO Installation. Multi-disk Management​ Add Additional Disks​ Users can view and add multiple disks as additional data volumes from the edit host page. Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config. Select the Storage tab and click Add Disk. caution As of Harvester v1.0.2, we no longer support adding partitions as additional disks. If you want to add it as an additional disk, be sure to delete all partitions first (e.g., using fdisk). Select an additional raw block device to add as an additional data volume. The Force Formatted option is required if the block device has never been force-formatted. Last, you can click ⋮ &gt; Edit Config again to check the newly added disk. Meanwhile, you can also add the &quot;Host/Disk&quot; tag (details are described in the next section). note In order for Harvester to identify the disks, each disk needs to have a unique WWN. Otherwise, Harvester will refuse to add the disk. If your disk does not have a WWN, you can format it with the EXT4 filesystem to help Harvester recognize the disk. note If you are testing Harvester in a QEMU environment, you'll need to use QEMU v6.0 or later. Previous versions of QEMU will always generate the same WWN for NVMe disks emulation. This will cause Harvester to not add the additional disks, as explained above. However, you can still add a virtual disk with the SCSI controller. The WWN information could be added manually along with the disk attach operation. For more details, please refer to the script. Storage Tags​ The storage tag feature enables only certain nodes or disks to be used for storing Longhorn volume data. For example, performance-sensitive data can use only the high-performance disks which can be tagged as fast, ssd or nvme, or only the high-performance nodes tagged as baremetal. This feature supports both disks and nodes. Setup​ The tags can be set up through the Harvester UI on the host page: Click Hosts -&gt; Edit Config -&gt; StorageClick Add Host/Disk Tags to start typing and hit enter to add new tags.Click Save to update tags.On the StorageClasses page, create a new storage class and select those defined tags on the Node Selector and Disk Selector fields. All the existing scheduled volumes on the node or disk won’t be affected by the new tags. note When multiple tags are specified for a volume, the disk and the nodes (that the disk belongs to) must have all the specified tags to become usable. Remove disks​ Before removing a disk, you must first evict Longhorn replicas on the disk. note The replica data would be rebuilt to another disk automatically to keep the high availability. Identify the disk to remove (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select the node name and go to the Storage tab.Find the disk you want to remove. Let's assume we want to remove /dev/sdb, and the disk's mount point is /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04. Evict replicas (Longhorn dashboard)​ Please follow this session to enable the embedded Longhorn dashboard.Visit the Longhorn dashboard and go to the Node page.Expand the node containing the disk. Confirm the mount point /var/lib/harvester/extra-disks/1b805b97eb5aa724e6be30cbdb373d04 is in the disks list. Select Edit node and disks. Scroll to the disk you want to remove. Set Scheduling to Disable.Set Eviction Requested to True.Select Save. Do not select the delete icon. The disk will be disabled. Please wait until the disk replica count becomes 0 to proceed with removing the disk. Remove the disk (Harvester dashboard)​ Go to the Hosts page.On the node containing the disk, select ⋮ &gt; Edit Config.Go to the Storage tab and select x to remove the disk. Select Save to remove the disk. Ksmtuned Mode​ Available as of v1.1.0 Ksmtuned is a KSM automation tool deployed as a DaemonSet to run Ksmtuned on each node. It will start or stop the KSM by watching the available memory percentage ratio (i.e. Threshold Coefficient). By default, you need to manually enable Ksmtuned on each node UI. You will be able to see the KSM statistics from the node UI after 1-2 minutes.(check KSM for more details). Quick Run​ Go to the Hosts page.On the node you want to modify, click ⋮ &gt; Edit Config.Select the Ksmtuned tab and select Run in Run Strategy.(Optional) You can modify Threshold Coefficient as needed. Click Save to update.Wait for about 1-2 minutes and you can check its Statistics by clicking Your Node &gt; Ksmtuned tab. Parameters​ Run Strategy: Stop: Stop Ksmtuned and KSM. VMs can still use shared memory pages.Run: Run Ksmtuned.Prune: Stop Ksmtuned and prune KSM memory pages. Threshold Coefficient: configures the available memory percentage ratio. If the available memory is less than the threshold, KSM will be started; otherwise, KSM will be stopped. Merge Across Nodes: specifies if pages from different NUMA nodes can be merged. Mode: Standard: The default mode. The control node ksmd uses about 20% of a single CPU. It uses the following parameters: Boost: 0 Decay: 0 Maximum Pages: 100 Minimum Pages: 100 Sleep Time: 20 High-performance: Node ksmd uses 20% to 100% of a single CPU and has higher scanning and merging efficiency. It uses the following parameters: Boost: 200 Decay: 50 Maximum Pages: 10000 Minimum Pages: 100 Sleep Time: 20 Customized: You can customize the configuration to reach the performance that you want. Ksmtuned uses the following parameters to control KSM efficiency: Parameters\tDescriptionBoost\tThe number of scanned pages is incremented each time if the available memory is less than the Threshold Coefficient. Decay\tThe number of scanned pages is decremented each time if the available memory is greater than the Threshold Coefficient. Maximum Pages\tMaximum number of pages per scan. Minimum Pages\tThe minimum number of pages per scan, also the configuration for the first run. Sleep Time (ms)\tThe interval between two scans, which is calculated with the formula (Sleep Time * 16 * 1024* 1024 / Total Memory). Minimum: 10ms. For example, assume you have a 512GiB memory node that uses the following parameters: Boost: 300 Decay: 100 Maximum Pages: 5000 Minimum Pages: 1000 Sleep Time: 50 When Ksmtuned starts, initialize pages_to_scan in KSM to 1000 (Minimum Pages) and set sleep_millisecs to 10 (50 * 16 * 1024 * 1024 / 536870912 KiB &lt; 10). KSM starts when the available memory falls below the Threshold Coefficient. If it detects that it is running, pages_to_scan increments by 300 (Boost) every minute until it reaches 5000 (Maximum Pages). KSM will stop when the available memory is above the Threshold Coefficient. If it detects that it is stopped, pages_to_scan decrements by 100 (Decay) every minute until it reaches 1000 (Minimum Pages). NTP Configuration​ Time synchronization is an important aspect of distributed cluster architecture. Because of this, Harvester now provides a simpler way for configuring NTP settings. In previous Harvester versions, NTP settings were mainly configurable during the installation process. To modify the settings, you needed to manually update the configuration file on each node. Beginning with version v1.2.0, Harvester is supporting NTP configuration on the Harvester UI Settings screen (Advanced &gt; Settings). You can configure NTP settings for the entire Harvester cluster at any time, and the settings are applied to all nodes in the cluster. You can set up multiple NTP servers at once. You can check the settings in the node.harvesterhci.io/ntp-service annotation in Kubernetes nodes: ntpSyncStatus: Status of the connection to NTP servers (possible values: disabled, synced and unsynced)currentNtpServers: List of existing NTP servers $ kubectl get nodes harvester-node-0 -o yaml |yq -e '.metadata.annotations.[&quot;node.harvesterhci.io/ntp-service&quot;]' {&quot;ntpSyncStatus&quot;:&quot;synced&quot;,&quot;currentNtpServers&quot;:&quot;0.suse.pool.ntp.org 1.suse.pool.ntp.org&quot;} Note: Do not modify the NTP configuration file on each node. Harvester will automatically sync the settings that you configured on the Harvester UI to the nodes.If you upgraded Harvester from an earlier version, the ntp-servers list on the Settings screen will be empty (see screenshot). You must manually configure the NTP settings because Harvester is unaware of the previous settings and is unable to detect conflicts. Cloud-Native Node Configuration​ You may need to customize one or more nodes after installing Harvester. This process usually entails updating the runtime configuration and modifying files in the /oem directory of each node to make changes persist after rebooting. In Harvester v1.3.0, these customizations can be described in a Kubernetes manifest and then applied to the underlying cluster using kubectl or other GitOps-centric tools such as Fleet. danger Misconfigurations might compromise the ability of a Harvester node to boot up, or even damage the overall stability of the cluster. You can prevent such issues by reading the Elemental toolkit documentation to learn how to correctly customize Elemental. Creating a CloudInit Resource​ Harvester node customization is bounded only by your creativity and by what the Elemental toolkit markup can syntactically express. The documentation, therefore, cannot provide an exhaustive list of possible customizations and use cases. Example: You want to add an SSH authorized key for the default rancher user on all nodes. Start by creating a Kubernetes manifest for a CloudInit resource. file: ssh_access.yaml apiVersion: node.harvesterhci.io/v1beta1 kind: CloudInit metadata: name: ssh-access spec: matchSelector: {} filename: 99_ssh.yaml contents: | stages: network: - authorized_keys: rancher: - ssh-ed25519 AAAA... This manifest describes an Elemental cloud-init document that will be applied to all nodes (because the empty matchSelector: {} field matches everything). The YAML document in the .spec.contents field will be rendered to /oem/99_ssh.yaml (because of the .spec.filename field.) Apply this example using the command kubectl apply -f ssh_access.yaml. tip Reboot the relevant Harvester nodes so that the Elemental toolkit executor can apply the new configuration at boot. CloudInit Resource Spec​ Field\tRequired\tDescriptionmatchSelector\tYes\tSetting that allows you to specify the nodes that will receive the configuration changes. filename\tYes\tName of the file that appears in /oem. contents\tYes\tElemental toolkit cloud-init-style file that will be rendered to a file in /oem. paused\tNo\tWhen set to true, the file will not be updated on nodes as it changes. The matchSelector field can be used to target specific nodes or groups of nodes based on their labels. Example: matchSelector: kubernetes.io/hostname: &quot;harvester-node-1&quot; note All label key-value pairs listed in the matchSelector field must match the labels of the intended nodes. In the following example, matchSelector will match harvester-node-1 only if that node also has the example.com/role label with the value role-a. matchSelector: kubernetes.io/hostname: &quot;harvester-node-1&quot; example.com/role: &quot;role-a&quot; Updating a CloudInit Resource​ You can use the command kubectl edit to update a CloudInit resource. However, there is a caveat if the matchSelector field is updated to exclude one or more nodes from the customization. See the note in the Deleting a CloudInit Resource section regarding rolling back customizations. # kubectl edit cloudinit CLOUDINIT_NAME Deleting a CloudInit Resource​ You can use the command kubectl delete to remove a CloudInit resource from the Harvester cluster. # kubectl delete cloudinit CLOUDINIT_NAME note Harvester is unable to &quot;roll back&quot; previously described customizations because the CloudInit resource can describe anything that can be expressed as an Elemental toolkit customization, including arbitrary shell commands. In the Creating a CloudInit Resource example, the YAML file contains the authorized_keys stanza. This is an append-only action in the Elemental toolkit. When the resource is changed or deleted, the authorized_keys file in Rancher will still contain the old public key. You are responsible for amending or creating a CloudInit resource that rolls the changes back (if necessary) before you reboot the node. Troubleshooting CloudInit Rollouts​ If an Elemental toolkit cloud-init document does not appear in /oem or does not contain the expected contents, the status block of the CloudInit resource might contain useful hints. # kubectl get cloudinit CLOUDINIT_NAME -o yaml status: rollouts: harvester-dngmf: conditions: - lastTransitionTime: &quot;2024-02-28T22:31:23Z&quot; message: &quot;&quot; reason: CloudInitApplicable status: &quot;True&quot; type: Applicable - lastTransitionTime: &quot;2024-02-28T22:31:23Z&quot; message: Local file checksum is the same as the CloudInit checksum reason: CloudInitChecksumMatch status: &quot;False&quot; type: OutOfSync - lastTransitionTime: &quot;2024-02-28T22:31:23Z&quot; message: 99_ssh.yaml is present under /oem reason: CloudInitPresentOnDisk status: &quot;True&quot; type: Present The harvester-node-manager pod(s) in the harvester-system namespace may also contain some hints as to why it is not rendering a file to a node. This pod is part of a daemonset, so it may be worth checking the pod that is running on the node of interest.","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Instance","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-instance","content":"Read a Virtual Machine Instance GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachineinstances/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineInstance object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object activePods object property name* string conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] evacuationNodeName string fsFreezeStatus string guestOSInfo object id string kernelRelease string kernelVersion string machine string name string prettyName string version string versionId string interfaces object[] Array [ infoSource string interfaceName string ipAddress string ipAddresses string[] mac string name string ] launcherContainerImageVersion string migrationMethod string migrationState object abortRequested boolean abortStatus string completed boolean endTimestamp string failed boolean migrationConfiguration object allowAutoConverge boolean allowPostCopy boolean bandwidthPerMigration string completionTimeoutPerGiB int64 disableTLS boolean network string nodeDrainTaintKey string parallelMigrationsPerCluster int64 parallelOutboundMigrationsPerNode int64 progressTimeout int64 unsafeMigrationOverride boolean migrationPolicyName string migrationUid string mode string sourceNode string startTimestamp string targetAttachmentPodUID string targetCPUSet int32[] targetDirectMigrationNodePorts object property name* int32 targetNode string targetNodeAddress string targetNodeDomainDetected boolean targetNodeTopology string targetPod string migrationTransport string nodeName string phase string phaseTransitionTimestamps object[] Array [ phase string phaseTransitionTimestamp string ] qosClass string reason string runtimeUser int64 topologyHints object tscFrequency int64 virtualMachineRevisionName string volumeStatus object[] Array [ hotplugVolume object attachPodName string attachPodUID string memoryDumpVolume object claimName string endTimestamp string startTimestamp string targetFileName string message string name stringrequired persistentVolumeClaimInfo object accessModes string[] capacity object property name* string Default value: [object Object] filesystemOverhead string preallocated boolean requests object property name* string Default value: [object Object] volumeMode string phase string reason string size int64 target stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Harvester Configuration","type":0,"sectionRef":"#","url":"/v1.4/install/harvester-configuration","content":"Harvester Configuration Configuration Example​ Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: scheme_version: 1 server_url: https://cluster-VIP:443 token: TOKEN_VALUE os: ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files: - encoding: &quot;&quot; content: test content owner: root path: /etc/test.txt permissions: '0755' hostname: myhost modules: - kvm - nvme sysctls: kernel.printk: &quot;4 4 1 7&quot; kernel.kptr_restrict: &quot;1&quot; dns_nameservers: - 8.8.8.8 - 1.1.1.1 ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org password: rancher environment: http_proxy: http://myserver https_proxy: http://myserver labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7C&quot; method: dhcp force_efi: true device: /dev/sda data_disk: /dev/sdb silent: true iso_url: http://myserver/test.iso poweroff: true no_format: true debug: true tty: ttyS0 vip: 10.10.0.19 vip_hw_addr: 52:54:00:ec:0e:0b vip_mode: dhcp force_mbr: false addons: harvester_vm_import_controller: enabled: false values_content: &quot;&quot; harvester_pcidevices_controller: enabled: false values_content: &quot;&quot; rancher_monitoring: enabled: true values_content: &quot;&quot; rancher_logging: enabled: false values_content: &quot;&quot; harvester_seeder: enabled: false values_content: &quot;&quot; system_settings: auto-disk-provision-paths: &quot;&quot; Configuration Reference​ Below is a reference of all configuration keys. caution Security Risks: The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. note Configuration Priority: When you provide a remote Harvester Configuration file during the install of Harvester, the Harvester Configuration file will not overwrite the values for the inputs you had previously filled out and selected. Priority is given to the values that you input during the guided install. For instance, if you have in your Harvester Configuration file specified os.hostname and during install you fill in the field of hostname when prompted, the value that you filled in will take priority over your Harvester Configuration's os.hostname. scheme_version​ Definition​ The version of scheme reserved for future configuration migration. This configuration is mandatory for migrating the configuration to a new scheme version. It tells Harvester the previous version and the need to migrate. note This field didn't take any effect in the current Harvester version. caution Make sure that your custom configuration always has the correct scheme version. server_url​ Definition​ server_url is the URL of the Harvester cluster, which is used for the new node to join the cluster. This configuration is mandatory when the installation is in JOIN mode. The default format of server_url is https://cluster-VIP:443. note To ensure a high availability (HA) Harvester cluster, please use either the Harvester cluster VIP or a domain name in server_url. Example​ server_url: https://cluster-VIP:443 install: mode: join token​ Definition​ The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example​ token: myclustersecret Or a node token token: &quot;K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4&quot; os.ssh_authorized_keys​ Definition​ A list of SSH authorized keys that should be added to the default user, rancher. SSH keys can be obtained from GitHub user accounts by using the formatgithub:${USERNAME}. This is done by downloading the keys from https://github.com/${USERNAME}.keys. Example​ os: ssh_authorized_keys: - &quot;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D&quot; - &quot;github:ibuildthecloud&quot; os.write_files​ A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: &quot;&quot;: content data are written in plain text. In this case, the encoding field can be also omitted.b64, base64: content data are base64-encoded.gz, gzip: content data are gzip-compressed.gz+base64, gzip+base64, gz+b64, gzip+b64: content data are gzip-compressed first and then base64-encoded. Example os: write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner: root:root path: /etc/connman/main.conf permissions: '0644' - content: | # My new /etc/sysconfig/samba file SMDBOPTIONS=&quot;-D&quot; path: /etc/sysconfig/samba - content: !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path: /bin/arch permissions: '0555' - content: | 15 * * * * root ship_logs path: /etc/crontab os.persistent_state_paths​ Definition​ The os.persistent_state_paths option allows you to configure custom paths where modifications made to files will persist across reboots. Any changes to files in these paths will not be lost after a reboot. Example​ Refer to the following example config for installing rook-ceph in Harvester: os: persistent_state_paths: - /var/lib/rook - /var/lib/ceph modules: - rbd - nbd os.after_install_chroot_commands​ Definition​ You can add additional software packages with after_install_chroot_commands. The after-install-chroot stage, provided by elemental-toolkit, allows you to execute commands not restricted by file system write issues, ensuring the persistence of user-defined commands even after a system reboot. Example​ Refer to the following example config for installing an RPM package in Harvester: os: after_install_chroot_commands: - rpm -ivh &lt;the url of rpm package&gt; DNS resolution is unavailable in the after-install-chroot stage, and the nameserver might not be available. If you need to access a domain name to install a package using an URL, create a temporary /etc/resolv.conf file first. For example: os: after_install_chroot_commands: - &quot;rm -f /etc/resolv.conf &amp;&amp; echo 'nameserver 8.8.8.8' | sudo tee /etc/resolv.conf&quot; - &quot;mkdir /usr/local/bin&quot; - &quot;curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 &amp;&amp; chmod 700 get_helm.sh &amp;&amp; ./get_helm.sh&quot; - &quot;rm -f /etc/resolv.conf &amp;&amp; ln -s /var/run/netconfig/resolv.conf /etc/resolv.conf&quot; note Upgrading Harvester causes the changes to the OS in the after-install-chroot stage to be lost. You must also configure the after-upgrade-chroot to make your changes persistent across an upgrade. Refer to Runtime persistent changes before upgrading Harvester. os.hostname​ Definition​ Set the system hostname. The installer will generate a random hostname if the user doesn't provide a value. Example​ os: hostname: myhostname os.modules​ Definition​ A list of kernel modules to be loaded on start. Example​ os: modules: - kvm - nvme os.sysctls​ Definition​ Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf. Values must be specified as strings. Example​ os: sysctls: kernel.printk: 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict: &quot;1&quot; # force the YAML parser to read as a string os.dns_nameservers​ Definition​ Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example​ os: dns_nameservers: - 8.8.8.8 - 1.1.1.1 os.ntp_servers​ Definition​ Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Highly recommend to configure os.ntp_servers to avoid time synchronization issue between machines. Example​ os: ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org os.password​ Definition​ The password for the default user, rancher. By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from/etc/shadow. You can also encrypt a password using OpenSSL. For the encryption algorithms supported by Harvester, please refer to the table below. Algorithm\tCommand\tSupportSHA-512\topenssl passwd -6\tYes SHA-256\topenssl passwd -5\tYes MD5\topenssl passwd -1\tYes MD5, Apache variant\topenssl passwd -apr1\tYes AIX-MD5\topenssl passwd -aixmd5\tNo Example​ Encrypted: os: password: &quot;$6$kZYUnRaTxNdg4W8H$WSEJydGWsNpaRbbbRdTDLJ2hDLbkizxSFGW2RtexlqG6njEATaGQG9ssztjaKDCsaNUPBZ1E1YdsvSLMAi/IO/&quot; Or clear text: os: password: supersecure os.environment​ Definition​ Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example​ os: environment: http_proxy: http://myserver https_proxy: http://myserver note This example sets the HTTP(S) proxy for foundational OS components. To set up an HTTP(S) proxy for Harvester components such as fetching external images and backup to S3 services, see Settings/http-proxy. os.labels​ Definition​ Labels to be added to this Node. Example​ os: labels: topology.kubernetes.io/zone: zone1 foo: bar mylabel: myvalue os.sshd.sftp​ Definition​ Subsystem used to configure the OpenSSH Daemon (sshd). Harvester currently only supports sftp. Example​ os: sshd: sftp: true # The SFTP subsystem is enabled. install.mode​ Definition​ Harvester installation mode: create: Creating a new Harvester installation.join: Join an existing Harvester installation. Need to specify server_url. Example​ install: mode: create install.role​ Definition​ Role assigned to a node at the time of installation. When unspecified, Harvester assigns the default role. default: Allows a node to function as a management node or a worker node.management: Allows a node to be prioritized when Harvester promotes nodes to management nodes.worker: Restricts a node to being a worker node (never promoted to management node) in a specific cluster.witness: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster. install.management_interface​ Definition​ Configure network interfaces for the host machine. Valid configuration fields are: method: Method to assign an IP to this network. The following are supported: static: Manually assign an IP and gateway.dhcp: Request an IP from the DHCP server. ip: Static IP for this network. Required if static method is chosen.subnet_mask: Subnet mask for this network. Required if static method is chosen.gateway: Gateway for this network. Required if static method is chosen.interfaces: An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name: The name of the slave interface for the bonded network.interfaces.hwAddr: The hardware MAC address of the interface. bond_options: Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlbmiimon: 100 mtu: The MTU for the interface.vlan_id: The VLAN ID for the interface. note Harvester uses the systemd net naming scheme. Please make sure the interface name is present on the target machine before installation. Example​ install: mode: create management_interface: interfaces: - name: ens5 hwAddr: &quot;B8:CA:3A:6A:64:7D&quot; # The hwAddr is optional method: dhcp bond_options: mode: balance-tlb miimon: 100 mtu: 1492 vlan_id: 101 install.force_efi​ Force EFI installation even when EFI is not detected. Default: false. install.device​ The device to install the OS. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. install.silent​ Reserved. install.iso_url​ ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff​ Shutdown the machine after installation instead of rebooting install.no_format​ Do not partition and format, assume layout exists already. install.debug​ Run the installation with additional logging and debugging enabled for the installed system. install.persistent_partition_size​ Definition​ Configure the size of partition COS_PERSISTENT in Gi or Mi. This partition is used to store data like system packages and container images. The default and minimum value is 150Gi. Example​ install: persistent_partition_size: 150Gi install.skipchecks​ Definition​ Allow installation to proceed even if minimum requirements for production use are not met. Default: false. The installer automatically checks if the hardware meets the minimum requirements for production use. When performing automated installation via PXE Boot, if any of the checks fail, installation is stopped, and warnings are printed to the system console and saved to /var/log/console.log in the installation environment. To override this behavior, set install.skipchecks=true. When set to true, warning messages are still saved to /var/log/console.log, but the installation proceeds even if hardware requirements for production use are not met. Example​ install: skipchecks: true install.tty​ Definition​ The tty device used for the console. Example​ install: tty: ttyS0,115200n8 install.vip​ install.vip_mode​ install.vip_hw_addr​ Definition​ install.vip: The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://&lt;VIP&gt;.install.vip_mode dhcp: Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided.static: Harvester uses a static VIP. install.vip_hw_addr: The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp. See Management Address for more information. Example​ Configure a static VIP. install: vip: 192.168.0.100 vip_mode: static Configure a DHCP VIP. install: vip: 10.10.0.19 vip_mode: dhcp vip_hw_addr: 52:54:00:ec:0e:0b install.force_mbr​ Definition​ By default, Harvester uses GPT partitioning scheme on both UEFI and BIOS systems. However, if you face compatibility issues, the MBR partitioning scheme can be forced on BIOS systems. note Harvester creates an additional partition for storing VM data ifinstall.data_disk is configured to use the same storage device as the one set for install.device. When force using MBR, no additional partition will be created and VM data will be stored in a partition shared with the OS data. Example​ install: force_mbr: true install.data_disk​ Available as of v1.0.1 Definition​ Sets the default storage device to store the VM data. Prefer to use /dev/disk/by-id/$id or /dev/disk/by-path/$path to specify the storage device if your machine contains multiple physical volumes via pxe installation. Default: Same storage device as the one set for install.device Example​ install: data_disk: /dev/sdb install.addons​ Available as of v1.2.0 Definition​ Sets the default enabled/disabled status of Harvester addons. Default: The addons are disabled. Example​ install: addons: rancher_monitoring: enabled: true rancher_logging: enabled: false Harvester v1.2.0 ships with five addons: vm-import-controller (chartName: harvester-vm-import-controller)pcidevices-controller (chartName: harvester-pcidevices-controller)rancher-monitoringrancher-loggingharvester-seeder (experimental) install.harvester.storage_class.replica_count​ Available as of v1.1.2 Definition​ Sets the replica count of Harvester's default storage class harvester-longhorn. Default: 3 Supported values: 1, 2, 3. All other values are considered 3. In edge scenarios where users may deploy single-node Harvester clusters, they can set this value to 1. In most scenarios, it is recommended to keep the default value 3 for storage high availability. Please refer to longhorn-replica-count for more details. Example​ install: harvester: storage_class: replica_count: 1 install.harvester.longhorn.default_settings.guaranteedEngineManagerCPU​ Available as of v1.2.0 Definition​ Sets the default percentage of the total allocatable CPU on each node will be reserved for each Longhorn engine manager Pod. Default: 12 Supported values: 0-12. All other values are considered 12. This integer value indicates what percentage of the total allocatable CPU on each node will be reserved for each engine manager Pod. In edge scenarios where users may deploy single-node Harvester clusters, they can set this parameter to a value smaller than 12. In most scenarios, it is recommended to keep the default value for system high availability. Before setting the value, please refer to longhorn-guaranteed-engine-manager-cpu for more details. Example​ install: harvester: longhorn: default_settings: guaranteedEngineManagerCPU: 6 install.harvester.longhorn.default_settings.guaranteedReplicaManagerCPU​ Available as of v1.2.0 Definition​ Sets the default percentage of the total allocatable CPU on each node will be reserved for each Longhorn replica manager Pod. Default: 12 Supported values: 0-12. All other values are considered 12. This integer value indicates what percentage of the total allocatable CPU on each node will be reserved for each replica manager Pod. In edge scenarios where users may deploy single-node Harvester clusters, can set this parameter to a value smaller than 12. In most scenarios, it is recommended to keep the default value for system high availability. Before setting the value, please refer to longhorn-guaranteed-replica-manager-cpu for more details. Example​ install: harvester: longhorn: default_settings: guaranteedReplicaManagerCPU: 6 system_settings​ Definition​ You can overwrite the default Harvester system settings by configuring system_settings. See the Settings page for additional information and the list of all the options. note Overwriting system settings only works when Harvester is installed in &quot;create&quot; mode. If you install Harvester in &quot;join&quot; mode, this setting is ignored. Installing in &quot;join&quot; mode will adopt the system settings from the existing Harvester system. Example​ The example below overwrites containerd-registry, http-proxy and ui-source settings. The values must be a string. system_settings: containerd-registry: '{&quot;Mirrors&quot;: {&quot;docker.io&quot;: {&quot;Endpoints&quot;: [&quot;https://myregistry.local:5000&quot;]}}, &quot;Configs&quot;: {&quot;myregistry.local:5000&quot;: {&quot;Auth&quot;: {&quot;Username&quot;: &quot;testuser&quot;, &quot;Password&quot;: &quot;testpassword&quot;}, &quot;TLS&quot;: {&quot;InsecureSkipVerify&quot;: false}}}}' http-proxy: '{&quot;httpProxy&quot;: &quot;http://my.proxy&quot;, &quot;httpsProxy&quot;: &quot;https://my.proxy&quot;, &quot;noProxy&quot;: &quot;some.internal.svc&quot;}' ui-source: auto ","keywords":"Harvester harvester Rancher rancher Harvester Configuration","version":"v1.4 (dev)"},{"title":"ISO Installation","type":0,"sectionRef":"#","url":"/v1.4/install/index","content":"ISO Installation Harvester ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image. To get the ISO image, download 💿 harvester-v1.x.x-amd64.iso from the Harvester releases page. During the installation, you can either choose to create a new Harvester cluster or join the node to an existing Harvester cluster. The following video shows a quick overview of an ISO installation. Installation Steps​ Mount the Harvester ISO file and boot the server by selecting the Harvester Installer option. The installer automatically checks the hardware and displays warning messages if the minimum requirements are not met. The Hardware Checks screen is not displayed if all checks are passed. Use the arrow keys to choose an installation mode. By default, the first node will be the management node of the cluster. Create a new Harvester cluster: creates an entirely new Harvester cluster. Join an existing Harvester cluster: joins an existing Harvester cluster. You need the VIP and cluster token of the cluster you want to join. Install Harvester binaries only: If you choose this option, additional setup is required after the first bootup. info When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config by providing a URL of Harvester configuration on the customize the host step. In this case, at least three different zones are required. Choose a role for the node. You are required to perform this step if you selected the installation mode Join an existing Harvester cluster. Default Role: Allows a node to function as a management node or a worker node. This role does not have any specific privileges or restrictions.Management Role: Allows a node to be prioritized when Harvester promotes nodes to management nodes.Witness Role: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster.Worker Role: Restricts a node to being a worker node (never promoted to management node) in a specific cluster. Choose the installation disk you want to install the Harvester cluster on and the data disk you want to store VM data on. By default, Harvester uses GUID Partition Table (GPT) partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select Master boot record (MBR). Installation disk: The disk to install the Harvester cluster on. Data disk: The disk to store VM data on. Choosing a separate disk to store VM data is recommended.Persistent size: If you only have one disk or use the same disk for both OS and VM data, you need to configure persistent partition size to store system packages and container images. The default and minimum persistent partition size is 150 GiB. You can specify a size like 200Gi or 153600Mi. Configure the HostName of the node. Configure network interface(s) for the management network. By default, Harvester creates a bonded NIC named mgmt-bo, and the IP address can be configured via DHCP or statically assigned. note It is not possible to change the node IP throughout the lifecycle of a Harvester cluster. If using DHCP, you must ensure the DHCP server always offers the same IP for the same node. If the node IP is changed, the related node cannot join the cluster and might even break the cluster. In addition, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. For example: Linux~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see DHCP Server Configuration. (Optional) Configure the DNS Servers. Use commas as a delimiter to add more DNS servers. Leave it blank to use the default DNS server. Configure the virtual IP (VIP) by selecting a VIP Mode. This VIP is used to access the cluster or for other nodes to join the cluster. note If using DHCP to configure the IP address, you need to configure a static MAC-to-IP address mapping on your DHCP server to have a persistent virtual IP (VIP), and the VIP must be unique. Configure the Cluster token. This token is used for adding other nodes to the cluster. Configure and confirm a Password to access the node. The default SSH user is rancher. Configure NTP servers to make sure all nodes' times are synchronized. This defaults to 0.suse.pool.ntp.org. Use commas as a delimiter to add more NTP servers. (Optional) If you need to use an HTTP proxy to access the outside world, enter the Proxy address. Otherwise, leave this blank. (Optional) You can choose to import SSH keys by providing HTTP URL. For example, your GitHub public keys https://github.com/&lt;username&gt;.keys can be used. (Optional) If you need to customize the host with a Harvester configuration file, enter the HTTP URL here. Review and confirm your installation options. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, your node restarts. After the restart, the Harvester console displays the management URL and status. The default URL of the web interface is https://your-virtual-ip. You can use F12 to switch from the Harvester console to the Shell and type exit to go back to the Harvester console. note Choosing Install Harvester binaries only on the first page requires additional setup after the first bootup. You will be prompted to set the password for the default admin user when logging in for the first time. Known Issue​ Installer may crash when using an older graphics card/monitor​ In some cases, if you are using an older graphics card/monitor, you may encounter a panic: invalid dimensions error during ISO installation. We are working on this known issue and planning a fix for a future release. You can try to use another GRUB entry to force it to use the resolution of 1024x768 when booting up. If you are using a version earlier than v1.1.1, please try the following workaround: Boot up with the ISO, and press E to edit the first menu entry: Append vga=792 to the line started with $linux: Press Ctrl+X or F10 to boot up. Fail to join nodes using FQDN to a cluster which has custom SSL certificate configured​ You may encounter that newly joined nodes stay in the Not Ready state indefinitely. This is likely the outcome if you already have a set of custom SSL certificates configured on the to-be-joined Harvester cluster and provide an FQDN instead of a VIP address for the management address during the Harvester installation. You can check the SSL certificates on the Harvester dashboard's setting page or using the command line tool kubectl get settings.harvesterhci.io ssl-certificates to see if there is any custom SSL certificate configured (by default, it is empty). The second thing to look at is the joining nodes. Try to get access to the nodes via consoles or SSH sessions and then check the log of rancherd: $ journalctl -u rancherd.service Oct 06 03:36:06 node-0 systemd[1]: Starting Rancher Bootstrap... Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/50-defaults.yaml]&quot; Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Loading config file [/usr/share/rancher/rancherd/config.yaml.d/91-harvester-bootstrap-repo.yaml]&quot; Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Loading config file [/etc/rancher/rancherd/config.yaml]&quot; Oct 06 03:36:06 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:06Z&quot; level=info msg=&quot;Bootstrapping Rancher (v2.7.5/v1.25.9+rke2r1)&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;Writing plan file to /var/lib/rancher/rancherd/plan/plan.json&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;Applying plan with checksum &quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231006-033608-applied.plan/_0&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;Running command: /usr/bin/env [sh /var/lib/rancher/rancherd/install.sh]&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent configuration directory /etc/rancher/agent&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Using default agent var directory /var/lib/rancher/agent&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stderr]: [WARN] /usr/local is read-only or a mount point; installing to /opt/rancher-system-agent&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Determined CA is necessary to connect to Rancher&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded CA certificate&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Value from https://harvester.192.168.48.240.sslip.io:443/cacerts is an x509 certificate&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully tested Rancher connection&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Downloading rancher-system-agent binary from https://harvester.192.168.48.240.sslip.io:443/assets/rancher-system-agent-amd64&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded the rancher-system-agent binary.&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Downloading rancher-system-agent-uninstall.sh script from https://harvester.192.168.48.240.sslip.io:443/assets/system-agent-uninstall.sh&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded the rancher-system-agent-uninstall.sh script.&quot; Oct 06 03:36:08 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:08Z&quot; level=info msg=&quot;[stdout]: [INFO] Generating Cattle ID&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Successfully downloaded Rancher connection information&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] systemd: Creating service file&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Creating environment file /etc/systemd/system/rancher-system-agent.env&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Enabling rancher-system-agent.service&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stderr]: Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service.&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stdout]: [INFO] Starting/restarting rancher-system-agent.service&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;No image provided, creating empty working directory /var/lib/rancher/rancherd/plan/work/20231006-033608-applied.plan/_1&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;Running command: /usr/bin/rancherd [probe]&quot; Oct 06 03:36:09 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:09Z&quot; level=info msg=&quot;[stderr]: time=\\&quot;2023-10-06T03:36:09Z\\&quot; level=info msg=\\&quot;Running probes defined in /var/lib/rancher/rancherd/plan/plan.json\\&quot;&quot; Oct 06 03:36:10 node-0 rancherd[2171]: time=&quot;2023-10-06T03:36:10Z&quot; level=info msg=&quot;[stderr]: time=\\&quot;2023-10-06T03:36:10Z\\&quot; level=info msg=\\&quot;Probe [kubelet] is unhealthy\\&quot;&quot; The above log shows that rancherd is waiting for kubelet to become healthy. rancherd is doing nothing wrong and is working as expected. The next step is to check the rancher-system-agent: $ journalctl -u rancher-system-agent.service Oct 06 03:43:51 node-0 systemd[1]: rancher-system-agent.service: Scheduled restart job, restart counter is at 88. Oct 06 03:43:51 node-0 systemd[1]: Stopped Rancher System Agent. Oct 06 03:43:51 node-0 systemd[1]: Started Rancher System Agent. Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Rancher System Agent version v0.3.3 (9e827a5) is starting&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Using directory /var/lib/rancher/agent/work for work&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Starting remote watch of plans&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=info msg=&quot;Initial connection to Kubernetes cluster failed with error Get \\&quot;https://harvester.192.168.48.240.sslip.io/version\\&quot;: x509: certificate signed by unknown authority, removing CA data and trying again&quot; Oct 06 03:43:51 node-0 rancher-system-agent[4164]: time=&quot;2023-10-06T03:43:51Z&quot; level=fatal msg=&quot;error while connecting to Kubernetes cluster with nullified CA data: Get \\&quot;https://harvester.192.168.48.240.sslip.io/version\\&quot;: x509: certificate signed by unknown authority&quot; Oct 06 03:43:51 node-0 systemd[1]: rancher-system-agent.service: Main process exited, code=exited, status=1/FAILURE Oct 06 03:43:51 node-0 systemd[1]: rancher-system-agent.service: Failed with result 'exit-code'. If you see a similar log output, you need to manually add the CA to the trust list on each joining node with the following commands: # prepare the CA as embedded-rancher-ca.pem on the nodes $ sudo cp embedded-rancher-ca.pem /etc/pki/trust/anchors/ $ sudo update-ca-certificates After adding the CA to the trust list, the nodes can join to the cluster successfully.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.4 (dev)"},{"title":"Install Harvester Binaries Only","type":0,"sectionRef":"#","url":"/v1.4/install/install-binaries-mode","content":"Install Harvester Binaries Only Available as of v1.2.0 The Install Harvester binaries only mode allows you to install and configure binaries only, making it ideal for cloud and edge use cases. Background​ Currently when a new Harvester node is launched it needs to be the first node in the cluster or join an existing cluster. These two modes are useful when you already know enough about the environment to install the Harvester node. However, for use cases such as bare-metal cloud providers and the edge, these installation modes load the OS and Harvester content to the node without letting you configure the network. Moreover, the K8s and networking configuration will not be applied. If you choose Install Harvester binaries only, you will need to perform additional configuration after the first bootup: Create/Join option for HarvesterManagement network interface detailsCluster tokenNode password Then, the installer will apply the endpoint configuration and boot Harvester. No further reboots will be required. Stream disk mode​ Harvester has published a raw image artifact for pre-installed Harvester. The Harvester installer now allows streaming a pre-installed image directly to disk to support better integration with cloud providers. On Equinix Metal, you can use the following kernel arguments to use the streaming mode: ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl root=live:http://${artifactEndpoint}/harvester-v1.2.0-rootfs-amd64.squashfs harvester.install.automatic=true harvester.scheme_version=1 harvester.install.device=/dev/vda harvester.os.password=password harvester.install.raw_disk_image_path=http://${artifactEndpoint}/harvester-v1.2.0-amd64.raw harvester.install.mode=install console=tty1 harvester.install.tty=tty1 harvester.install.config_url=https://metadata.platformequinix.com/userdata harvester.install.management_interface.interfaces=&quot;name:enp1s0&quot; harvester.install.management_interface.method=dhcp harvester.install.management_interface.bond_options.mode=balance-tlb harvester.install.management_interface.bond_options.miimon=100 note When streaming to disk, it is recommended to host the raw disk artifact closer to the targets, as the raw disk artifact is nearly 16G in size.","keywords":"Harvester harvester Rancher rancher ISO Installation","version":"v1.4 (dev)"},{"title":"Management Address","type":0,"sectionRef":"#","url":"/v1.4/install/management-address","content":"Management Address Harvester provides a fixed virtual IP (VIP) as the management address, VIP must be different from any Node IP. You can find the management address on the console dashboard after the installation. note If you selected the IP address to be configured via DHCP, you will need to configure static MAC-to-IP address mapping on your DHCP server in order to have a persistent Virtual IP How to get the VIP MAC address​ To get the VIP MAC address, you can run the following command on the management node: $ kubectl get svc -n kube-system ingress-expose -ojsonpath='{.metadata.annotations}' Example of output: {&quot;kube-vip.io/hwaddr&quot;:&quot;02:00:00:09:7f:3f&quot;,&quot;kube-vip.io/requestedIP&quot;:&quot;10.84.102.31&quot;} Usages​ The management address: Allows the access to the Harvester API/UI via HTTPS protocol.Allows other nodes to join the cluster.","keywords":"VIP","version":"v1.4 (dev)"},{"title":"Net Install ISO","type":0,"sectionRef":"#","url":"/v1.4/install/net-install","content":"Net Install ISO The Harvester net install ISO is a minimal installation image that contains only the core OS components, allowing the installer to boot and then install the Harvester OS on a disk. After installation is completed, the Harvester OS pulls all required container images from the internet (mostly from Docker Hub). You can use the net install ISO in the following situations: The virtual media implementation on a server is buggy or slow. Community users have reported that ISO redirection is too slow to preload all images onto a system. For more information, see Issue 2651.You have a private registry that contains all Harvester images, as well as the knowledge and experience required to configure image mirrors for containerd. caution You must always use the full ISO to bootstrap a Harvester cluster (in other words, use the ISO without the -net-install suffix). The full ISO contains all required images, and the installer preloads those images during installation. You can easily reach the Docker Hub rate limit when using a net install ISO to bootstrap the Harvester cluster. Usage​ Download the net install ISO from the GitHub Releases page, and then boot the ISO to install Harvester. Net install ISO file names have the suffix net-install (for example, https://releases.rancher.com/harvester/v1.3.0/harvester-v1.3.0-amd64-net-install.iso). PXE Installation​ If you decide to use the net install ISO as the PXE installation source, add the following parameter when booting the kernel: harvester.install.with_net_images=true Please check PXE Boot Installation for more information.","keywords":"Harvester Net ISO Installation BMC ISO Redirection BMC Virtual Media","version":"v1.4 (dev)"},{"title":"Hardware and Network Requirements","type":0,"sectionRef":"#","url":"/v1.4/install/requirements","content":"Hardware and Network Requirements As an HCI solution on bare metal servers, there are minimum node hardware and network requirements for installing and running Harvester. A three-node cluster is required to fully realize the multi-node features of Harvester. The first node that is added to the cluster is by default the management node. When the cluster has three or more nodes, the two nodes added after the first are automatically promoted to management nodes to form a high availability (HA) cluster. Certain versions of Harvester support the deployment of single-node clusters. Such clusters do not support high availability, multiple replicas, and live migration. Hardware Requirements​ Harvester nodes have the following hardware requirements and recommendations for installation and testing. Type\tRequirements and RecommendationsCPU\tx86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum for testing; 16-core or above required for production Memory\t32 GB minimum for testing; 64 GB or above required for production Disk Capacity\t250 GB minimum for testing (180 GB minimum when using multiple disks); 500 GB or above required for production Disk Performance\t5,000+ random IOPS per disk(SSD/NVMe). Management nodes (first 3 nodes) must be fast enough for etcd Network Card\t1 Gbps Ethernet minimum for testing; 10Gbps Ethernet required for production Network Switch\tTrunking of ports required for VLAN support important Use server-class hardware to achieve the best results. Laptops and nested virtualization are not supported.Each node must have a unique product_uuid (fetched from /sys/class/dmi/id/product_uuid) to prevent errors from occurring during VM live migration and other operations. For more information, see Issue #4025. Network Requirements​ Harvester nodes have the following network requirements for installation. Port Requirements for Harvester Nodes​ Harvester nodes require the following port connections or inbound rules. Typically, all outbound traffic is allowed. Protocol\tPort\tSource\tDescriptionTCP\t2379\tHarvester management nodes\tEtcd client port TCP\t2381\tHarvester management nodes\tEtcd health checks TCP\t2380\tHarvester management nodes\tEtcd peer port TCP\t10010\tHarvester management and compute nodes\tContainerd TCP\t6443\tHarvester management nodes\tKubernetes API TCP\t9345\tHarvester management nodes\tKubernetes API TCP\t10252\tHarvester management nodes\tKube-controller-manager health checks TCP\t10257\tHarvester management nodes\tKube-controller-manager secure port TCP\t10251\tHarvester management nodes\tKube-scheduler health checks TCP\t10259\tHarvester management nodes\tKube-scheduler secure port TCP\t10250\tHarvester management and compute nodes\tKubelet TCP\t10256\tHarvester management and compute nodes\tKube-proxy health checks TCP\t10258\tHarvester management nodes\tCloud-controller-manager TCP\t9091\tHarvester management and compute nodes\tCanal calico-node felix TCP\t9099\tHarvester management and compute nodes\tCanal CNI health checks UDP\t8472\tHarvester management and compute nodes\tCanal CNI with VxLAN TCP\t2112\tHarvester management nodes\tKube-vip TCP\t6444\tHarvester management and compute nodes\tRKE2 agent TCP\t10246/10247/10248/10249\tHarvester management and compute nodes\tNginx worker process TCP\t8181\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t8444\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t10245\tHarvester management and compute nodes\tNginx-ingress-controller TCP\t80\tHarvester management and compute nodes\tNginx TCP\t9796\tHarvester management and compute nodes\tNode-exporter TCP\t30000-32767\tHarvester management and compute nodes\tNodePort port range TCP\t22\tHarvester management and compute nodes\tsshd UDP\t68\tHarvester management and compute nodes\tWicked TCP\t3260\tHarvester management and compute nodes\tiscsid Port Requirements for Integrating Harvester with Rancher​ If you want to integrate Harvester with Rancher, you need to make sure that all Harvester nodes can connect to TCP port 443 of the Rancher load balancer. When provisioning VMs with Kubernetes clusters from Rancher into Harvester, you need to be able to connect to TCP port 443 of the Rancher load balancer. Otherwise, the cluster won't be manageable by Rancher. For more information, refer to Rancher Architecture. Port Requirements for K3s or RKE/RKE2 Clusters​ For the port requirements for guest clusters deployed inside Harvester VMs, refer to the following links: K3s NetworkingRKE PortsRKE2 Networking","keywords":"Installation Requirements","version":"v1.4 (dev)"},{"title":"PXE Boot Installation","type":0,"sectionRef":"#","url":"/v1.4/install/pxe-boot-install","content":"PXE Boot Installation Starting from version 0.2.0, Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples. Prerequisite​ Nodes need to have at least 8 GiB of RAM because the installer loads the full ISO file into tmpfs. info The installer automatically checks if the hardware meets the minimum requirements for production use. If any of the checks fail, installation will be stopped. To override this behavior, set either the configuration file option install.skipchecks=true or the kernel parameter harvester.install.skipchecks=true. Preparing HTTP Servers​ An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10, and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/. Preparing Boot Files​ Download the required files from the Harvester releases page. The ISO: harvester-&lt;version&gt;-amd64.isoThe kernel: harvester-&lt;version&gt;-vmlinuz-amd64The initrd: harvester-&lt;version&gt;-initrd-amd64The rootfs squashfs image: harvester-&lt;version&gt;-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-&lt;version&gt;-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts​ When performing an automatic installation, there are two modes: CREATE: we are installing a node to construct an initial Harvester cluster.JOIN: we are installing a node to join an existing Harvester cluster. Harvester v1.3.0 introduces roles that you can assign to nodes to support different scenarios. For more information, see Harvester Configuration. CREATE Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml scheme_version: 1 token: token # Replace with a desired token os: hostname: node1 # Set a hostname. This can be omitted if DHCP server offers hostnames ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password ntp_servers: - 0.suse.pool.ntp.org - 1.suse.pool.ntp.org install: mode: create management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console vip: 10.100.0.99 # The VIP to access the Harvester GUI. Make sure the IP is free to use vip_mode: static # Or dhcp, check configuration file for more information # vip_hw_addr: 52:54:00:ec:0e:0b # Leave empty when vip_mode is static For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create. note If you have multiple network interfaces, you can leverage dracut's ip= parameter to specify the booting interface and any other network configurations that dracut supports (e.g., ip=eth1:dhcp). See man dracut.cmdline for more information. Use ip= parameter to designate the booting interface only, as we only support one single ip= parameter. JOIN Mode​ caution Security Risks: The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml scheme_version: 1 server_url: https://10.100.0.99:443 # Should be the VIP set up in &quot;CREATE&quot; config token: token os: hostname: node2 ssh_authorized_keys: - ssh-rsa ... # Replace with your public key password: p@ssword # Replace with your password dns_nameservers: - 1.1.1.1 - 8.8.8.8 install: mode: join management_interface: # available as of v1.1.0 interfaces: - name: ens5 default_route: true method: dhcp bond_options: mode: balance-tlb miimon: 100 device: /dev/sda # The target disk to install # data_disk: /dev/sdb # It is recommended to use a separate disk to store VM data iso_url: http://10.100.0.10/harvester/harvester-&lt;version&gt;-amd64.iso # tty: ttyS1,115200n8 # For machines without a VGA console Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-&lt;version&gt;-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-&lt;version&gt;-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join. DHCP Server Configuration​ note In the PXE installation scenario, you are required to add the routers option (option routers) when configuring the DHCP server. This option is used to add the default route on the Harvester host. Without the default route, the node will fail to start. In the ISO installation scenario, when the management network interface is in DHCP mode, you are also required to add the routers option (option routers) when configuring the DHCP server. For example: Harvester Host:~ # ip route default via 192.168.122.1 dev mgmt-br proto dhcp For more information, see ISC DHCPv4 Option Configuration. The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16; subnet 10.100.0.0 netmask 255.255.255.0 { option routers 10.100.0.10; option domain-name-servers 192.168.2.1; range 10.100.0.100 10.100.0.253; } group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } group { # join group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-join-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-join&quot;; } } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node2 { hardware ethernet 52:54:00:69:d5:92; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration​ For more information about Harvester configuration, please refer to the Harvester configuration page. By default, the first node will be the management node of the cluster. When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label topology.kubernetes.io/zone in the os.labels config. In this case, at least three different zones are required. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support​ UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program​ Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration​ If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = &quot;iPXE&quot; { # iPXE Boot if option architecture-type = 00:07 { filename &quot;http://10.100.0.10/harvester/ipxe-create-efi&quot;; } else { filename &quot;http://10.100.0.10/harvester/ipxe-create&quot;; } } elsif substring (option vendor-class-identifier, 0, 10) = &quot;HTTPClient&quot; { # UEFI HTTP Boot option vendor-class-identifier &quot;HTTPClient&quot;; filename &quot;http://10.100.0.10/harvester/ipxe.efi&quot;; } else { # PXE Boot if option architecture-type = 00:07 { # UEFI filename &quot;ipxe.efi&quot;; } else { # Non-UEFI filename &quot;undionly.kpxe&quot;; } } host node1 { hardware ethernet 52:54:00:6b:13:e2; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi. The iPXE Script for UEFI Boot​ It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-&lt;version&gt;-vmlinuz initrd=harvester-&lt;version&gt;-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-&lt;version&gt;-initrd boot The parameter initrd=harvester-&lt;version&gt;-initrd is required. Useful Kernel Parameters​ Besides the Harvester configuration, you can also specify other kernel parameters that are useful in different scenarios. See also dracut.cmdline(7). ip=dhcp​ If you have multiple network interfaces, you could add the ip=dhcp parameter to get IP from the DHCP server from all interfaces. rd.net.dhcp.retry=&lt;cnt&gt;​ Failing to get IP from the DHCP server would cause iPXE booting to fail. You can add parameter rd.net.dhcp.retry=&lt;cnt&gt;to retry DHCP request for &lt;cnt&gt; times. harvester.install.skipchecks=true​ Installation is stopped if the hardware checks fail (because the minimum requirements for production use are not met). To override this behavior, set the kernel parameter harvester.install.skipchecks=true. When set to true, warning messages are still saved to /var/log/console.log, but the installation proceeds even if hardware requirements for production use are not met. harvester.install.with_net_images=true​ The installer does not preload images during installation and instead pulls all required images from the internet after installation is completed. Usage of this parameter is not recommended in most cases. For more information, see Net Install ISO.","keywords":"Harvester harvester Rancher rancher Install Harvester Installing Harvester Harvester Installation PXE Boot Install","version":"v1.4 (dev)"},{"title":"Update Harvester Configuration After Installation","type":0,"sectionRef":"#","url":"/v1.4/install/update-harvester-configuration","content":"Update Harvester Configuration After Installation Harvester's OS has an immutable design, which means most files in the OS revert to their pre-configured state after a reboot. The Harvester OS loads the pre-configured values of system components from configuration files during the boot time. This page describes how to edit some of the most-requested Harvester configurations. To update a configuration, you must first update the runtime value in the system and then update configuration files to make the changes persistent between reboots. note If you upgrade from a version before v1.1.2, the cloud-init file in examples will be /oem/99_custom.yaml. Please substitute the value if needed. DNS servers​ Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Edit /etc/sysconfig/network/config and update the following line. Use a space to separate DNS server addresses if there are multiple servers. NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot; Update and reload the configuration with the following command: netconfig update Confirm the file /etc/resolv.conf contains the correct DNS servers with the cat command: cat /etc/resolv.conf Configuration persistence​ Beginning with v1.1.2, the persistent name of the cloud-init file is /oem/90_custom.yaml. Harvester now uses a newer version of Elemental, which creates the file during installation. When upgrading from an earlier version to v1.1.2 or later, Harvester retains the old file name (/oem/99_custom.yaml) to avoid confusion. You can manually rename the file to /oem/90_custom.yaml if necessary. Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the value under the yaml path stages.initramfs[0].commands. The commands array must contain a line to manipulate the NETCONFIG_DNS_STATIC_SERVERS config. Add the line if the line doesn't exist. The following example adds a line to change the NETCONFIG_DNS_STATIC_SERVERS config: stages: initramfs: - commands: - sed -i 's/^NETCONFIG_DNS_STATIC_SERVERS.*/NETCONFIG_DNS_STATIC_SERVERS=&quot;8.8.8.8 1.1.1.1&quot;/' /etc/sysconfig/network/config Replace the DNS server addresses and save the file. Harvester sets up new servers after rebooting. NTP servers​ We introduce the new mechanism for the NTP configuration in Harvester v1.2.0. For more information about NTP settings in Harvester v1.2.0 and later versions, see the NTP servers. Password of user rancher​ Runtime change​ Log in to a Harvester node as user rancher. See how to log into a Harvester node for more details.To reset the password for the user rancher, run the command passwd. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].users.rancher.passwd. Refer to the configuration os.password for details on how to specify the password in an encrypted form. Bonding slaves​ You can update the slave interfaces of Harvester's management bonding interface mgmt-bo. Runtime change​ Log in to a Harvester node and become root. See how to log into a Harvester node for more details. Identify the interface names with the following command: ip a Edit /etc/sysconfig/network/ifcfg-mgmt-bo and update the lines associated with bonding slaves and bonding mode: BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' Restart the network with the wicked ifreload command: wicked ifreload mgmt-bo caution A mistake in the configuration may disrupt the SSH session. Configuration persistence​ Backup the elemental cloud-init file /oem/90_custom.yaml as follows: cp /oem/90_custom.yaml /oem/install/90_custom.yaml.$(date --iso-8601=minutes) Edit /oem/90_custom.yaml and update the yaml path stages.initramfs[0].files. More specifically, update the content of the /etc/sysconfig/network/ifcfg-mgmt-bo file and edit the BONDING_SLAVE_X and BONDING_MODULE_OPTS entries accordingly: stages: initramfs: - ... files: - path: /etc/sysconfig/network/ifcfg-mgmt-bo permissions: 384 owner: 0 group: 0 content: |+ STARTMODE='onboot' BONDING_MASTER='yes' BOOTPROTO='none' POST_UP_SCRIPT=&quot;wicked:setup_bond.sh&quot; BONDING_SLAVE_0='ens5' BONDING_SLAVE_1='ens6' BONDING_MODULE_OPTS='miimon=100 mode=balance-tlb ' DHCLIENT_SET_DEFAULT_ROUTE='no' encoding: &quot;&quot; ownerstring: &quot;&quot; - path: /etc/sysconfig/network/ifcfg-ens6 permissions: 384 owner: 0 group: 0 content: | STARTMODE='hotplug' BOOTPROTO='none' encoding: &quot;&quot; ownerstring: &quot;&quot; note If you didn't select an interface during installation, you must add an entry to initialize the interface. Please check the /etc/sysconfig/network/ifcfg-ens6 file creation in the above example. The file name should be /etc/sysconfig/network/ifcfg-&lt;interface-name&gt;.","keywords":"Harvester configuration Configuration","version":"v1.4 (dev)"},{"title":"USB Installation","type":0,"sectionRef":"#","url":"/v1.4/install/usb-install","content":"USB Installation Create a bootable USB flash drive​ There are a couple of ways to create a USB installation flash drive. caution Known Issue: For the v1.2.0 ISO image, there is a known issue where the interactive ISO installation will get stuck using the USB method. To resolve this, you can use the patched ISO. This patched version only corrects the partition label, and there are no other changes. You can also use the related sha512 file to verify the ISO. Refer to the Harvester interactive ISO hangs with the USB installation method for details and a workaround. No matter which tool you use, creating a bootable device erases your USB device data. Please back up all data on your USB device before making a bootable device. Rufus​ Rufus allows you to create an ISO image on your USB flash drive on a Windows computer. Open Rufus and insert a clean USB stick into your computer. Rufus automatically detects your USB. Select the USB device you want to use from the Device drop-down menu. For Boot Selection, choose Select and find the Harvester installation ISO image you want to burn onto the USB. info If using older versions of Rufus, both DD mode and ISO mode works. DD mode works just like the dd command in Linux, and you can't browse partitions after you create a bootable device. ISO mode creates partitions on your device automatically and copies files to these partitions, and you can browse these partitions even after you create a bootable device. balenaEtcher​ balenaEtcher supports writing an image to a USB flash drive on most Linux distros, macOS, and Windows. It has a GUI and is easy to use. Select the Harvester installation ISO. Select the target USB device to create a USB installation flash drive. dd command​ You can use the 'dd' command on Linux or other platforms to create a USB installation flash drive. Ensure you choose the correct device; the following command erases data on the selected device. # sudo dd if=&lt;path_to_iso&gt; of=&lt;path_to_usb_device&gt; bs=64k Known issues​ A GRUB _ text is displayed, but nothing happens when booting from a USB installation flash drive​ If you use the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. For example, select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. The representation varies from system to system. Graphics issue​ Firmwares of some graphic cards are not shipped in v0.3.0. You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Harvester installer is not displayed​ If a USB flash driver boots, but you can't see the harvester installer, try one of the following workarounds: Plug the USB flash drive into a USB 2.0 slot.For version v0.3.0 or above, remove the console=ttyS0 parameter when booting. Press e to edit the GRUB menu entry and remove the console=ttyS0 parameter. Harvester interactive ISO hangs with the USB installation method​ During installation from a USB flash drive with v1.2.0 ISO image (created by tools like balenaEtcher, dd, etc.), the installation process may get stuck on the initial image loading process because a required label is missing on the boot partition. Therefore, the installation cannot mount the data partition correctly, causing some checks in dracut to be blocked. If you encounter this issue, you'll observe the following similar output, and the process will hang for at least 50 minutes (the default timeout value from dracut). Workaround​ To address this problem, you can manually modify the root partition as follows: # Replace the `CDLABEL=COS_LIVE` with your USB data partition. Usually, your USB data partition is the first partition with the device name `sdx` that hangs on your screen. # Original $linux ($root)/boot/kernel cdroot root=live:CDLABEL=COS_LIVE rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable net.ifnames=1 # Modified $linux ($root)/boot/kernel cdroot root=live:/dev/sda1 rd.live.dir=/ rd.live.squashimg=rootfs.squashfs console=tty1 console=ttyS0 rd.cos.disable net.ifnames=1 The modified parameter should look like the following: After making this adjustment, press Ctrl + x to initiate booting. You should now enter the installer as usual. Related issue: [BUG] v1.2.0 Interactive ISO Fails to Install On Some Bare-Metal Devices","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine","content":"Read a Virtual Machine GET /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-backup","content":"Read a Virtual Machine Backup GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.4/monitoring/harvester-monitoring","content":"Monitoring Available as of v1.2.0 The monitoring feature is now implemented with an addon and is disabled by default in new installations. Users can enable/disable rancher-monitoring addon from the Harvester WebUI after installation. Users can also enable/disable the rancher-monitoring addon in their Harvester installation by customizing the harvester-configuration file. For Harvester clusters upgraded from version v1.1.x, the monitoring feature is converted to an addon automatically and kept enabled as before. Dashboard Metrics​ Harvester has provided a built-in monitoring integration using Prometheus. Monitoring is automatically enabled during the Harvester installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboards on the Grafana UI. note Only admin users are able to view the cluster dashboard metrics. Additionally, Grafana is provided by rancher-monitoring, so the default admin password is: prom-operator Reference: values.yaml VM Detail Metrics​ For VMs, you can view VM metrics by clicking on the VM details page &gt; VM Metrics. note The current Memory Usage is calculated based on (1 - free/total) * 100%, not (used/total) * 100%. For example, in a Linux OS, the free -h command outputs the current memory statistics as follows $ free -h total used free shared buff/cache available Mem: 7.7Gi 166Mi 4.6Gi 1.0Mi 2.9Gi 7.2Gi Swap: 0B 0B 0B The corresponding Memory Usage is (1 - 4.6/7.7) * 100%, roughly 40%. How to Configure Monitoring Settings​ Available as of v1.0.2 Monitoring has several components that help to collect and aggregate metric data from all Nodes/Pods/VMs. The resources required for monitoring depend on your workloads and hardware resources. Harvester sets defaults based on general use cases, and you can change them accordingly. Currently, Resources Settings can be configured for the following components: PrometheusPrometheus Node Exporter From UI​ On the Advanced page, you can view and change the resource settings as follows: Go to the Advanced &gt; Addons page and select the rancher-monitoring page.From the Prometheus tab, change the resource requests and limits.Select Save when finished configuring the settings for the rancher-monitoring addon. The Monitoring deployments restart within a few seconds. Please be aware that the reboot can take time to reload previous data. note The UI configuration is only visible when the rancher-monitoring addon is enabled. The most frequently used option is the memory setting: The Requested Memory is the minimum memory required by the Monitoring resource. The recommended value is about 5% to 10% of the system memory of one single management node. A value less than 500Mi will be denied. The Memory Limit is the maximum memory that can be allocated to a Monitoring resource. The recommended value is about 30% of the system's memory for one single management node. When the Monitoring reaches this threshold, it will automatically restart. Depending on the available hardware resources and system loads, you may change the above settings accordingly. note If you have multiple management nodes with different hardware resources, please set the value of Prometheus based on the smaller one. caution When an increasing number of VMs get deployed on one node, the prometheus-node-exporter pod might get killed due to OOM (out of memory). In that case, you should increase the value of limits.memory. From CLI​ You can use the following kubectl command to change resource configurations for the rancher-monitoring addon: kubectl edit addons.harvesterhci.io -n cattle-monitoring-system rancher-monitoring. The resource path and default values are as follows: apiVersion: harvesterhci.io/v1beta1 kind: Addon metadata: name: rancher-monitoring namespace: cattle-monitoring-system spec: valuesContent: | prometheus: prometheusSpec: resources: limits: cpu: 1000m memory: 2500Mi requests: cpu: 850m memory: 1750Mi note You can still make configuration adjustments when the addon is disabled. However, these changes only take effect when you re-enable the addon. Alertmanager​ Harvester uses Alertmanager to collect and manage all the alerts that happened/happening in the cluster. Alertmanager Config​ Enable/Disable Alertmanager​ Alertmanager is enabled by default. You may disable it from the following config path. Change Resource Setting​ You can also change the resource settings of Alertmanager as shown in the picture above. Configure AlertmanagerConfig from WebUI​ To send the alerts to third-party servers, you need to config AlertmanagerConfig. On the WebUI, navigate to Monitoring &amp; Logging -&gt; Monitoring -&gt; Alertmanager Configs. On the Alertmanager Config: Create page, click Namespace to select the target namespace from the drop-down list and set the Name. After this, click Create in the lower right corner. Click the Alertmanager Configs you just created to continue the configuration. Click Add Receiver. Set the Name for the receiver. After this, select the receiver type, for example, Webhook, and click Add Webhook. Fill in the required parameters and click Create. Configure AlertmanagerConfig from CLI​ You can also add AlertmanagerConfig from the CLI. Exampe: a Webhook receiver in the default namespace. cat &lt;&lt; EOF &gt; a-single-receiver.yaml apiVersion: monitoring.coreos.com/v1alpha1 kind: AlertmanagerConfig metadata: name: amc-example # namespace: your value labels: alertmanagerConfig: example spec: route: continue: true groupBy: - cluster - alertname receiver: &quot;amc-webhook-receiver&quot; receivers: - name: &quot;amc-webhook-receiver&quot; webhookConfigs: - sendResolved: true url: &quot;http://192.168.122.159:8090/&quot; EOF # kubectl apply -f a-single-receiver.yaml alertmanagerconfig.monitoring.coreos.com/amc-example created # kubectl get alertmanagerconfig -A NAMESPACE NAME AGE default amc-example 27s Example of an Alert Received by Webhook​ Alerts sent to the webhook server will be in the following format: { 'receiver': 'longhorn-system-amc-example-amc-webhook-receiver', 'status': 'firing', 'alerts': [], 'groupLabels': {}, 'commonLabels': {'alertname': 'LonghornVolumeStatusWarning', 'container': 'longhorn-manager', 'endpoint': 'manager', 'instance': '10.52.0.83:9500', 'issue': 'Longhorn volume is Degraded.', 'job': 'longhorn-backend', 'namespace': 'longhorn-system', 'node': 'harv2', 'pod': 'longhorn-manager-r5bgm', 'prometheus': 'cattle-monitoring-system/rancher-monitoring-prometheus', 'service': 'longhorn-backend', 'severity': 'warning'}, 'commonAnnotations': {'description': 'Longhorn volume is Degraded for more than 5 minutes.', 'runbook_url': 'https://longhorn.io/docs/1.3.0/monitoring/metrics/', 'summary': 'Longhorn volume is Degraded'}, 'externalURL': 'https://192.168.122.200/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy', 'version': '4', 'groupKey': '{}/{namespace=&quot;longhorn-system&quot;}:{}', 'truncatedAlerts': 0 } note Different receivers may present the alerts in different formats. For details, please refer to the related documents. Known Limitation​ The AlertmanagerConfig is enforced by the namespace. Gloabl-level AlertmanagerConfig without a namespace is not supported. We have already created a GithHb issue to track upstream changes. Once the feature is available, Harvester will adopt it. View and Manage Alerts​ From Alertmanager Dashboard​ You can visit the original dashboard of Alertmanager from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-alertmanager:9093/proxy/#/alerts The overall view of the Alertmanager dashboard is as follows. You can view the details of an alert: From Prometheus Dashboard​ You can visit the original dashboard of Prometheus from the link below. Note that you need to replace the-cluster-vip with the actual cluster-vip: https://the-cluster-vip/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-prometheus:9090/proxy/ The Alerts menu in the top navigation bar shows all defined rules in Prometheus. You can use the filters Inactive, Pending, and Firing to quickly find the information that you need. Troubleshooting​ For Monitoring support and troubleshooting, please refer to the troubleshooting page .","keywords":"","version":"v1.4 (dev)"},{"title":"Harvester Network Deep Dive","type":0,"sectionRef":"#","url":"/v1.4/networking/deep-dive","content":"Harvester Network Deep Dive The network topology below reveals how we implement the Harvester network. The diagram contains the built-in cluster network mgmt and a custom cluster network called oob. As shown above, the Harvester network primarily focuses on OSI model layer 2. We leverage Linux network devices and protocols to construct traffic paths for the communication between VM to VM, VM to host, and VM to external network devices. The Harvester network is composed of three layers, including: KubeVirt networking layer Harvester networking layer External networking layer KubeVirt Networking​ The general purpose of KubeVirt is to run VM inside the Kubernetes pod. The KubeVirt network builds the network path between the pod and VM. Please refer to the KubeVirt official document for more details. Harvester Networking​ Harvester networking is designed to build the network path between pods and the host network. It implements a management network, VLAN networks and untagged networks. We can refer to the last two networks as bridge networks, because bridge plays a vital role in their implementation. Bridge Network​ We leverage multus CNI and bridge CNI to implement the bridge network. Multus CNI is a Container Network Interface (CNI) plugin for Kubernetes that can attach multiple network interfaces to a pod. Its capability allows a VM to have one NIC for the management network and multiple NICs for the bridge network. Using the bridge CNI, the VM pod will be plugged into the L2 bridge specified in the Network Attachment Definition config. # Example 1 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;vlan100&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;mgmt-br&quot;, &quot;promiscMode&quot;: true, &quot;vlan&quot;: 100, } # Example 2 { &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;name&quot;: &quot;untagged-network&quot;, &quot;type&quot;: &quot;bridge&quot;, &quot;bridge&quot;: &quot;oob-br&quot;, &quot;promiscMode&quot;: true, &quot;ipam&quot;: {} } Example 1 is a typical VLAN configuration with VLAN ID 100, while Example 2 is an untagged network configuration with no VLAN ID. The VM pod configured using Example 1 will be plugged into the bridge mgmt-br, while the VM pod using Example 2 will be plugged into the bridge oob-br. To achieve high availability and fault tolerance, a bond device where the real NICs are bound is created to serve as the uplink of the bridge. By default, this bond device will allow the target tagged traffic/packets to pass through. harvester-0:/home/rancher # bridge -c vlan show dev oob-bo port vlan ids oob-bo 1 PVID Egress Untagged 100 200 The example above shows that the bond oob-bo allows packages with tag 1, 100 or 200. Management Network​ The management network is based on Canal. It is worth mentioning that the Canal interface where the Harvester configures the node IP is the bridge mgmt-br or a VLAN sub-interface of mgmt-br. This design has two benefits: The built-in mgmt cluster network supports both the management network and bridge network.With the VLAN network interface, we can assign a VLAN ID to the management network. As components of the mgmt cluster network, it's not allowed to delete or modify the bridge mgmt-br, the bond mgmt-bo and the VLAN device. External Networking​ External network devices typically refer to switches and DHCP servers. With a cluster network, we can group host NICs and connect them to different switches for traffic isolation. Below are some usage instructions. To allow tagged packets to pass, you need to set the port type of the external switch or other devices (such as a DHCP server) to trunk or hybrid mode and allow the specified VLAN tag. You need to configure link aggregation on the switch based on the bond mode of the peer host. Link aggregation can work in manual mode or LACP mode. The following lists the correspondence between bond mode and link aggregation mode. Bond Mode\tLink Aggregation Modemode 0(balance-rr)\tmanual mode 1(active-backup)\tnone mdoe 2(balance-oxr)\tmanual mode 3(broadcast)\tmanual mode 4(802.3ad)\tLACP mode 5(balance-tlb)\tnone mode 6(balance-alb)\tnone If you want VMs in a VLAN to be able to obtain IP addresses through the DHCP protocol, configure an IP pool for that VLAN in the DHCP server.","keywords":"Harvester Networking Topology","version":"v1.4 (dev)"},{"title":"VM Network","type":0,"sectionRef":"#","url":"/v1.4/networking/harvester-network","content":"VM Network Harvester provides three types of networks for virtual machines (VMs), including: Management NetworkVLAN NetworkUntagged Network The management network is usually used for VMs whose traffic only flows inside the cluster. If your VMs need to connect to the external network, use the VLAN network or untagged network. Available as of v1.0.1 Harvester also introduced storage networking to separate the storage traffic from other cluster-wide workloads. Please refer to the storage network document for more details. Management Network​ Harvester uses Canal as its default management network. It is a built-in network that can be used directly from the cluster. By default, the management network IP of a VM can only be accessed within the cluster nodes, and the management network IP will change after the VM reboot. This is non-typical behaviour that needs to be taken note of since VM IPs are expected to remain unchanged after a reboot. However, you can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. How to use management network​ Since the management network is built-in and doesn't require extra operations, you can add it directly when configuring the VM network. important Network interfaces of VMs connected to the management network have an MTU value of 1450. This is because a VXLAN overlay network typically has a slightly higher per-packet overhead. If any of your workloads involve transmission of network traffic, you must specify the appropriate MTU value for the affected VM network interfaces and bridges. VLAN Network​ The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. Create a VM Network​ Go to Networks &gt; VM Networks. Select Create. Configure the following settings: Namespace Name Description (optional) On the Basics tab, configure the following settings: Type: Select L2VlanNetwork.Vlan ID Cluster Network On the Route tab, select an option and then specify the related IPv4 addresses. Auto(DHCP): The Harvester network controller retrieves the CIDR and gateway addresses from the DHCP server. You can specify the DHCP server address. Manual: Specify the CIDR and gateway addresses. important Harvester uses the information to verify that all nodes can access the VM network you are creating. If that is the case, the Network connectivity column on the VM Networks screen indicates that the network is active. Otherwise, the screen indicates that an error has occurred. Create a VM with VLAN Network​ You can now create a new VM using the VLAN network configured above: Click the Create button on the Virtual Machines page.Specify the required parameters and click the Networks tab.Either configure the default network to be a VLAN network or select an additional network to add. Untagged Network​ As is known, the traffic under a VLAN network has a VLAN ID tag and we can use the VLAN network with PVID (default 1) to communicate with any normal untagged traffic. However, some network devices may not expect to receive an explicitly tagged VLAN ID that matches the native VLAN on the switch the uplink belongs to. That's the reason why we provide the untagged network. How to use untagged network​ The usage of untagged network is similar to the VLAN network. To create a new untagged network, go to the Networks &gt; VM Networks page and click the Create button. You have to specify the name, select the type Untagged Network and choose the cluster network. note Starting from Harvester v1.1.2, Harvester supports updating and deleting VM networks. Make sure to stop all affected VMs before updating or deleting VM networks.","keywords":"Harvester Network","version":"v1.4 (dev)"},{"title":"Logging","type":0,"sectionRef":"#","url":"/v1.4/logging/harvester-logging","content":"Logging Available as of v1.2.0 It is important to know what is happening/has happened in the Harvester Cluster. Harvester collects the cluster running log, kubernetes audit and event log right after the cluster is powered on, which is helpful for monitoring, logging, auditing and troubleshooting. Harvester supports sending those logs to various types of log servers. note The size of logging data is related to the cluster scale, workload and other factors. Harvester does not use persistent storage to store log data inside the cluster. Users need to set up a log server to receive logs accordingly. The logging feature is now implemented with an addon and is disabled by default in new installations. Users can enable/disable the rancher-logging addon from the Harvester UI after installation. Users can also enable/disable the rancher-logging addon in their Harvester installation by customizing the harvester-configuration file. For Harvester clusters upgraded from version v1.1.x, the logging feature is converted to an addon automatically and kept enabled as before. High-level Architecture​ The Banzai Cloud Logging operator now powers both Harvester and Rancher as an in-house logging solution. In Harvester's practice, the Logging, Audit and Event shares one architecture, the Logging is the infrastructure, while the Audit and Event are on top of it. Logging​ The Harvester logging infrastructure allows you to aggregate Harvester logs into an external service such as Graylog, Elasticsearch, Splunk, Grafana Loki and others. Collected Logs​ See below for a list logs that are collected: Logs from all cluster PodsKernel logs from each nodeLogs from select systemd services from each node rke2-serverrke2-agentrancherdrancher-system-agentwickediscsid note Users are able to configure and modify where the aggregated logs are sent, as well as some basic filtering. It is not supported to change which logs are collected. Configuring Log Resources​ Underneath Banzai Cloud's logging operator are fluentd and fluent-bit, which handle the log routing and collecting respectively. If desired, you can modify how many resources are dedicated to those components. From UI​ Go to the Advanced &gt; Addons page and select the rancher-logging addon.From the Fluentbit tab, change the resource requests and limits.From the Fluentd tab, change the resource requests and limits.Select Save when finished configuring the settings for the rancher-logging addon. note The UI configuration is only visible when the rancher-logging addon is enabled. From CLI​ You can use the following kubectl command to change resource configurations for the rancher-logging addon: kubectl edit addons.harvesterhci.io -n cattle-logging-system rancher-logging. The resource path and default values are as follows. apiVersion: harvesterhci.io/v1beta1 kind: Addon metadata: name: rancher-logging namespace: cattle-logging-system spec: valuesContent: | fluentbit: resources: limits: cpu: 200m memory: 200Mi requests: cpu: 50m memory: 50Mi fluentd: resources: limits: cpu: 1000m memory: 800Mi requests: cpu: 100m memory: 200Mi note You can still make configuration adjustments when the addon is disabled. However, these changes only take effect when you re-enable the addon. Configuring Log Destinations​ Logging is backed by the Banzai Cloud Logging Operator, and so is controlled by Flows/ClusterFlows and Outputs/ClusterOutputs. You can route and filter logs as you like by applying these CRDs to the Harvester cluster. When applying new Ouptuts and Flows to the cluster, it can take some time for the logging operator to effectively apply them. So please allow a few minutes for the logs to start flowing. Clustered vs Namespaced​ One important thing to understand when routing logs is the difference between ClusterFlow vs Flow and ClusterOutput vs Output. The main difference between the clustered and non-clustered version of each is that the non-clustered versions are namespaced. The biggest implication of this is that Flows can only access Outputs that are within the same namespace, but can still access any ClusterOutput. For more information, see the documentation: Flows/ClusterFlowsOutputs/ClusterOutputs From UI​ note UI images are for Output and Flow whose configuration process is almost identical to their clustered counterparts. Any differences will be noted in the steps below. Creating Outputs​ Choose the option to create a new Output or ClusterOutput.If creating an Output, select the desired namespace.Add a name for the resources.Select the logging type.Select the logging output type. Configure the output buffer if necessary. Add any labels or annotations. Once done, click Create on the lower right. note Depending on the output selected (Splunk, Elasticsearch, etc), there will be additional fields to specify in the form. Output​ The fields present in the Output form will change depending on the Output chosen, in order to expose the fields present for each output plugin. Output Buffer​ The Output Buffer editor allows you to describe how you want the output buffer to behave. You can find the documentation for the buffer fields here. Labels &amp; Annotations​ You can append labels and annotations to the created resource. Creating Flows​ Choose the option to create a new Flow or ClusterFlow.If creating a Flow, select the desired namespace.Add a name for the resource.Select any nodes whose logs to include or exclude. Select target Outputs and ClusterOutputs. Add any filters if desired. Once done, click Create on the lower left. Matches​ Matches allow you to filter which logs you want to include in the Flow. The form only allows you to include or exclude node logs, but if needed, you can add other match rules supported by the resource by selecting Edit as YAML. For more information about the match directive, see Routing your logs with match directive. Outputs​ Outputs allow you to select one or more OutputRefs to send the aggregated logs to. When creating or editing a Flow / ClusterFlow, it is required that the user selects at least one Output. note There must be at least one existing ClusterOutput or Output that can be attached to the flow, or you will not be able to create / edit the flow. Filters​ Filters allow you to transform, process, and mutate the logs. In the text edit, you will find descriptions of the supported filters, but for more information, you can visit the list of supported filters. From CLI​ To configure log routes via the command line, you only need to define the YAML files for the relevant resources: # elasticsearch-logging.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: Output metadata: name: elasticsearch-example namespace: fleet-local labels: example-label: elasticsearch-example annotations: example-annotation: elasticsearch-example spec: elasticsearch: host: &lt;url-to-elasticsearch-server&gt; port: 9200 --- apiVersion: logging.banzaicloud.io/v1beta1 kind: Flow metadata: name: elasticsearch-example namespace: fleet-local spec: match: - select: {} globalOutputRefs: - elasticsearch-example And then apply them: kubectl apply -f elasticsearch-logging.yaml Referencing Secrets​ There are 3 ways Banzai Cloud allows specifying secret values via yaml values. The simplest is to use the value key, which is a simple string value for the desired secret. This method should only be used for testing and never in production: aws_key_id: value: &quot;secretvalue&quot; The next is to use valueFrom, which allows referencing a specific value from a secret by a name and key pair: aws_key_id: valueFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; Some plugins require a file to read from rather than simply receiving a value from the secret (this is often the case for CA cert files). In these cases, you need to use mountFrom, which will mount the secret as a file to the underlying fluentd deployment and point the plugin to the file. The valueFrom and mountFrom object look the same: tls_cert_path: mountFrom: secretKeyRef: name: &lt;kubernetes-secret-name&gt; key: &lt;kubernetes-secret-key&gt; For more information, you can find the related documentation here. Example Outputs​ Elasticsearch​ For the simplest deployment, you can deploy Elasticsearch on your local system using docker: docker run --name elasticsearch -p 9200:9200 -p 9300:9300 -e xpack.security.enabled=false -e node.name=es01 -it docker.elastic.co/elasticsearch/elasticsearch:6.8.23 Make sure that you have set vm.max_map_count to be &gt;= 262144 or the docker command above will fail. Once the Elasticsearch server is up, you can create the yaml file for the ClusterOutput and ClusterFlow: cat &lt;&lt; EOF &gt; elasticsearch-example.yaml apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: elasticsearch-example namespace: cattle-logging-system spec: elasticsearch: host: 192.168.0.119 port: 9200 buffer: timekey: 1m timekey_wait: 30s timekey_use_utc: true --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: elasticsearch-example namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - elasticsearch-example EOF And apply the file: kubectl apply -f elasticsearch-example.yaml After allowing some time for the logging operator to apply the resources, you can test that the logs are flowing: $ curl localhost:9200/fluentd/_search { &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: { &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 }, &quot;hits&quot;: { &quot;total&quot;: 11603, &quot;max_score&quot;: 1, &quot;hits&quot;: [ { &quot;_index&quot;: &quot;fluentd&quot;, &quot;_type&quot;: &quot;fluentd&quot;, &quot;_id&quot;: &quot;yWHr0oMBXcBggZRJgagY&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: { &quot;stream&quot;: &quot;stderr&quot;, &quot;logtag&quot;: &quot;F&quot;, &quot;message&quot;: &quot;I1013 02:29:43.020384 1 csi_handler.go:248] Attaching \\&quot;csi-974b4a6d2598d8a7a37b06d06557c428628875e077dabf8f32a6f3aa2750961d\\&quot;&quot;, &quot;kubernetes&quot;: { &quot;pod_name&quot;: &quot;csi-attacher-5d4cc8cfc8-hd4nb&quot;, &quot;namespace_name&quot;: &quot;longhorn-system&quot;, &quot;pod_id&quot;: &quot;c63c2014-9556-40ce-a8e1-22c55de12e70&quot;, &quot;labels&quot;: { &quot;app&quot;: &quot;csi-attacher&quot;, &quot;pod-template-hash&quot;: &quot;5d4cc8cfc8&quot; }, &quot;annotations&quot;: { &quot;cni.projectcalico.org/containerID&quot;: &quot;857df09c8ede7b8dee786a8c8788e8465cca58f0b4d973c448ed25bef62660cf&quot;, &quot;cni.projectcalico.org/podIP&quot;: &quot;10.52.0.15/32&quot;, &quot;cni.projectcalico.org/podIPs&quot;: &quot;10.52.0.15/32&quot;, &quot;k8s.v1.cni.cncf.io/network-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;k8s.v1.cni.cncf.io/networks-status&quot;: &quot;[{\\n \\&quot;name\\&quot;: \\&quot;k8s-pod-network\\&quot;,\\n \\&quot;ips\\&quot;: [\\n \\&quot;10.52.0.15\\&quot;\\n ],\\n \\&quot;default\\&quot;: true,\\n \\&quot;dns\\&quot;: {}\\n}]&quot;, &quot;kubernetes.io/psp&quot;: &quot;global-unrestricted-psp&quot; }, &quot;host&quot;: &quot;harvester-node-0&quot;, &quot;container_name&quot;: &quot;csi-attacher&quot;, &quot;docker_id&quot;: &quot;f10e4449492d4191376d3e84e39742bf077ff696acbb1e5f87c9cfbab434edae&quot;, &quot;container_hash&quot;: &quot;sha256:03e115718d258479ce19feeb9635215f98e5ad1475667b4395b79e68caf129a6&quot;, &quot;container_image&quot;: &quot;docker.io/longhornio/csi-attacher:v3.4.0&quot; } } }, ... ] } } Graylog​ You can follow the instructions here to deploy and view cluster logs via Graylog: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;all-logs-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;example-gelf-hs&quot; --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;example-gelf-hs&quot; namespace: &quot;cattle-logging-system&quot; spec: gelf: host: &quot;192.168.122.159&quot; port: 12202 protocol: &quot;udp&quot; Splunk​ You can follow the instructions here to deploy and view cluster logs via Splunk. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: splunkHec: hec_host: 192.168.122.101 hec_port: 8088 insecure_ssl: true index: harvester-log-index hec_token: valueFrom: secretKeyRef: key: HECTOKEN name: splunk-hec-token2 buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-logging-splunk namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: globalOutputRefs: - harvester-logging-splunk Loki​ You can follow the instructions in the logging HEP on deploying and viewing cluster logs via Grafana Loki. apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-loki namespace: cattle-logging-system spec: match: - select: {} globalOutputRefs: - harvester-loki --- apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: harvester-loki namespace: cattle-logging-system spec: loki: url: http://loki-stack.cattle-logging-system.svc:3100 extra_labels: logOutput: harvester-loki Audit​ Harvester collects Kubernetes audit and is able to send the audit to various types of log servers. The policy file to guide kube-apiserver is here. Audit Definition​ In kubernetes, the audit data is generated by kube-apiserver according to defined policy. ... Audit policy Audit policy defines rules about what events should be recorded and what data they should include. The audit policy object structure is defined in the audit.k8s.io API group. When an event is processed, it's compared against the list of rules in order. The first matching rule sets the audit level of the event. The defined audit levels are: None - don't log events that match this rule. Metadata - log request metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body. Request - log event metadata and request body but not response body. This does not apply for non-resource requests. RequestResponse - log event metadata, request and response bodies. This does not apply for non-resource requests. Audit Log Format​ Audit Log Format in Kubernetes​ Kubernetes apiserver logs audit with following JSON format into a local file. { &quot;kind&quot;:&quot;Event&quot;, &quot;apiVersion&quot;:&quot;audit.k8s.io/v1&quot;, &quot;level&quot;:&quot;Metadata&quot;, &quot;auditID&quot;:&quot;13d0bf83-7249-417b-b386-d7fc7c024583&quot;, &quot;stage&quot;:&quot;RequestReceived&quot;, &quot;requestURI&quot;:&quot;/apis/flowcontrol.apiserver.k8s.io/v1beta2/prioritylevelconfigurations?fieldManager=api-priority-and-fairness-config-producer-v1&quot;, &quot;verb&quot;:&quot;create&quot;, &quot;user&quot;:{&quot;username&quot;:&quot;system:apiserver&quot;,&quot;uid&quot;:&quot;d311c1fe-2d96-4e54-a01b-5203936e1046&quot;,&quot;groups&quot;:[&quot;system:masters&quot;]}, &quot;sourceIPs&quot;:[&quot;::1&quot;], &quot;userAgent&quot;:&quot;kube-apiserver/v1.24.7+rke2r1 (linux/amd64) kubernetes/e6f3597&quot;, &quot;objectRef&quot;:{&quot;resource&quot;:&quot;prioritylevelconfigurations&quot;, &quot;apiGroup&quot;:&quot;flowcontrol.apiserver.k8s.io&quot;, &quot;apiVersion&quot;:&quot;v1beta2&quot;}, &quot;requestReceivedTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot;, &quot;stageTimestamp&quot;:&quot;2022-10-19T18:55:07.244781Z&quot; } Audit Log Format before Being Sent to Log Servers​ Harvester keeps the audit log unchanged before sending it to the log server. Audit Log Output/ClusterOutput​ To output audit related log, the Output/ClusterOutput requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit Only from the Type drpo-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterOutput metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: http: endpoint: &quot;http://192.168.122.159:8096/&quot; open_timeout: 3 format: type: &quot;json&quot; buffer: chunk_limit_size: 3MB timekey: 2m timekey_wait: 1m loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Audit Log Flow/ClusterFlow​ To route audit related logs, the Flow/ClusterFlow requires the value of loggingRef to be harvester-kube-audit-log-ref. When you configure from the Harvester dashboard, the field is added automatically. Select type Audit. When you config from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: &quot;harvester-audit-webhook&quot; namespace: &quot;cattle-logging-system&quot; spec: globalOutputRefs: - &quot;harvester-audit-webhook&quot; loggingRef: harvester-kube-audit-log-ref # this reference is fixed and must be here Harvester​ Event​ Harvester collects Kubernetes event and is able to send the event to various types of log servers. Event Definition​ Kubernetes events are objects that show you what is happening inside a cluster, such as what decisions were made by the scheduler or why some pods were evicted from the node. All core components and extensions (operators/controllers) may create events through the API Server. Events have no direct relationship with log messages generated by the various components, and are not affected with the log verbosity level. When a component creates an event, it often emits a corresponding log message. Events are garbage collected by the API Server after a short time (typically after an hour), which means that they can be used to understand issues that are happening, but you have to collect them to investigate past events. Events are the first thing to look at for application, as well as infrastructure operations when something is not working as expected. Keeping them for a longer period is essential if the failure is the result of earlier events, or when conducting post-mortem analysis. Event Log Format​ Event Log Format in Kubernetes​ A kubernetes event example: { &quot;apiVersion&quot;: &quot;v1&quot;, &quot;count&quot;: 1, &quot;eventTime&quot;: null, &quot;firstTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;involvedObject&quot;: { &quot;apiVersion&quot;: &quot;kubevirt.io/v1&quot;, &quot;kind&quot;: &quot;VirtualMachineInstance&quot;, &quot;name&quot;: &quot;vm-ide-1&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604601&quot;, &quot;uid&quot;: &quot;1bd4133f-5aa3-4eda-bd26-3193b255b480&quot; }, &quot;kind&quot;: &quot;Event&quot;, &quot;lastTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;message&quot;: &quot;VirtualMachineInstance defined.&quot;, &quot;metadata&quot;: { &quot;creationTimestamp&quot;: &quot;2022-08-24T11:17:35Z&quot;, &quot;name&quot;: &quot;vm-ide-1.170e43cbdd833b62&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;resourceVersion&quot;: &quot;604626&quot;, &quot;uid&quot;: &quot;0114f4e7-1d4a-4201-b0e5-8cc8ede202f4&quot; }, &quot;reason&quot;: &quot;Created&quot;, &quot;reportingComponent&quot;: &quot;&quot;, &quot;reportingInstance&quot;: &quot;&quot;, &quot;source&quot;: { &quot;component&quot;: &quot;virt-handler&quot;, &quot;host&quot;: &quot;harv1&quot; }, &quot;type&quot;: &quot;Normal&quot; }, Event Log Format before Being Sent to Log Servers​ Each event log has the format of: {&quot;stream&quot;:&quot;&quot;,&quot;logtag&quot;:&quot;F&quot;,&quot;message&quot;:&quot;&quot;,&quot;kubernetes&quot;:{&quot;&quot;}}. The kubernetes event is in the field message. { &quot;stream&quot;:&quot;stdout&quot;, &quot;logtag&quot;:&quot;F&quot;, &quot;message&quot;:&quot;{ \\\\&quot;verb\\\\&quot;:\\\\&quot;ADDED\\\\&quot;, \\\\&quot;event\\\\&quot;:{\\\\&quot;metadata\\\\&quot;:{\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1.170e446c3f890433\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;0b44b6c7-b415-4034-95e5-a476fcec547f\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612482\\\\&quot;,\\\\&quot;creationTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;managedFields\\\\&quot;:[{\\\\&quot;manager\\\\&quot;:\\\\&quot;virt-controller\\\\&quot;,\\\\&quot;operation\\\\&quot;:\\\\&quot;Update\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;v1\\\\&quot;,\\\\&quot;time\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;}]},\\\\&quot;involvedObject\\\\&quot;:{\\\\&quot;kind\\\\&quot;:\\\\&quot;VirtualMachineInstance\\\\&quot;,\\\\&quot;namespace\\\\&quot;:\\\\&quot;default\\\\&quot;,\\\\&quot;name\\\\&quot;:\\\\&quot;vm-ide-1\\\\&quot;,\\\\&quot;uid\\\\&quot;:\\\\&quot;1bd4133f-5aa3-4eda-bd26-3193b255b480\\\\&quot;,\\\\&quot;apiVersion\\\\&quot;:\\\\&quot;kubevirt.io/v1\\\\&quot;,\\\\&quot;resourceVersion\\\\&quot;:\\\\&quot;612477\\\\&quot;},\\\\&quot;reason\\\\&quot;:\\\\&quot;SuccessfulDelete\\\\&quot;,\\\\&quot;message\\\\&quot;:\\\\&quot;Deleted PodDisruptionBudget kubevirt-disruption-budget-hmmgd\\\\&quot;,\\\\&quot;source\\\\&quot;:{\\\\&quot;component\\\\&quot;:\\\\&quot;disruptionbudget-controller\\\\&quot;},\\\\&quot;firstTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;lastTimestamp\\\\&quot;:\\\\&quot;2022-08-24T11:29:04Z\\\\&quot;,\\\\&quot;count\\\\&quot;:1,\\\\&quot;type\\\\&quot;:\\\\&quot;Normal\\\\&quot;,\\\\&quot;eventTime\\\\&quot;:null,\\\\&quot;reportingComponent\\\\&quot;:\\\\&quot;\\\\&quot;,\\\\&quot;reportingInstance\\\\&quot;:\\\\&quot;\\\\&quot;} }&quot;, &quot;kubernetes&quot;:{&quot;pod_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;namespace_name&quot;:&quot;cattle-logging-system&quot;,&quot;pod_id&quot;:&quot;d3453153-58c9-456e-b3c3-d91242580df3&quot;,&quot;labels&quot;:{&quot;app.kubernetes.io/instance&quot;:&quot;harvester-default-event-tailer&quot;,&quot;app.kubernetes.io/name&quot;:&quot;event-tailer&quot;,&quot;controller-revision-hash&quot;:&quot;harvester-default-event-tailer-747b9d4489&quot;,&quot;statefulset.kubernetes.io/pod-name&quot;:&quot;harvester-default-event-tailer-0&quot;},&quot;annotations&quot;:{&quot;cni.projectcalico.org/containerID&quot;:&quot;aa72487922ceb4420ebdefb14a81f0d53029b3aec46ed71a8875ef288cde4103&quot;,&quot;cni.projectcalico.org/podIP&quot;:&quot;10.52.0.178/32&quot;,&quot;cni.projectcalico.org/podIPs&quot;:&quot;10.52.0.178/32&quot;,&quot;k8s.v1.cni.cncf.io/network-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;k8s.v1.cni.cncf.io/networks-status&quot;:&quot;[{\\\\n \\\\&quot;name\\\\&quot;: \\\\&quot;k8s-pod-network\\\\&quot;,\\\\n \\\\&quot;ips\\\\&quot;: [\\\\n \\\\&quot;10.52.0.178\\\\&quot;\\\\n ],\\\\n \\\\&quot;default\\\\&quot;: true,\\\\n \\\\&quot;dns\\\\&quot;: {}\\\\n}]&quot;,&quot;kubernetes.io/psp&quot;:&quot;global-unrestricted-psp&quot;},&quot;host&quot;:&quot;harv1&quot;,&quot;container_name&quot;:&quot;harvester-default-event-tailer-0&quot;,&quot;docker_id&quot;:&quot;455064de50cc4f66e3dd46c074a1e4e6cfd9139cb74d40f5ba00b4e3e2a7ab2d&quot;,&quot;container_hash&quot;:&quot;docker.io/banzaicloud/eventrouter@sha256:6353d3f961a368d95583758fa05e8f4c0801881c39ed695bd4e8283d373a4262&quot;,&quot;container_image&quot;:&quot;docker.io/banzaicloud/eventrouter:v0.1.0&quot;} } Event Log Output/ClusterOutput​ Events share the Output/ClusterOutput with Logging. Select Logging/Event from the Type drop-down list. Event Log Flow/ClusterFlow​ Compared with the normal Logging Flow/ClusterFlow, the Event related Flow/ClusterFlow, has one more match field with the value of event-tailer. When you configure from the Harvester dashboard, the field is added automatically. Select Event from the Type drop-down list. When you configure from the CLI, please add the field manually. Example: apiVersion: logging.banzaicloud.io/v1beta1 kind: ClusterFlow metadata: name: harvester-event-webhook namespace: cattle-logging-system spec: filters: - tag_normaliser: {} match: - select: labels: app.kubernetes.io/name: event-tailer globalOutputRefs: - harvester-event-webhook ","keywords":"Harvester Logging Audit Event","version":"v1.4 (dev)"},{"title":"Cluster Network","type":0,"sectionRef":"#","url":"/v1.4/networking/index","content":"Cluster Network Concepts​ Cluster Network​ Available as of v1.1.0 In Harvester v1.1.0, we introduced a new concept called cluster network for traffic isolation. The following diagram describes a typical network architecture that separates data-center (DC) traffic from out-of-band (OOB) traffic. We abstract the sum of devices, links, and configurations on a traffic-isolated forwarding path on Harvester as a cluster network. In the above case, there will be two cluster networks corresponding to two traffic-isolated forwarding paths. Network Configuration​ Specifications including network devices of the Harvester hosts can be different. To be compatible with such a heterogeneous cluster, we designed the network configuration. Network configuration only works under a certain cluster network. Each network configuration corresponds to a set of hosts with uniform network specifications. Therefore, multiple network configurations are required for a cluster network on non-uniform hosts. VM Network​ A VM network is an interface in a virtual machine that connects to the host network. As with a network configuration, every network except the built-in management network must be under a cluster network. Harvester supports adding multiple networks to one VM. If a network's cluster network is not enabled on some hosts, the VM that owns this network will not be scheduled to those hosts. Please refer to network part for more details about networks. Relationship Between Cluster Network, Network Config, VM Network​ The following diagram shows the relationship between a cluster network, a network config, and a VM network. All Network Configs and VM Networks are grouped under a cluster network. A label can be assigned to each host to categorize hosts based on their network specifications. A network config can be added for each group of hosts using a node selector. For example, in the diagram above, the hosts in ClusterNetwork-A are divided into three groups as follows: The first group includes host0, which corresponds to network-config-A.The second group includes host1 and host2, which correspond to network-config-B.The third group includes the remaining hosts (host3, host4, and host5), which do not have any related network config and therefore do not belong to ClusterNetwork-A. The cluster network is only effective on hosts that are covered by the network configuration. A VM using a VM network under a specific cluster network can only be scheduled on a host where the cluster network is active. In the diagram above, we can see that: ClusterNetwork-A is active on host0, host1, and host2. VM0 uses VM-network-A, so it can be scheduled on any of these hosts.VM1 uses both VM-network-B and VM-network-C, so it can only be scheduled on host2 where both ClusterNetwork-A and ClusterNetwork-B are active.VM0, VM1, and VM2 cannot run on host3 where the two cluster networks are inactive. Overall, this diagram provides a clear visualization of the relationship between cluster networks, network configurations, and VM networks, as well as how they impact VM scheduling on hosts. Cluster Network Details​ Built-in Cluster Network​ Harvester provides a built-in cluster network called mgmt. It's different from the custom cluster network. The mgmt cluster network: Cannot be deleted.Does not need any network configuration.Is enabled on all hosts and cannot be disabled.Shares the same traffic egress with the management network. If there is no need for traffic separation, you can put all your network under the mgmt cluster network. Custom Cluster Network​ You are allowed to add the custom cluster network, which will not be available until it's enabled on some hosts by adding a network configuration. How to create a new cluster network​ To create a cluster network, go to the Networks &gt; ClusterNetworks/Configs page and click the Create button. You only need to specify the name. Click the Create Network Config button on the right of the cluster network to create a new network configuration. In the Node Selector tab, specify the name and choose one of the three methods to select nodes where the network configuration will apply. If you want to cover the unselected nodes, you can create another network configuration. Click the Uplink tab to add the NICs, and configure the bond options and link attributes. The bond mode defaults to active-backup. note The NICs drop-down list shows all the common NICs on all the selected nodes. The drop-down list will change as you select different nodes.The text enp7s3 (1/3 Down) in the NICs drop-down list indicates that the enp7s3 NIC is down in one of the three selected nodes. In this case, you need to find the NIC, set it up, and refresh this page. After this, it should be selectable. note Starting with Harvester v1.1.2, Harvester supports updating network configs. Make sure to stop all affected VMs before updating network configs.","keywords":"Harvester Networking ClusterNetwork NetworkConfig Network","version":"v1.4 (dev)"},{"title":"Read a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.4/api/read-namespaced-virtual-machine-template-version","content":"Read a Virtual Machine Template Version GET /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Get a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects Query Parameters exact boolean Should the export be exact. Exact export maintains cluster-specific fields like 'Namespace'. export boolean Should this value be exported. Export strips fields that a user can not specify. Responses​ 200401 OK application/jsonapplication/yamlapplication/json;stream=watch SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"IP Pool","type":0,"sectionRef":"#","url":"/v1.4/networking/ippool","content":"IP Pool Available as of v1.2.0 Harvester IP Pool is a built-in IP address management (IPAM) solution exclusively available to Harvester load balancers (LBs). Features​ Multiple IP ranges: Each IP pool can contain multiple IP ranges or CIDRs.Allocation history: The IP pool keeps track of the allocation history of every IP address and prioritizes assigning previously allocated addresses by load balancer name. status: allocatedHistory: 192.168.178.8: default/rke2-default-lb-pool-2fab9ac0 Scope: IP pools can be confined to a particular network, project, namespace, or guest cluster. How to create​ To create a new IP pool: Go to the Networks &gt; IP Pools page and select Create.Specify the Name of the IP pool.Go to the Range tab to specify the IP ranges for the IP pool. You can add multiple IP ranges.Go to the Selector tab to specify the Scope and Priority of the IP pool. Selection policy​ Each IP pool will have a specific range, and you can specify the corresponding requirements in the LB annotations. IP pools that meet the specified requirements will automatically assign IP addresses to LBs. LBs utilize the following annotations to express requirements (all annotations are optional): loadbalancer.harvesterhci.io/network specifies the VM network the guest cluster nodes use.loadbalancer.harvesterhci.io/project and loadbalancer.harvesterhci.io/namespace identify the project and namespace of the VMs that comprise the guest cluster.loadbalancer.harvesterhci.io/cluster denotes the name of the guest cluster. The IP pool has a selector, including network and scope, to match the requirements of the LB. Network is a hard condition. The optional IP pool must match the value of the LB annotation loadbalancer.harvesterhci.io/network.Every IP pool, except the global IP pool, has a unique scope different from others if its priority is 0. The project, namespace, or cluster name of LBs should be in the scope of the IP pool if they want to get an IP from this pool. spec.selector.priority specifies the priority of the IP Pool. The larger the number, the higher the priority. If the priority is not 0, the value should differ. The priority helps you to migrate the old IP pool to the new one.If the IP Pool has a scope that matches all projects, namespaces, and guest clusters, it's called a global IP pool, and only one global IP pool is allowed. If there is no IP pool matching the requirements of the LB, the IPAM will allocate an IP address from the global IP pool if it exists. Examples​ Example 1: You wish to set up an IP pool within the range 192.168.100.0/24 for the default namespace. In this scenario, all load balancers within the default namespace will receive an IP address from this designated IP pool: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: default-ip-pool spec: ranges: - subnet: 192.168.100.0/24 selector: scope: namespace: default Example 2: You have a guest cluster rke2 deployed within the network default/vlan1, and its project/namespace name is product/default. If you want to configure an exclusive IP pool range 192.168.10.10-192.168.10.20 for it. Refer to the following YAML config: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: rke2-ip-pool spec: ranges: - subnet: 192.168.10.0/24 rangeStart: 192.168.10.10 rangeEnd: 192.168.10.20 selector: network: default/vlan1 scope: - project: product namespace: default guestCluster: rke2 Example 3: If you have specified the IP pool default-ip-pool for the default namespace, you want to migrate the IP pool default-ip-pool to a different IP pool default-ip-pool-2 with range 192.168.200.0/24. It's not allowed to specify over one IP pool for the same scope, but you can give the IP pool default-ip-pool-2 a higher priority than default-ip-pool. Refer to the following YAML config: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: default-ip-pool-2 spec: ranges: - subnet: 192.168.200.0/24 selector: priority: 1 # The priority is higher than default-ip-pool scope: namespace: default Example 4: You want to configure a global IP pool with a CIDR range of 192.168.20.0/24: apiVersion: networking.harvesterhci.io/v1beta1 kind: IPPool metadata: name: global-ip-pool spec: ranges: - subnet: 192.168.20.0/24 selector: scope: - project: &quot;*&quot; namespace: &quot;*&quot; guestCluster: &quot;*&quot; Allocation policy​ The IP pool prioritizes the allocation of previously assigned IP addresses based on their allocation history.IP addresses are assigned in ascending order. note Starting with Harvester v1.2.0, the vip-pools setting is deprecated. Following the upgrade, this setting will be automatically migrated to the Harvester IP pools.","keywords":"IP Pool","version":"v1.4 (dev)"},{"title":"Load Balancer","type":0,"sectionRef":"#","url":"/v1.4/networking/loadbalancer","content":"Load Balancer Available as of v1.2.0 The Harvester load balancer (LB) is a built-in Layer 4 load balancer that distributes incoming traffic across workloads deployed on Harvester virtual machines (VMs) or guest Kubernetes clusters. VM load balancer​ Features​ Harvester VM load balancer supports the following features: Address assignment: Get the LB IP address from a DHCP server or a pre-defined IP pool.Protocol support: Supports both TCP and UDP protocols for load balancing.Multiple listeners: Create multiple listeners to handle incoming traffic on different ports or with other protocols.Label selector: The LB uses label selectors to match the backend servers. Therefore, you must configure the corresponding labels for the backend VMs you want to add to the LB.Health check: Only send traffic to healthy backend instances. Limitations​ Harvester VM load balancer has the following limitations: Namespace restriction: This restriction facilitates permission management and ensures the LB only uses VMs in the same namespace as the backend servers.IPv4-only: The LB is only compatible with IPv4 addresses for VMs.Guest agent installation: Installing the guest agent on each backend VM is required to obtain IP addresses. Connectivity Requirement: Network connectivity must be established between backend VMs and Harvester hosts. When a VM has multiple IP addresses, the LB will select the first one as the backend address.Access Restriction: The VM LB address is exposed only within the same network as the Harvester hosts. To access the LB from outside the network, you must provide a route from outside to the LB address. note Harvester VM load balancer doesn't support Windows VMs because the guest agent is not available for Windows VMs. How to create​ To create a new Harvester VM load balancer: Go to the Networks &gt; Load Balancers page and select Create.Select the Namespace and specify the Name.Go to the Basic tab to choose the IPAM mode, which can be DHCP or IP Pool. If you select IP Pool, prepare an IP pool first, specify the IP pool name, or choose auto. If you choose auto, the LB automatically selects an IP pool according to the IP pool selection policy.Go to the Listeners tab to add listeners. You must specify the Port, Protocol, and Backend Port for each listener.Go to the Backend Server Selector tab to add label selectors. To add the VM to the LB, go to the Virtual Machine &gt; Instance Labels tab to add the corresponding labels to the VM.Go to the Health Check tab to enable health check and specify the parameters, including the Port, Success Threshold, Failure Threshold, Interval, and Timeout if the backend service supports health check. Refer to Health Checks for more details. Health Checks​ The Harvester load balancer supports TCP health checks. You can specify the parameters in the Harvester UI if you've enabled the Health Check option. Name\tValue Type\tRequired\tDefault\tDescriptionHealth Check Port\tint\ttrue\tN/A\tSpecifies the port. The prober will access the address composed of the backend server IP and the port. Health Check Success Threshold\tint\tfalse\t1\tSpecifies the health check success threshold. Disabled by default. The backend server will start forwarding traffic if the number of times the prober continuously detects an address successfully reaches the threshold. Health Check Failure Threshold\tint\tfalse\t3\tSpecifies the health check failure threshold. Disabled by default. The backend server will stop forwarding traffic if the number of health check failures reaches the threshold. Health Check Period\tint\tfalse\t5\tSpecifies the health check period in seconds. Disabled by default. Health Check Timeout\tint\tfalse\t3\tSpecifies the timeout of every health check in seconds. Disabled by default. Guest Kubernetes cluster load balancer​ In conjunction with Harvester Cloud Provider, the Harvester load balancer provides load balancing for LB services in the guest cluster.When you create, update, or delete an LB service on a guest cluster with Harvester Cloud Provider, the Harvester Cloud Provider will create a Harvester LB automatically. For more details, refer to Harvester Cloud Provider.","keywords":"Load Balancer","version":"v1.4 (dev)"},{"title":"Harvester Cloud Provider","type":0,"sectionRef":"#","url":"/v1.4/rancher/cloud-provider","content":"Harvester Cloud Provider RKE1 and RKE2 clusters can be provisioned in Rancher using the built-in Harvester Node Driver. Harvester provides load balancer and Harvester cluster storage passthrough support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2 cluster.How to use the Harvester load balancer. Backward Compatibility Notice​ note Please note a known backward compatibility issue if you're using the Harvester cloud provider version v0.2.2 or higher. If your Harvester version is below v1.2.0 and you intend to use newer RKE2 versions (i.e., &gt;= v1.26.6+rke2r1, v1.25.11+rke2r1, v1.24.15+rke2r1), it is essential to upgrade your Harvester cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester cloud provider. For a detailed support matrix, please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section of the official website. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hope to remove this limitation in a future release of Harvester. Deploying to the RKE1 Cluster with Harvester Node Driver​ When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select Harvester(Out-of-tree) option. Install Harvester Cloud Provider from the Rancher marketplace. Deploying to the RKE2 Cluster with Harvester Node Driver​ When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Deploying to the RKE2 custom cluster (experimental)​ Use generate_addon.sh to generate a cloud-config and place it into the directory /etc/kubernetes/cloud-config on every custom node. curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | bash -s &lt;serviceaccount name&gt; &lt;namespace&gt; note The generate_addon.sh script depends on kubectl and jq to operate the Harvester cluster. The script needs access to the Harvester Cluster kubeconfig to work. You can find the kubeconfig file from one of the Harvester management nodes in the /etc/rancher/rke2/rke2.yaml path. The namespace needs to be the namespace in which the guest cluster will be created. Configure the Cloud Provider to Harvester and select Create to spin up the cluster. Deploying to the K3s cluster with Harvester node driver (experimental)​ When spinning up a K3s cluster using the Harvester node driver, you can perform the following steps to deploy the harvester cloud provider: Use generate_addon.sh to generate cloud config. curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | bash -s &lt;serviceaccount name&gt; &lt;namespace&gt; The output will look as follows: ########## cloud config ############ apiVersion: v1 clusters: - cluster: certificate-authority-data: &lt;CACERT&gt; server: https://HARVESTER-ENDPOINT/k8s/clusters/local name: local contexts: - context: cluster: local namespace: default user: harvester-cloud-provider-default-local name: harvester-cloud-provider-default-local current-context: harvester-cloud-provider-default-local kind: Config preferences: {} users: - name: harvester-cloud-provider-default-local user: token: &lt;TOKEN&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: &lt;CONTENT&gt; owner: root:root path: /etc/kubernetes/cloud-config permissions: '0644' Copy and paste the cloud-init user data content to Machine Pools &gt;Show Advanced &gt; User Data. Add the following HelmChart yaml of harvester-cloud-provider to Cluster Configuration &gt; Add-On Config &gt; Additional Manifest. apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: harvester-cloud-provider namespace: kube-system spec: targetNamespace: kube-system bootstrap: true repo: https://charts.harvesterhci.io/ chart: harvester-cloud-provider version: 0.2.2 helmVersion: v3 Disable the in-tree cloud provider in the following ways: Click the Edit as YAML button. Disable servicelb and set disable-cloud-controller: true to disable the default K3s cloud controller. machineGlobalConfig: disable: - servicelb disable-cloud-controller: true Add cloud-provider=external to use the Harvester cloud provider. machineSelectorConfig: - config: kubelet-arg: - cloud-provider=external protect-kernel-defaults: false With these settings in place a K3s cluster should provision successfully while using the external cloud provider. Upgrade Cloud Provider​ Upgrade RKE2​ The cloud provider can be upgraded by upgrading the RKE2 version. You can upgrade the RKE2 cluster via the Rancher UI as follows: Click ☰ &gt; Cluster Management.Find the guest cluster that you want to upgrade and select ⋮ &gt; Edit Config.Select Kubernetes Version.Click Save. Upgrade RKE/K3s​ RKE/K3s upgrade cloud provider via the Rancher UI, as follows: Click ☰ &gt; RKE/K3s Cluster &gt; Apps &gt; Installed Apps.Find the cloud provider chart and select ⋮ &gt; Edit/Upgrade.Select Version. Click Next &gt; Update. Load Balancer Support​ Once you've deployed the Harvester cloud provider, you can leverage the Kubernetes LoadBalancer service to expose a microservice within the guest cluster to the external world. Creating a Kubernetes LoadBalancer service assigns a dedicated Harvester load balancer to the service, and you can make adjustments through the Add-on Config within the Rancher UI. IPAM​ Harvester's built-in load balancer offers both DHCP and Pool modes, and you can configure it by adding the annotation cloudprovider.harvesterhci.io/ipam: $mode to its corresponding service. Starting from Harvester cloud provider &gt;= v0.2.0, it also introduces a unique Share IP mode. A service shares its load balancer IP with other services in this mode. DCHP: A DHCP server is required. The Harvester load balancer will request an IP address from the DHCP server. Pool: An IP pool must be configured first. The Harvester load balancer controller will allocate an IP for the load balancer service following the IP pool selection policy. Share IP: When creating a new load balancer service, you can re-utilize an existing load balancer service IP. The new service is referred to as a secondary service, while the currently chosen service is the primary one. To specify the primary service in the secondary service, you can add the annotation cloudprovider.harvesterhci.io/primary-service: $primary-service-name. However, there are two known limitations: Services that share the same IP address can't use the same port.Secondary services cannot share their IP with additional services. note Modifying the IPAM mode isn't allowed. You must create a new service if you intend to change the IPAM mode. Health checks​ Beginning with Harvester cloud provider v0.2.0, additional health checks of the LoadBalancer service within the guest Kubernetes cluster are no longer necessary. Instead, you can configure liveness and readiness probes for your workloads. Consequently, any unavailable pods will be automatically removed from the load balancer endpoints to achieve the same desired outcome.","keywords":"Harvester harvester RKE rke RKE2 rke2 Harvester Cloud Provider","version":"v1.4 (dev)"},{"title":"Harvester CSI Driver","type":0,"sectionRef":"#","url":"/v1.4/rancher/csi-driver","content":"Harvester CSI Driver The Harvester Container Storage Interface (CSI) Driver provides a standard CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying​ Prerequisites​ The Kubernetes cluster is built on top of Harvester virtual machines.The Harvester virtual machines that run as guest Kubernetes nodes are in the same namespace.The Harvester virtual machine guests' hostnames match their corresponding Harvester virtual machine names. Guest cluster Harvester VMs can't have different hostnames than their Harvester VM names when using the Harvester CSI driver. We hopeto remove this limitation in a future release of Harvester. note Currently, the Harvester CSI driver only supports single-node read-write(RWO) volumes. Please follow the issue #1992 for future multi-node read-only(ROX) and read-write(RWX) support. Deploying with Harvester RKE1 node driver​ Select the Harvester(Out-of-tree) option. Install Harvester CSI Driver from the Rancher marketplace. Deploying with Harvester RKE2 node driver​ When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed automatically when Harvester cloud provider is selected. Install CSI driver manually in the RKE2 cluster​ If you prefer to install the Harvester CSI driver without enabling the Harvester cloud provider, you can refer to the following steps: Prerequisites of manual install​ Ensure that you have the following prerequisites in place: You have kubectl and jq installed on your system.You have the kubeconfig file for your bare-metal Harvester cluster. You can find the kubeconfig file from one of the Harvester management nodes in the /etc/rancher/rke2/rke2.yaml path. export KUBECONFIG=/path/to/your/harvester-kubeconfig Perform the following steps to deploy the Harvester CSI driver manually: Deploy Harvester CSI driver​ Generate the cloud-config. You can generate the cloud-config file using the generate_addon_csi.sh script. It is available on the harvester/harvester-csi-driver repo. &lt;serviceaccount name&gt; usually corresponds to your guest cluster name, and &lt;namespace&gt; should match the machine pool's namespace. ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; RKE2 The generated output will be similar to the following one: ########## cloud-config ############ apiVersion: v1 clusters: - cluster: &lt;token&gt; server: https://&lt;YOUR HOST HARVESTER VIP&gt;:6443 name: default contexts: - context: cluster: default namespace: default user: rke2-guest-01-default-default name: rke2-guest-01-default-default current-context: rke2-guest-01-default-default kind: Config preferences: {} users: - name: rke2-guest-01-default-default user: token: &lt;token&gt; ########## cloud-init user data ############ write_files: - encoding: b64 content: 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 owner: root:root path: /var/lib/rancher/rke2/etc/config-files/cloud-provider-config permissions: '0644' Copy and paste the cloud-init user data content to Machine Pools &gt; Show Advanced &gt; User Data. The cloud-provider-config file will be created after you apply the cloud-init user data above. You can find it on the guest Kubernetes nodes at the path /var/lib/rancher/rke2/etc/config-files/cloud-provider-config. Configure the Cloud Provider either to Default - RKE2 Embedded or External. Select Create to create your RKE2 cluster. Once the RKE2 cluster is ready, install the Harvester CSI Driver chart from the Rancher marketplace. You do not need to change the cloud-config path by default. note If you prefer not to install the Harvester CSI driver using Rancher (Apps &gt; Charts), you can use Helm instead. The Harvester CSI driver is packaged as a Helm chart. For more information, see https://charts.harvesterhci.io. By following the above steps, you should be able to see those CSI driver pods are up and running on the kube-system namespace, and you can verify it by provisioning a new PVC using the default StorageClass harvester on your RKE2 cluster. Deploying with Harvester K3s node driver​ You can follow the Deploy Harvester CSI Driver steps described in the RKE2 section. The only difference is in generating the cloud-init config where you need to specify the provider type as k3s: ./generate_addon_csi.sh &lt;serviceaccount name&gt; &lt;namespace&gt; k3s Customize the Default StorageClass​ The Harvester CSI driver provides the interface for defining the default StorageClass. If the default StorageClass in unspecified, the Harvester CSI driver uses the default StorageClass of the host Harvester cluster. You can use the parameter host-storage-class to customize the default StorageClass. Create a StorageClass for the host Harvester cluster. Example: Deploy the CSI driver with the parameter host-storage-class. Example: Verify that the Harvester CSI driver is ready. On the PersistentVolumeClaims screen, create a PVC. Select Use a Storage Class to provision a new Persistent Volume and specify the StorageClass you created. Example: Once the PVC is created, note the name of the provisioned volume and verify that the status is Bound. Example: On the Volumes screen, verify that the volume was provisioned using the StorageClass that you created. Example: Passthrough Custom StorageClass​ Beginning with Harvester CSI driver v0.1.15, it's possible to create a PersistentVolumeClaim (PVC) using a different Harvester StorageClass on the guest Kubernetes Cluster. note Harvester CSI driver v0.1.15 is supported out of the box starting with the following RKE2 versions. For RKE1, manual installation of the CSI driver chart is required: v1.23.16+rke2r1 and laterv1.24.10+rke2r1 and laterv1.25.6+rke2r1 and laterv1.26.1+rke2r1 and laterv1.27.1+rke2r1 and later Prerequisites​ Add the following prerequisites to your Harvester cluster to ensure the Harvester CSI driver displays error messages correctly. Proper RBAC settings are essential for error message visibility, especially when creating a PVC with a non-existent StorageClass, as shown in the image below: Follow these steps to set up RBAC for error message visibility: Create a new clusterrole named harvesterhci.io:csi-driver using the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: apiserver app.kubernetes.io/name: harvester app.kubernetes.io/part-of: harvester name: harvesterhci.io:csi-driver rules: - apiGroups: - storage.k8s.io resources: - storageclasses verbs: - get - list - watch Create a new clusterrolebinding associated with the clusterrole above with the relevant serviceaccount using the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: &lt;namespace&gt;-&lt;serviceaccount name&gt; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: harvesterhci.io:csi-driver subjects: - kind: ServiceAccount name: &lt;serviceaccount name&gt; namespace: &lt;namespace&gt; Make sure the serviceaccount name and namespace match your cloud provider settings. Perform the following steps to retrieve these details. Find the rolebinding associated with your cloud provider: $ kubectl get rolebinding -A |grep harvesterhci.io:cloudprovider default default-rke2-guest-01 ClusterRole/harvesterhci.io:cloudprovider 7d1h Extract the subjects information from this rolebinding: $ kubectl get rolebinding default-rke2-guest-01 -n default -o yaml |yq -e '.subjects' Identify the ServiceAccount information: - kind: ServiceAccount name: rke2-guest-01 namespace: default Deploying​ Now you can create a new StorageClass that you intend to use in your guest Kubernetes cluster. For administrators, you can create a desired StorageClass (e.g., named replica-2) in your bare-metal Harvester cluster. Then, on the guest Kubernetes cluster, create a new StorageClass associated with the StorageClass named replica-2 from the Harvester Cluster: note When choosing a Provisioner, select Harvester (CSI). The Host StorageClass parameter should match the StorageClass name created on the Harvester Cluster.For guest Kubernetes owners, you may request that the Harvester cluster administrator create a new StorageClass.If you leave the Host StorageClass field empty, the default StorageClass of the Harvester cluster will be used. You can now create a PVC based on this new StorageClass, which utilizes the Host StorageClass to provision volumes on the bare-metal Harvester cluster.","keywords":"Harvester harvester Rancher Integration","version":"v1.4 (dev)"},{"title":"Creating an K3s Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.4/rancher/node/k3s-cluster","content":"Creating an K3s Kubernetes Cluster You can now provision K3s Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. note Harvester K3s node driver is in Tech Preview.VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For the port requirements of the guest clusters deployed within Harvester, please refer to the port requirements for guest clusters. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create K3s Kubernetes cluster​ You can create a K3s Kubernetes cluster from the Cluster Management page via the K3s node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).Click Create. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules. This provides high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled according to the affinity rules. Using Harvester K3s node driver in air gapped environment​ K3s provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with the required packages (e.g., iptables, qemu-guest-agent).Option 2. Go to Show Advanced &gt; User Data to allow VMs to install the required packages via an HTTP(S) proxy. Example of user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.4 (dev)"},{"title":"Harvester Node Driver","type":0,"sectionRef":"#","url":"/v1.4/rancher/node/node-driver","content":"Harvester Node Driver The Harvester node driver, similar to the Docker Machine driver, is used to provision VMs in the Harvester cluster, and Rancher uses it to launch and manage Kubernetes clusters. One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected. Additionally, the Harvester node driver is integrated with the Harvester cloud provider by default, providing built-in load balancer support as well as storage passthrough from the bare-metal cluster to the guest Kubernetes clusters to gain native storage performance. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. note The Harvester node driver only supports cloud images. This is because ISO images usually require additional setup that interferes with a clean deployment (without requiring user intervention), and they are not typically used in cloud environments. Harvester node driver​ Starting from Rancher v2.6.3, the Harvester node driver is enabled by default. You can go to the Cluster Management &gt; Drivers &gt; Node Drivers page to check the Harvester node driver status. When the Harvester node driver is enabled, you can create Kubernetes clusters on top of the Harvester cluster and manage them from Rancher. note Refer to the Rancher downstream cluster support matrix for its supported RKE2 versions and guest OS versions. Changes made to the node driver configuration is not persisted. Any modifications applied will be reset upon restarting the Rancher container. Starting with Harvester node driver v0.6.3, the automatic injection of the qemu-guest-agent has been removed from the backend. If the image you are using does not contain the qemu-guest-agent package, you can still install it via the userdata config. Otherwise, the cluster will not be provisioned successfully. #cloud-config package_update: true packages: - qemu-guest-agent runcmd: - - systemctl - enable - '--now' - qemu-guest-agent.service RKE1 Kubernetes cluster​ Click to learn how to create RKE1 Kubernetes Clusters. RKE2 Kubernetes cluster​ Click to learn how to create RKE2 Kubernetes Clusters. K3s Kubernetes cluster​ Click to learn how to create k3s Kubernetes Clusters. Topology spread constraints​ Available as of v1.0.3 Within your guest Kubernetes cluster, you can use topology spread constraints to manage how workloads are distributed across nodes, accounting for factors such as failure domains like regions and zones. This helps achieve high availability and efficient resource utilization of the Harvester cluster resources. For RKE2 versions before v1.25.x, the minimum required versions to support the topology label sync feature are as follows: Minimum Required RKE2 Version&gt;= v1.24.3+rke2r1 &gt;= v1.23.9+rke2r1 &gt;= v1.22.12+rke2r1 Furthermore, for custom installation, the Harvester cloud provider version should be &gt;= v0.1.4. Sync topology labels to the guest cluster node​ During the cluster installation, the Harvester node driver will automatically help synchronize topology labels from VM nodes to guest cluster nodes. Currently, only region and zone topology labels are supported. Configure topology labels on the Harvester nodes on the Hosts &gt; Edit Config &gt; Labels page. For example, add the topology labels as follows: topology.kubernetes.io/region: us-east-1 topology.kubernetes.io/zone: us-east-1a Create a downstream RKE2 cluster using the Harvester node driver with Harvester cloud provider enabled. We recommend adding the node affinity rules, which prevents nodes from drifting to other zones after VM rebuilding. After the cluster is ready, confirm that those topology labels are successfully synchronized to the nodes on the guest Kubernetes cluster. Now deploy workloads on your guest Kubernetes cluster, and you should be able to manage them using the topology spread constraints. note For Harvester cloud provider &gt;= v0.2.0, topology labels on the Harvester node will be automatically resynchronized when a VM (corresponding to the guest node) undergoes migration or update. For Harvester cloud provider &lt; v0.2.0, label synchronization will only occur during the initialization of guest nodes. To prevent nodes from drifting to different regions or zones, we recommend adding node affinity rules during cluster provisioning. This will allow you to schedule VMs in the same zone even after rebuilding.","keywords":"Harvester harvester Rancher rancher Harvester Node Driver","version":"v1.4 (dev)"},{"title":"Creating an RKE1 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.4/rancher/node/rke1-cluster","content":"Creating an RKE1 Kubernetes Cluster You can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. RKE1 and RKE2 have several slight behavioral differences. Refer to the differences between RKE1 and RKE2 to get some high-level insights. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For port requirements of guest clusters deployed within Harvester, please refer to the port requirements for guest clusters. When you create a Kubernetes cluster hosted by the Harvester infrastructure, node templates are used to provision the cluster nodes. These templates use Docker Machine configuration options to define an operating system image and settings/parameters for the node. Node templates can use cloud credentials to access the credentials information required to provision nodes in the infrastructure providers. The same cloud credentials can be used by multiple node templates. By using cloud credentials, you do not have to re-enter access keys for the same cloud provider. Cloud credentials are stored as Kubernetes secrets. You can create cloud credentials in two contexts: During the creation of a node template for a cluster.In the User Settings page All cloud credentials are bound to your user profile and cannot be shared with other users. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential name.Select &quot;Imported Harvester Cluster&quot;.Click Create. Create node templates​ You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials.Configure Instance Options: Configure the CPU, memory, and diskSelect an OS image that is compatible with the cloud-init config.Select a network that the node driver is able to connect to; currently, only VLAN is supported.Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu. (Optional) Configure Advanced Options if you want to customise the cloud-init config of the VMs:Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the node template during the cluster creation, click Add Node Template or edit your existing node template via RKE1 Configuration &gt; Node Templates: Check the Advanced Options tab and click Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Click Create to save the node template. After the cluster is installed, you can check whether its machine nodes are scheduled accordingly to the affinity rules. Create an RKE1 Kubernetes cluster​ Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE1.Select Harvester node driver.Enter Cluster Name (required).Enter Name Prefix (required).Enter Template (required).Select etcd and Control Plane (required).On the Cluster Options configure Cloud Provider to Harvester if you want to use the Harvester Cloud Provider and CSI Diver.Click Create. Using Harvester RKE1 node driver in air-gapped environments​ RKE1 provisioning relies on the qemu-guest-agent to get the IP of the virtual machine, and docker to set up the RKE cluster. However, It may not be feasible to install qemu-guest-agent and docker in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with both qemu-guest-agent and docker.Option 2. Configure the cloud-init user data to enable the VMs to install qemu-guest-agent and docker via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 write_files: - path: /etc/environment content: | HTTP_PROXY=&quot;http://192.168.0.1:3128&quot; HTTPS_PROXY=&quot;http://192.168.0.1:3128&quot; append: true ","keywords":"","version":"v1.4 (dev)"},{"title":"Creating an RKE2 Kubernetes Cluster","type":0,"sectionRef":"#","url":"/v1.4/rancher/node/rke2-cluster","content":"Creating an RKE2 Kubernetes Cluster You can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher using the built-in Harvester node driver. note VLAN network is required for Harvester node driver.Harvester node driver only supports cloud images.For the port requirements of the guest clusters deployed within Harvester, please refer to the doc here.For RKE2 with Harvester cloud provider support matrix, please refer to the website here. Backward Compatibility Notice​ note Please note a known backward compatibility issue if you're using the Harvester cloud provider version v0.2.2 or higher. If your Harvester version is below v1.2.0 and you intend to use newer RKE2 versions (i.e., &gt;= v1.26.6+rke2r1, v1.25.11+rke2r1, v1.24.15+rke2r1), it is essential to upgrade your Harvester cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester cloud provider. For a detailed support matrix, please refer to the Harvester CCM &amp; CSI Driver with RKE2 Releases section of the official website. Create your cloud credentials​ Click ☰ &gt; Cluster Management.Click Cloud Credentials.Click Create.Click Harvester.Enter your cloud credential nameSelect &quot;Imported Harvester Cluster&quot;.Click Create. Create RKE2 kubernetes cluster​ Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu.Click Create button.Toggle Switch to RKE2/K3s.Select Harvester node driver.Select a Cloud Credential.Enter Cluster Name (required).Enter Namespace (required).Enter Image (required).Enter Network Name (required).Enter SSH User (required).(optional) Configure the Show Advanced &gt; User Data to install the required packages of VM. #cloud-config packages: - iptables note Calico and Canal networks require the iptables or xtables-nft package to be installed on the node, for more details, please refer to the RKE2 known issues. Click Create. note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration.Only imported Harvester clusters are supported by the Harvester node driver. Add node affinity​ Available as of v1.0.3 + Rancher v2.6.7 The Harvester node driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization. Node affinity can be added to the machine pools during the cluster creation: Click the Show Advanced button and click the Add Node SelectorSet priority to Required if you wish the scheduler to schedule the machines only when the rules are met.Click Add Rule to specify the node affinity rules, e.g., for the topology spread constraints use case, you can add the region and zone labels as follows: key: topology.kubernetes.io/region operator: in list values: us-east-1 --- key: topology.kubernetes.io/zone operator: in list values: us-east-1a Add workload affinity​ Available as of v1.2.0 + Rancher v2.7.6 The workload affinity rules allow you to constrain which nodes your machines can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels. Workload affinity rules can be added to the machine pools during the cluster creation: Select Show Advanced and choose Add Workload Selector.Select Type, Affinity or Anti-Affinity.Select Priority. Prefered means it's an optional rule, and Required means a mandatory rule.Select the namespaces for the target workloads.Select Add Rule to specify the workload affinity rules.Set Topology Key to specify the label key that divides Harvester hosts into different topologies. See the Kubernetes Pod Affinity and Anti-Affinity Documentation for more details. Update RKE2 Kubernetes cluster​ The fields highlighted below of the RKE2 machine pool represent the Harvester VM configurations. Any modifications to these fields will trigger node reprovisioning. Using Harvester RKE2 node driver in air gapped environment​ RKE2 provisioning relies on the qemu-guest-agent package to get the IP of the virtual machine. Calico and Canal require the iptables or xtables-nft package to be installed on the node. However, it may not be feasible to install packages in an air gapped environment. You can address the installation constraints with the following options: Option 1. Use a VM image preconfigured with required packages (e.g., iptables, qemu-guest-agent).Option 2. Go to Show Advanced &gt; User Data to allow VMs to install the required packages via an HTTP(S) proxy. Example user data in Harvester node template: #cloud-config apt: http_proxy: http://192.168.0.1:3128 https_proxy: http://192.168.0.1:3128 ","keywords":"","version":"v1.4 (dev)"},{"title":"Rancher Integration","type":0,"sectionRef":"#","url":"/v1.4/rancher/rancher-integration","content":"Rancher Integration Rancher is an open-source multi-cluster management platform. Starting with Rancher v2.6.1, Rancher has integrated Harvester by default to centrally manage VMs and containers. Users can import and manage multiple Harvester clusters using the Rancher Virtualization Management feature. Leveraging the Rancher's authentication feature and RBAC control for multi-tenancy support. For a comprehensive overview of the support matrix, please refer to the Harvester &amp; Rancher Support Matrix. For the network requirements, please refer to the doc here. Deploying Rancher server​ To use Rancher with Harvester, please install Rancher on a separate server. If you want to try out the integration features, you can create a VM in Harvester and install the Rancher server by following the Helm CLI quick start. For production setup, please use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix MetalOutscale (uses Terraform) If you prefer, the following guide will take you through the same process in individual steps. Use this if you want to run Rancher with a different provider, on prem, or if you want to see how easy it is. Manual Install caution Do not install Rancher with Docker in production. Otherwise, your environment may be damaged, and your cluster may not be able to be recovered. Installing Rancher in Docker should only be used for quick evaluation and testing purposes. Virtualization management​ With Rancher's virtualization management feature, you can import and manage your Harvester cluster. By clicking one of the imported clusters, you can easily access and manage a range of Harvester cluster resources, including hosts, VMs, images, volumes, and more. Additionally, the virtualization management feature leverages Rancher's existing capabilities, such as authentication with various auth providers and multi-tenancy support. For in-depth insights, please refer to the virtualization management page. Creating Kubernetes clusters using the Harvester node driver​ You can launch a Kubernetes cluster from Rancher using the Harvester node driver. When Rancher deploys Kubernetes onto these nodes, you can choose between Rancher Kubernetes Engine (RKE) or RKE2 distributions. One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, Rancher can automatically create another node to join the cluster to ensure that the count of the node pool is as expected. Starting from Rancher version v2.6.1, the Harvester node driver is included by default. You can refer to the node-driver page for more details. Harvester baremetal container workload support (experimental)​ Available as of Harvester v1.2.0 + Rancher v2.7.6 Starting with Rancher v2.7.6, Harvester introduces a new feature that enables you to deploy and manage container workloads directly to the underlying Harvester cluster. With this feature, you can seamlessly combine the power of virtual machines with the flexibility of containerization, allowing for a more versatile and efficient infrastructure setup. This guide will walk you through enabling and using this experimental feature, highlighting its capabilities and best practices. To enable this new feature flag, follow these steps: Click the hamburger menu and choose the Global Settings tab.Click Feature Flags and locate the new feature flag harvester-baremetal-container-workload.Click the drop-down menu and select Activate to enable this feature.If the feature state changes to Active, the feature is successfully enabled. Key Features​ Unified Dashboard View:Once you've enabled the feature, you can explore the dashboard view of the Harvester cluster, just like you would with other standard Kubernetes clusters. This unified experience simplifies the management and monitoring of both your virtual machines and container workloads from a single, user-friendly interface. Deploy Custom Workloads:This feature lets you deploy custom container workloads directly to the bare-metal Harvester cluster. While this functionality is experimental, it introduces exciting possibilities for optimizing your infrastructure. However, we recommend deploying container and VM workloads in separate namespaces to ensure clarity and separation. note Critical system components such as monitoring, logging, Rancher, KubeVirt, and Longhorn are all managed by the Harvester cluster itself. You can't upgrade or modify these components. Therefore, exercise caution and avoid making changes to these critical system components.It is essential not to deploy any workloads to the system namespaces cattle-system, harvester-system, or longhorn-system. Keeping your workloads in separate namespaces is crucial to maintaining clarity and preserving the integrity of the system components.For best practices, we recommend deploying container and VM workloads in separate namespaces. note With this feature enabled, your Harvester cluster does not appear on the Continuous Delivery page in the Rancher UI. Please check the issue #4482 for further updates.","keywords":"Harvester harvester Rancher rancher Rancher Integration","version":"v1.4 (dev)"},{"title":"Resource Quotas","type":0,"sectionRef":"#","url":"/v1.4/rancher/resource-quota","content":"Resource Quotas ResourceQuota is used to limit the usage of resources within a namespace. It helps administrators control and restrict the allocation of cluster resources to ensure fairness and controlled resource distribution among namespaces. In Harvester, ResourceQuota can define usage limits for the following resources: CPU: Limits compute resource usage, including CPU cores and CPU time.Memory: Limits the usage of memory resources in bytes or other recognizable memory units. Set ResourceQuota via Rancher​ In the Rancher UI, administrators can configure resource quotas for namespaces through the following steps: Click the hamburger menu and choose the Virtualization Management tab.Choose one of the clusters and go to Projects/Namespaces &gt; Create Project.Specify the desired project Name. Next, go to the Resource Quotas tab and select the Add Resource option. Within the Resource Type field, select either CPU Limit or Memory Limit and define the Project Limit and Namespace Default Limit values. You can configure the Namespace limits as follows: Find the newly created project, and select Create Namespace.Specify the desired namespace Name, and adjust the limits.Complete the process by selecting Create. Overhead memory of virtual machine​ Upon creating a virtual machine (VM), the VM controller seamlessly incorporates overhead resources into the VM's configuration. These additional resources intend to guarantee the consistent and uninterrupted functioning of the VM. It's important to note that configuring memory limits requires a higher memory reservation due to the inclusion of these overhead resources. For example, consider the creation of a new VM with the following configuration: CPU: 8 coresMemory: 16Gi note The operating system, either Linux or Windows, does not affect overhead calculations. Memory Overhead is calculated in the following sections: Memory PageTables Overhead: This accounts for one bit for every 512b RAM size. For instance, a memory of 16Gi requires an overhead of 32Mi.VM Fixed Overhead: This consists of several components: VirtLauncherMonitorOverhead: 25Mi (the ps RSS for virt-launcher-monitor)VirtLauncherOverhead: 75Mi (the ps RSS for the virt-launcher process)VirtlogdOverhead: 17Mi (the ps RSS for virtlogd)LibvirtdOverhead: 33Mi (the ps RSS for libvirtd)QemuOverhead : 30Mi (the ps RSS for qemu, minus the RAM of its (stressed) guest, minus the virtual page table) 8Mi per CPU (vCPU) Overhead: Additionally, 8Mi of overhead per vCPU is added, along with a fixed 8Mi overhead for IOThread.Extra Added Overhead: This encompasses various factors like video RAM overhead and architecture overhead. Refer to Additional Overhead for further details. This calculation demonstrates that the VM instance necessitates an additional memory overhead of approximately 276Mi. For more information, see Memory Overhead. For more information on how the memory overhead is calculated in Kubevirt, refer to kubevirt/pkg/virt-controller/services/template.go. Automatic adjustment of ResourceQuota during migration​ When the allocated resource quota controlled by the ResourceQuota object reaches its limit, migrating a VM becomes unfeasible. The migration process automatically creates a new pod mirroring the resource requirements of the source VM. If these pod creation prerequisites surpass the defined quota, the migration operation cannot proceed. Available as of v1.2.0 In Harvester, the ResourceQuota values will dynamically expand ahead of migration to accommodate the resource needs of the target virtual machine. After migration, the ResourceQuotas will be reinstated to their prior configurations. Please be aware of the following constrains of the automatic resizing of ResourceQuota: ResourceQuota cannot be changed during VM migration.When raising the ResourceQuota value, if you create, start, or restore other VMs, Harvester will verify if the resources are sufficient based on the original ResourceQuota. If the conditions are not met, the system will alert that the migration process is not feasible.After expanding ResourceQuota, potential resource contention may occur between non-VM pods and VM pods, leading to migration failures. Therefore, deploying custom container workloads and VMs to the same namespace is not recommended.Due to the concurrent limitation of the webhook validator, the VM controller will execute a secondary validation to confirm resource sufficiency. If the resource is insufficient, it will auto config the VM's RunStrategy to Halted, and a new annotation harvesterhci.io/insufficient-resource-quota will be added to the VM object, informing you that the VM was shut down due to insufficient resources.","keywords":"Harvester harvester Rancher rancher Resource Quota","version":"v1.4 (dev)"},{"title":"Virtualization Management","type":0,"sectionRef":"#","url":"/v1.4/rancher/virtualization-management","content":"Virtualization Management With Rancher's virtualization management capabilities, you can import and manage multiple Harvester clusters. It provides a solution that unifies virtualization and container management from a single pane of glass. Additionally, Harvester leverages Rancher's existing capabilities, such as authentication and RBAC control, to provide full multi-tenancy support. Importing Harvester cluster​ Please refer to the Harvester &amp; Rancher Support Matrix to find a desired Rancher version. You can use one of the following guides to deploy and provision Rancher and a Kubernetes cluster with the provider of your choice: AWS (uses Terraform)AWS Marketplace (uses Amazon EKS)Azure (uses Terraform)DigitalOcean (uses Terraform)GCP (uses Terraform)Hetzner Cloud (uses Terraform)VagrantEquinix MetalOutscale (uses Terraform)Manual Install Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server.Specify the Cluster Name and click Create. You will then see the registration guide; please open the dashboard of the target Harvester cluster and follow the guide accordingly.Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly.From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy​ In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication, users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions: Define user authorization outside the scope of any particular cluster. Cluster and Project Roles: Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC. Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings.A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example​ The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users &amp; Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project.A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab.Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save.Open an incognito browser and log in as project-owner.After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster and project to which you have been assigned.Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed.Create a VM with one of the images that you have uploaded.Log in with another user, e.g., project-readonly, and this user will only have the read permission of the assigned project. note The harvester-public namespace is a predefined namespace accessible to all users assigned to this cluster. Delete Imported Harvester Cluster​ Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management &gt; Harvester Clusters. Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. You will also need to reset the cluster-registration-url setting on the associated Harvester cluster to clean up the Rancher cluster agent. caution Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","keywords":"Harvester Rancher","version":"v1.4 (dev)"},{"title":"Harvester Terraform Provider","type":0,"sectionRef":"#","url":"/v1.4/terraform/terraform-provider","content":"Harvester Terraform Provider Support Matrix​ Harvester Version\tSupported Terraform Provider Harvester\tSupported Terraformer Harvesterv1.2.0\tv0.6.3\tv1.1.1-harvester v1.1.2\tv0.6.3\tv1.1.1-harvester v1.1.1\tv0.6.3\tv1.1.1-harvester v1.1.0\tv0.6.3\tv1.1.1-harvester Requirements​ Terraform &gt;= 0.13.xGo 1.18 to build the provider plugin Install The Provider​ copy and paste this code into your Terraform configuration. Then, run terraform init to initialize it. terraform { required_providers { harvester = { source = &quot;harvester/harvester&quot; version = &quot;&lt;replace to the latest release version&gt;&quot; } } } provider &quot;harvester&quot; { # Configuration options } Using the provider​ More details about the provider-specific configurations can be found in the docs. Github Repo: https://github.com/harvester/terraform-provider-harvester","keywords":"","version":"v1.4 (dev)"},{"title":"Installation","type":0,"sectionRef":"#","url":"/v1.4/troubleshooting/index","content":"Installation The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS)​ Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancherPassword: rancher Meeting hardware requirements​ Check that your hardware meets the minimum requirements to complete installation. Stuck in Loading images. This may take a few minutes...​ Because the system doesn't have a default route, your installer may become &quot;stuck&quot; in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev mgmt-br proto dhcp &lt;-- Does a default route exist? 10.10.0.0/24 dev mgmt-br proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. For more information, see DHCP Server Configuration. Modifying cluster token on agent nodes​ When an agent node fails to join the cluster, it can be related to the cluster token not being identical to the server node token. In order to confirm the issue, connect to your agent node (i.e. with SSH and check the rancherd service log with the following command: $ sudo journalctl -b -u rancherd If the cluster token setup in the agent node is not matching the server node token, you will find several entries of the following message: msg=&quot;Bootstrapping Rancher (v2.7.5/v1.25.9+rke2r1)&quot; msg=&quot;failed to bootstrap system, will retry: generating plan: response 502: 502 Bad Gateway getting cacerts: &lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt;\\r\\n&lt;body&gt;\\r\\n&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;/h1&gt;&lt;/center&gt;\\r\\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\\r\\n&lt;/body&gt;\\r\\n&lt;/html&gt;\\r\\n&quot; Note that the Rancher version and IP address depend on your environment and might differ from the message above. To fix the issue, you need to update the token value in the rancherd configuration file /etc/rancher/rancherd/config.yaml. For example, if the cluster token setup in the server node is ThisIsTheCorrectOne, you will update the token value as follow: token: 'ThisIsTheCorrectOne' To ensure the change is persistent across reboots, update the token value of the OS configuration file /oem/90_custom.yaml: name: Harvester Configuration stages: ... initramfs: - commands: - rm -f /etc/sysconfig/network/ifroute-mgmt-br files: - path: /etc/rancher/rancherd/config.yaml permissions: 384 owner: 0 group: 0 content: | server: https://$cluster-vip:443 role: agent token: &quot;ThisIsTheCorrectOne&quot; kubernetesVersion: v1.25.9+rke2r1 rancherVersion: v2.7.5 rancherInstallerImage: rancher/system-agent-installer-rancher:v2.7.5 labels: - harvesterhci.io/managed=true extraConfig: disable: - rke2-snapshot-controller - rke2-snapshot-controller-crd - rke2-snapshot-validation-webhook encoding: &quot;&quot; ownerstring: &quot;&quot; note To see what is the current cluster token value, log in your server node (i.e. with SSH) and look in the file /etc/rancher/rancherd/config.yaml. For example, you can run the following command to only display the token's value: $ sudo yq eval .token /etc/rancher/rancherd/config.yaml Collecting troubleshooting information​ Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. System information and logs. Available as of v1.0.2 Please follow the guide in Logging into the Harvester Installer (a live OS) to log in. And run the command to generate a tarball that contains troubleshooting information: supportconfig -k -c The command output messages contain the generated tarball path. For example the path is /var/loq/scc_aaa_220520_1021 804d65d-c9ba-4c54-b12d-859631f892c5.txz in the following example: note A failure PXE Boot installation automatically generates a tarball if the install.debug field is set to true in the Harvester configuration file.","keywords":"","version":"v1.4 (dev)"},{"title":"Harvester","type":0,"sectionRef":"#","url":"/v1.4/troubleshooting/harvester","content":"Harvester Fail to Deploy a Multi-node Cluster Due to Incorrect HTTP Proxy Setting​ ISO Installation Without a Harvester Configuration File​ Configure HTTP Proxy During Harvester Installation​ In some environments, you configure http-proxy of OS Environment during Harvester installation. Configure HTTP Proxy After First Node is Ready​ After the first node is installed successfully, you login into the Harvester GUI to configure http-proxy of Harvester System Settings. Then you continue to add more nodes to the cluster. One Node Becomes Unavailable​ The issue you may encounter: The first node is installed successfully. The second node is installed successfully. The third node is installed successfully. Then the second node changes to Unavialable state and cannot recover automatically. Solution​ When the nodes in the cluster do not use the HTTP Proxy to communicate with each other, after the first node is installed successfully, you need to configure http-proxy.noProxy against the CIDR used by those nodes. For example, your cluster assigns IPs from CIDR 172.26.50.128/27 to nodes via DHCP/static setting, please add this CIDR to noProxy. After setting this, you can continue to add new nodes to the cluster. For more details, please refer to Harvester issue 3091. ISO Installation With a Harvester Configuration File​ When a Harvester configuration file is used in ISO installation, please configure proper http-proxy in Harvester System Settings. PXE Boot Installation​ When PXE Boot Installation is adopted, please configure proper http-proxy in OS Environment and Harvester System Settings. Generate a Support Bundle​ Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Manually Download and Retain a Support Bundle File​ By default, a support bundle file is automatically generated, downloaded, and deleted after you click Create on the Harvester UI. However, you may want to retain a file for various reasons, including the following: You are unable to download the file because of network connectivity errors and other issues. You must use a previously generated file to troubleshoot issues (because generating a support bundle file takes time). You want to view information that only exists in a previously generated file. Even if the file remains in the cluster, the Harvester UI does not provide a download link. Use the following workaround to generate, manually download, and retain a support bundle file: Generate the File and Prevent Automatic Downloading​ On the Harvester UI, click Generate Support Bundle. When the progress indicator reaches 20% to 80%, close the browser tab to prevent automatic downloading of the generated file. Retrieve a list of all support bundles in all namespaces using kubectl. Example: $ kubectl get supportbundle -A NAMESPACE NAME ISSUE_URL DESCRIPTION AGE harvester-system bundle-htl5f sp1 3h43m Retrieve the details of all existing support bundles using the command kubectl get supportbundle -A -o yaml. Example: $ kubectl get supportbundle -A -oyaml apiVersion: v1 items: - apiVersion: harvesterhci.io/v1beta1 kind: SupportBundle metadata: creationTimestamp: &quot;2024-02-02T11:18:09Z&quot; generation: 5 name: bundle-htl5f // resource name namespace: harvester-system resourceVersion: &quot;1218311&quot; uid: a3776373-05fe-4584-8a9a-baac3fa91bbf spec: description: sp1 issueURL: &quot;&quot; status: conditions: - lastUpdateTime: &quot;2024-02-02T11:18:38Z&quot; status: &quot;True&quot; type: Initialized filename: supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-02T11-18-10Z.zip // support bundle file name filesize: 8868712 progress: 100 // 100 means successfully generated state: ready The file is ready for downloading when the value of progress is &quot;100&quot; and the value of state is &quot;ready&quot;. Download the File​ Create a download URL that includes the following information: VIP or DNS nameResource name of the fileParameter ?retain=true: If you do not include this parameter, resources related to the support bundle are automatically deleted after the file is successfully downloaded. Example: https://{vip/dns-name}/v1/harvester/supportbundles/bundle-htl5f/download?retain=true Download the file using either a command-line tool (for example, curl and wget) or a web browser. Example: curl -k https://{vip/dns-name}/v1/harvester/supportbundles/bundle-htl5f/download?retain=true -o sb2.zip Verify that resources related to the support bundle were not deleted. Example: $ kubectl get supportbundle -A NAMESPACE NAME ISSUE_URL DESCRIPTION AGE harvester-system bundle-htl5f sp1 3h43m (Optional) Delete the Related Resources​ Retained support bundle files consume memory and storage resources. Each file is backed by a supportbundle-manager-bundle* pod in the harvester-system namespace, and the generated ZIP file is stored in the /tmp folder of the pod's memory-based filesystem. Example: $ kubectl get pods -n harvester-system NAME READY STATUS RESTARTS AGE supportbundle-manager-bundle-dtl2k-69dcc69b59-w64vl 1/1 Running 0 8m18s You can delete the related resources using the following methods: Manual: Run the command kubectl delete supportbundle -n {namespace} {resource-name}. Deleting a support bundle object automatically deletes the pod that backs it. Example: $ kubectl delete supportbundle -n harvester-system bundle-htl5f supportbundle.harvesterhci.io &quot;bundle-htl5f&quot; deleted $ kubectl get supportbundle -A No resources found Automatic: Harvester deletes the related resources based on how the following settings are configured: support-bundle-expiration: Defines the time allowed for retaining a support bundle file support-bundle-timeout: Defines the time allowed for generating a support bundle file Manually Copy the Support Bundle File​ You can run the command kubectl cp to copy the generated file from the backing pod. Example: kubectl cp harvester-system/supportbundle-manager-bundle-dtl2k-69dcc69b59-w64vl:/tmp/support-bundle-kit/supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-02T11-18-10Z.zip bundle.zip Known Limitations​ Replacing the backing pod prevents the support bundle file from being downloaded. The support bundle file is stored in the /tmp folder of the pod's memory-based filesystem so it is removed when the pod is replaced during cluster and node rebooting, Kubernetes pod rescheduling, and other processes. After starting, the new pod regenerates the file but assigns a name that is different from the file name in the support bundle object. Example: A support bundle file is generated and retained. $ kubectl get supportbundle -A -oyaml apiVersion: v1 items: - apiVersion: harvesterhci.io/v1beta1 kind: SupportBundle metadata: creationTimestamp: &quot;2024-02-06T11:01:19Z&quot; generation: 5 name: bundle-yr2vq namespace: harvester-system resourceVersion: &quot;1583252&quot; uid: eb8538cf-886b-4791-a7b0-dbc34dcee524 spec: description: sp2 issueURL: &quot;&quot; status: conditions: - lastUpdateTime: &quot;2024-02-06T11:01:47Z&quot; status: &quot;True&quot; type: Initialized filename: supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-06T11-01-20Z.zip // file is ready to download filesize: 7832010 progress: 100 state: ready kind: List metadata: resourceVersion: &quot;&quot; The backing pod restarts. $ kubectl get pods -n harvester-system supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d -oyaml apiVersion: v1 kind: Pod metadata: ... labels: app: support-bundle-manager pod-template-hash: c5484fbdf rancher/supportbundle: bundle-yr2vq name: supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d namespace: harvester-system containerStatuses: - containerID: containerd://ea82b63875c18a2b5b36afea6a47a99a5efd26464f94d401cde1727d175ef740 ... name: manager ready: true restartCount: 1 started: true state: running: startedAt: &quot;2024-02-06T11:05:33Z&quot; // pod's latest starting timestamp, newer than the timestamp in support bundle's file name The backing pod regenerates the file after it starts. The name of the regenerated file is different from the file name recorded in the support bundle object. $ kubectl exec -i -t -n harvester-system supportbundle-manager-bundle-yr2vq-c5484fbdf-9pz8d -- ls /tmp/support-bundle-kit -alth total 2.2M drwxr-xr-x 3 root root 4.0K Feb 6 11:05 . -rw-r--r-- 1 root root 2.2M Feb 6 11:05 supportbundle_db25ccb6-b52a-4f9d-97dd-db2df2b004d4_2024-02-06T11-05-34Z.zip // different with above file name Attempts to download the regenerated file fail. The following download URL cannot be used to access the regenerated file. https://{vip/dns-name}/v1/harvester/supportbundles/bundle-yr2vq/download?retain=true. Retained support bundle files may affect system and node rebooting, node draining, and system upgrades. Retained support bundle files are backed by pods in the harvester-system namespace. These pods are replaced during system and node rebooting, node draining, and system upgrades, consuming CPU and memory resources. Moreover, the regenerated files are very similar in content to the retained files, which means that storage resources are also unnecessarily consumed. For more information, see Issue 3383. Access Embedded Rancher and Longhorn Dashboards​ Available as of v1.1.0 You can now access the embedded Rancher and Longhorn dashboards directly on the Support page, but you must first go to the Preferences page and check the Enable Extension developer features box under Advanced Features. note We only support using the embedded Rancher and Longhorn dashboards for debugging and validation purposes. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here. I can't access Harvester after I changed SSL/TLS enabled protocols and ciphers​ If you changedSSL/TLS enabled protocols and ciphers settingsand you no longer have access to Harvester GUI and API, it's highly possible that NGINX Ingress Controller has stopped working due to the misconfigured SSL/TLS protocols and ciphers. Follow these steps to reset the setting: Following FAQ to SSH into Harvester node and switch to root user. $ sudo -s Editing setting ssl-parameters manually using kubectl: # kubectl edit settings ssl-parameters Deleting the line value: ... so that NGINX Ingress Controller will use the default protocols and ciphers. apiVersion: harvesterhci.io/v1beta1 default: '{}' kind: Setting metadata: name: ssl-parameters ... value: '{&quot;protocols&quot;:&quot;TLS99&quot;,&quot;ciphers&quot;:&quot;WRONG_CIPHER&quot;}' # &lt;- Delete this line Save the change and you should see the following response after exit from the editor: setting.harvesterhci.io/ssl-parameters edited You can further check the logs of Pod rke2-ingress-nginx-controller to see if NGINX Ingress Controller is working correctly. Network interfaces are not showing up​ You may need help finding the correct interface with a 10G uplink since the interface is not showing up. The uplink doesn't show up when the ixgbe module fails to load because an unsupported SFP+ module type is detected. How to identify the issue with the unsupported SFP?​ Execute the command lspci | grep -i net to see the number of NIC ports connected to the motherboard. By running the command ip a, you can gather information about the detected interfaces. If the number of detected interfaces is less than the number of identified NIC ports, then it's likely that the problem arises from using an unsupported SFP+ module. Testing​ You can perform a simple test to verify whether the unsupported SFP+ is the cause. Follow these steps on a running node: Create the file /etc/modprobe.d/ixgbe.conf manually with the content: options ixgbe allow_unsupported_sfp=1 Then run following command: rmmod ixgbe &amp;&amp; modprobe ixgbe If the above steps are successful and the missing interface shows, we can confirm that the issue is an unsupported SFP+. However, the above test is not permanent and will be flushed out once rebooted. Solution​ Due to support issues, Intel restricts the types of SFPs used on their NICs. To make the above changes persistent, adding the following content to a config.yaml during installation is recommended. os: write_files: - content: | options ixgbe allow_unsupported_sfp=1 path: /etc/modprobe.d/ixgbe.conf - content: | name: &quot;reload ixgbe module&quot; stages: boot: - commands: - rmmod ixgbe &amp;&amp; modprobe ixgbe path: /oem/99_ixgbe.yaml ","keywords":"","version":"v1.4 (dev)"},{"title":"Upgrading Harvester","type":0,"sectionRef":"#","url":"/v1.4/upgrade/index","content":"Upgrading Harvester Upgrade support matrix​ The following table shows the upgrade path of all supported versions. Upgrade from version\tSupported new version(s)v1.1.2/v1.2.0\tv1.2.1 v1.1.1/v1.1.2\tv1.1.3 v1.1.0/v1.1.1\tv1.1.2 Rancher upgrade​ If you are using Rancher to manage your Harvester cluster, we recommend upgrading your Rancher server first. For more information, please refer to the Rancher upgrade guide. For the Harvester &amp; Rancher support matrix, please visit our website here. note Upgrading Rancher will not automatically upgrade your Harvester cluster. You still need to upgrade your Harvester cluster after upgrading Rancher.Upgrading Rancher will not bring your Harvester cluster down. You can still access your Harvester cluster using its virtual IP. Start an upgrade​ caution Before you upgrade your Harvester cluster, we highly recommend: Back up your VMs if needed. Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.Make sure your hardware meets the preferred hardware requirements. This is due to there will be intermediate resources consumed by an upgrade.Make sure each node has at least 30 GiB of free system partition space (df -h /usr/local/). If any node in the cluster has less than 30 GiB of free system partition space, the upgrade will be denied. Check free system partition space requirement for more information.Run the pre-check script on a Harvester control-plane node. Please pick a script according to your cluster's version: https://github.com/harvester/upgrade-helpers/tree/main/pre-check. caution Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server on each node: $ sudo -i # Add time servers $ vim /etc/systemd/timesyncd.conf [ntp] NTP=0.pool.ntp.org # Enable and start the systemd-timesyncd $ timedatectl set-ntp true # Check status $ sudo timedatectl status caution NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the knowledge base article for further information. Make sure to read the Warning paragraph at the top of this document first. Harvester checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard page. If the cluster is in an air-gapped environment, please see Prepare an air-gapped upgrade section first. You can also speed up the ISO download by using the approach in that section. Navigate to Harvester GUI and click the upgrade button on the Dashboard page. Select a version to start upgrading. Click the circle on the top to display the upgrade progress. Prepare an air-gapped upgrade​ caution Make sure to check Upgrade support matrix section first about upgradable versions. Download a Harvester ISO file from release pages. Save the ISO to a local HTTP server. Assume the file is hosted at http://10.10.0.1/harvester.iso. Download the version file from release pages, for example, https://releases.rancher.com/harvester/{version}/version.yaml Replace isoURL value in the version.yaml file: apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: name: v1.0.2 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://10.10.0.1/harvester.iso # change to local ISO URL releaseDate: '20220512' Assume the file is hosted at http://10.10.0.1/version.yaml. Log in to one of your control plane nodes. Become root and create a version: rancher@node1:~&gt; sudo -i rancher@node1:~&gt; kubectl create -f http://10.10.0.1/version.yaml An upgrade button should show up on the Harvester GUI Dashboard page. Free system partition space requirement​ Available as of v1.2.0 The minimum free system partition space requirement in Harvester v1.2.0 is 30 GiB, which will be revised in each release. Harvester will check the amount of free system partition space on each node when you select Upgrade. If any node does not meet the requirement, the upgrade will be denied as follows If some nodes do not have enough free system partition space, but you still want to try upgrading, you can customize the upgrade by updating the harvesterhci.io/minFreeDiskSpaceGB annotation of Version object. apiVersion: harvesterhci.io/v1beta1 kind: Version metadata: annotations: harvesterhci.io/minFreeDiskSpaceGB: &quot;30&quot; # the value is pre-defined and may be customized name: 1.2.0 namespace: harvester-system spec: isoChecksum: &lt;SHA-512 checksum of the ISO&gt; isoURL: http://192.168.0.181:8000/harvester-master-amd64.iso minUpgradableVersion: 1.1.2 releaseDate: &quot;20230609&quot; caution Setting a smaller value than the pre-defined value may cause the upgrade to fail and is not recommended in a production environment.","keywords":"Harvester harvester Rancher rancher Harvester Upgrade","version":"v1.4 (dev)"},{"title":"Monitoring","type":0,"sectionRef":"#","url":"/v1.4/troubleshooting/monitoring","content":"Monitoring The following sections contain tips to troubleshoot Harvester Monitoring. Monitoring is unusable​ When the Harvester Dashboard is not showing any monitoring metrics, it can be caused by the following reasons. Monitoring is unusable due to Pod being stuck in Terminating status​ Harvester Monitoring pods are deployed randomly on the cluster Nodes. When the Node hosting the pods accidentally goes down, the related pods may become stuck in the Terminating status rendering the Monitoring unusable from the WebUI. $ kubectl get pods -n cattle-monitoring-system NAMESPACE NAME READY STATUS RESTARTS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-crd-create-9wtzf 0/1 Terminating 0 137m cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz 3/3 Terminating 0 3d23h cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz 0/3 Init:0/2 0 132m cattle-monitoring-system rancher-monitoring-kube-state-metrics-5bc8bb48bd-nbd92 1/1 Running 4 4d1h ... Monitoring can be recovered using CLI commands to force delete the related pods. The cluster will redeploy new pods to replace them. # Delete each none-running Pod in namespace cattle-monitoring-system. $ kubectl delete pod --force -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 pod &quot;prometheus-rancher-monitoring-prometheus-0&quot; force deleted $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-admission-create-fwjn9 $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-crd-create-9wtzf $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-ph4nz $ kubectl delete pod --force -n cattle-monitoring-system rancher-monitoring-grafana-d9c56d79b-t24sz Wait for a few minutes so that the new pods are created and readied for the Monitoring dashboard to be usable again. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 98s rancher-monitoring-grafana-d9c56d79b-cp86w 0/3 Init:0/2 0 27s ... $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 7m57s rancher-monitoring-grafana-d9c56d79b-cp86w 3/3 Running 0 6m46s ... Expand PV/Volume Size​ Harvester integrates Longhorn as the default storage provider. Harvester Monitoring uses Persistent Volume (PV) to store running data. When a cluster has been running for a certain time, the Persistent Volume may need to expand its size. Based on the Longhorn Volume expansion guide, Harvester illustrates how to expand the volume size. View Volume​ From Embedded Longhorn WebUI​ Access the embedded Longhorn WebUI according to this document. The Longhorn dashboard default view. Click Volume to list all existing volumes. From CLI​ You can also use kubectl to get all Volumes. # kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 5Gi RWO harvester-longhorn 43h cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 50Gi RWO harvester-longhorn 43h cattle-monitoring-system rancher-monitoring-grafana Bound pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 2Gi RWO harvester-longhorn 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 attached degraded 2147483648 harv31 43h Scale Down a Deployment​ To detach the Volume, you need to scale down the deployment that uses the Volume. The example below is against the PVC claimed by rancher-monitoring-grafana. Find the deployment in the namespace cattle-monitoring-system. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // target deployment rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h Scale down the deployment rancher-monitoring-grafana to 0. # kubectl scale --replicas=0 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment and the volume. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 0/0 0 0 43h // scaled down rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h # kubectl get volume -A NAMESPACE NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-1b2fbbe9-14b1-4a65-941a-7d5645a89977 attached degraded 5368709120 harv31 43h longhorn-system pvc-7c6dcb61-51a9-4a38-b4c5-acaa11788978 attached degraded 53687091200 harv31 43h longhorn-system pvc-b2b2c07c-f7cd-4965-90e6-ac3319597bf7 detached unknown 2147483648 43h // volume is detached Expand Volume​ In the Longhorn WebUI, the related volume becomes Detached. Click the icon in the Operation column, and select Expand Volume. Input a new size, and Longhorn will expand the volume to this size. Scale Up a Deployment​ After the Volume is expanded to target size, you need to scale up the aforementioned deployment to its original replicas. For the above example of rancher-monitoring-grafana, the original replicas is 1. # kubectl scale --replicas=1 deployment/rancher-monitoring-grafana -n cattle-monitoring-system Check the deployment again. # kubectl get deployment -n cattle-monitoring-system NAME READY UP-TO-DATE AVAILABLE AGE rancher-monitoring-grafana 1/1 1 1 43h // scaled up rancher-monitoring-kube-state-metrics 1/1 1 1 43h rancher-monitoring-operator 1/1 1 1 43h rancher-monitoring-prometheus-adapter 1/1 1 1 43h The Volume is attached to the new POD. To now, the Volume is expanded to the new size and the POD is using it smoothly. Fail to Enable rancher-monitoring Addon​ You may encounter this when you install the Harvester v1.3.0 or higher version cluster with the minimal 250 GB disk per hardware requirements. Reproduce Steps​ Install the Harvester v1.3.0 cluster. Enable the rancher-monitoring addon, you will observe: The POD prometheus-rancher-monitoring-prometheus-0 in cattle-monitoring-system namespace fails to start due to PVC attached failed. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE alertmanager-rancher-monitoring-alertmanager-0 2/2 Running 0 3m22s helm-install-rancher-monitoring-4b5mx 0/1 Completed 0 3m41s prometheus-rancher-monitoring-prometheus-0 0/3 Init:0/1 0 3m21s // stuck in this status rancher-monitoring-grafana-d6f466988-hgpkb 4/4 Running 0 3m26s rancher-monitoring-kube-state-metrics-7659b76cc4-66sr7 1/1 Running 0 3m26s rancher-monitoring-operator-595476bc84-7hdxj 1/1 Running 0 3m25s rancher-monitoring-prometheus-adapter-55dc9ccd5d-pcrpk 1/1 Running 0 3m26s rancher-monitoring-prometheus-node-exporter-pbzv4 1/1 Running 0 3m26s $ kubectl describe pod -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-0 Name: prometheus-rancher-monitoring-prometheus-0 Namespace: cattle-monitoring-system Priority: 0 Service Account: rancher-monitoring-prometheus ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 3m48s (x3 over 4m15s) default-scheduler 0/1 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.. Normal Scheduled 3m44s default-scheduler Successfully assigned cattle-monitoring-system/prometheus-rancher-monitoring-prometheus-0 to harv41 Warning FailedMount 101s kubelet Unable to attach or mount volumes: unmounted volumes=[prometheus-rancher-monitoring-prometheus-db], unattached volumes=[prometheus-rancher-monitoring-prometheus-db], failed to process volumes=[]: timed out waiting for the condition Warning FailedAttachVolume 90s (x9 over 3m42s) attachdetach-controller AttachVolume.Attach failed for volume &quot;pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0&quot; : rpc error: code = Aborted desc = volume pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 is not ready for workloads $ kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 50Gi RWO harvester-longhorn 7m12s $ kubectl get volume -A NAMESPACE NAME DATA ENGINE STATE ROBUSTNESS SCHEDULED SIZE NODE AGE longhorn-system pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 v1 detached unknown 53687091200 6m55s The Longhorn manager is unable to schedule the replica. $ kubectl logs -n longhorn-system longhorn-manager-bf65b | grep &quot;pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0&quot; time=&quot;2024-02-19T10:12:56Z&quot; level=error msg=&quot;There's no available disk for replica pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0-r-dcb129fd, size 53687091200&quot; func=&quot;schedule r.(*ReplicaScheduler).ScheduleReplica&quot; file=&quot;replica_scheduler.go:95&quot; time=&quot;2024-02-19T10:12:56Z&quot; level=warning msg=&quot;Failed to schedule replica&quot; func=&quot;controller.(*VolumeController).reconcileVolumeCondition&quot; file=&quot;volume_controller.go:169 4&quot; accessMode=rwo controller=longhorn-volume frontend=blockdev migratable=false node=harv41 owner=harv41 replica=pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0-r-dcb129fd sta te= volume=pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 ... Workaround​ Disable the rancher-monitoring addon if you have alreay enabled it. All pods in cattle-monitoring-system are deleted but the PVCs are retained. For more information, see [Addons]. $ kubectl get pods -n cattle-monitoring-system No resources found in cattle-monitoring-system namespace. $ kubectl get pvc -n cattle-monitoring-system NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-cea6316e-f74f-4771-870b-49edb5442819 5Gi RWO harvester-longhorn 14m prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 Bound pvc-bbe8760d-926c-484a-851c-b8ec29ae05c0 50Gi RWO harvester-longhorn 14m Delete the PVC named prometheus, but retain the PVC named alertmanager. $ kubectl delete pvc -n cattle-monitoring-system prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0 persistentvolumeclaim &quot;prometheus-rancher-monitoring-prometheus-db-prometheus-rancher-monitoring-prometheus-0&quot; deleted $ kubectl get pvc -n cattle-monitoring-system NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE alertmanager-rancher-monitoring-alertmanager-db-alertmanager-rancher-monitoring-alertmanager-0 Bound pvc-cea6316e-f74f-4771-870b-49edb5442819 5Gi RWO harvester-longhorn 16m On the Addons screen of the Harvester UI, select ⋮ (menu icon) and then select Edit YAML. As indicated below, change the two occurrences of the number 50 to 30 under prometheusSpec, and then save. The prometheus feature will use a 30GiB disk to store data. Alternatively, you can use kubectl to edit the object. kubectl edit addons.harvesterhci.io -n cattle-monitoring-system rancher-monitoring retentionSize: 50GiB // Change 50 to 30 storageSpec: volumeClaimTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi // Change 50 to 30 storageClassName: harvester-longhorn Enable the rancher-monitoring addon and wait for a few minutes.. All pods are successfully deployed, and the rancher-monitoring feature is available. $ kubectl get pods -n cattle-monitoring-system NAME READY STATUS RESTARTS AGE alertmanager-rancher-monitoring-alertmanager-0 2/2 Running 0 3m52s helm-install-rancher-monitoring-s55tq 0/1 Completed 0 4m17s prometheus-rancher-monitoring-prometheus-0 3/3 Running 0 3m51s rancher-monitoring-grafana-d6f466988-hkv6f 4/4 Running 0 3m55s rancher-monitoring-kube-state-metrics-7659b76cc4-ght8x 1/1 Running 0 3m55s rancher-monitoring-operator-595476bc84-r96bp 1/1 Running 0 3m55s rancher-monitoring-prometheus-adapter-55dc9ccd5d-vtssc 1/1 Running 0 3m55s rancher-monitoring-prometheus-node-exporter-lgb88 1/1 Running 0 3m55s ","keywords":"","version":"v1.4 (dev)"},{"title":"Operating System","type":0,"sectionRef":"#","url":"/v1.4/troubleshooting/os","content":"Operating System Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the elemental-toolkit. The following sections contain information and tips to help users troubleshoot OS-related issues. How to log in to a Harvester node​ Users can log in to a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~&gt; sudo blkid # Or become root rancher@node1:~&gt; sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only?​ The OS file system, like a container image, is image-based and immutable except in some directories. We recommend using a toolbox container to run programs not packaged in the Harvester OS for debugging purposes. Please see this article to learn how to build and run a toolbox container. The Harvester OS also provides a way to enable the read-write mode temporarily. Please follow the following steps: caution Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0, we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat &gt; /oem/91_hack.yaml &lt;&lt;'EOF' name: &quot;Rootfs Layout Settings for debugrw&quot; stages: rootfs: - if: 'grep -q root=LABEL=COS_STATE /proc/cmdline &amp;&amp; grep -q rd.cos.debugrw /proc/cmdline' name: &quot;Layout configuration for debugrw&quot; environment_file: /run/cos/cos-layout.env environment: RW_PATHS: &quot; &quot; EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters​ note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry &quot;${display_name}&quot; --id cos { # label is kept around for backward compatibility set label=${active_label} set img=/cOS/active.img loopback $loopdev /$img source ($loopdev)/etc/cos/bootargs.cfg linux ($loopdev)$kernel $kernelcmd ${extra_cmdline} ${extra_active_cmdline} nomodeset initrd ($loopdev)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry​ To change the default entry, first check the --id attribute of a menu entry. Grub menu entries are located in the following files: /run/initramfs/cos-state/grub2/grub.cfg: Contains the default, fallback, and recovery entries/run/initramfs/cos-state/grubcustom: Contains the debug entry In the following example, the id of the entry is debug. # cat \\ /run/initramfs/cos-state/grub2/grub.cfg \\ /run/initramfs/cos-state/grubcustom &lt;...&gt; menuentry &quot;${display_name} (debug)&quot; --id debug { search --no-floppy --set=root --label COS_STATE set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd ${extra_cmdline} ${extra_passive_cmdline} ${crash_kernel_params} initrd (loop0)$initramfs } You can configure the default entry by running the following commands: # mount -o remount,rw /run/initramfs/cos-state # grub2-editenv /run/initramfs/cos-state/grub_oem_env set saved_entry=debug If necessary, you can undo the change by running the command grub2-editenv /run/initramfs/cos-state/grub_oem_env unset saved_entry. How to debug a system crash or hang​ Collect crash log​ If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps​ For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/&lt;time&gt; directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","keywords":"","version":"v1.4 (dev)"},{"title":"VM","type":0,"sectionRef":"#","url":"/v1.4/troubleshooting/vm","content":"VM The following sections contain information useful in troubleshooting issues related to Harvester VM management. VM Start Button is Not Visible​ Issue Description​ On rare occasions, the Start button is unavailable on the Harvester UI for VMs that are Off. Without that button, users are unable to start the VMs. VM General Operations​ On the Harvester UI, the Stop button is visible after a VM is created and started. The Start button is visible after the VM is stopped. When the VM is powered off from inside the VM, both the Start and Restart buttons are visible. General VM Related Objects​ A Running VM​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of all three objects is Running. # kubectl get vm NAME AGE STATUS READY vm8 7m25s Running True # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 78s Running 10.52.0.199 harv41 True # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 1/1 Running 0 80s A VM Stopped Using the Harvester UI​ Only the object vm exists and its status is Stopped. Both vmi and pod disappear. # kubectl get vm NAME AGE STATUS READY vm8 123m Stopped False # kubectl get vmi No resources found in default namespace. # kubectl get pod No resources found in default namespace. # A VM Stopped Using the VM's Poweroff Command​ The objects vm, vmi, and pod, which are all related to the VM, exist. The status of vm is Stopped, while the status of pod is Completed. # kubectl get vm NAME AGE STATUS READY vm8 134m Stopped False # kubectl get vmi NAME AGE PHASE IP NODENAME READY vm8 2m49s Succeeded 10.52.0.199 harv41 False # kubectl get pod NAME READY STATUS RESTARTS AGE virt-launcher-vm8-tl46h 0/1 Completed 0 2m54s Issue Analysis​ When the issue occurs, the objects vm, vmi, and pod exist. The status of the objects is similar to that of A VM Stopped Using the VM's Poweroff Command. Example: The VM ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachine ... status: conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready Similarly, the VMI (virtual machine instance) ocffm031v000 is not ready (status: &quot;False&quot;) because the virt-launcher pod is terminating (reason: &quot;PodTerminating&quot;). - apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance ... name: ocffm031v000 ... status: activePods: ec36a1eb-84a5-4421-b57b-2c14c1975018: aibfredg02 conditions: - lastProbeTime: &quot;2023-07-20T08:37:37Z&quot; lastTransitionTime: &quot;2023-07-20T08:37:37Z&quot; message: virt-launcher pod is terminating reason: PodTerminating status: &quot;False&quot; type: Ready On the other hand, the pod virt-launcher-ocffm031v000-rrkss is not ready (status: &quot;False&quot;) because the pod has run to completion (reason: &quot;PodCompleted&quot;). The underlying container 0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb is terminated, and the exitCode is 0. - apiVersion: v1 kind: Pod ... name: virt-launcher-ocffm031v000-rrkss ... ownerReferences: - apiVersion: kubevirt.io/v1 ... kind: VirtualMachineInstance name: ocffm031v000 uid: 8d2cf524-7e73-4713-86f7-89e7399f25db uid: ec36a1eb-84a5-4421-b57b-2c14c1975018 ... status: conditions: - lastProbeTime: &quot;2023-07-18T13:48:56Z&quot; lastTransitionTime: &quot;2023-07-18T13:48:56Z&quot; message: the virtual machine is not paused reason: NotPaused status: &quot;True&quot; type: kubevirt.io/virtual-machine-unpaused - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-18T13:48:55Z&quot; reason: PodCompleted status: &quot;True&quot; type: Initialized - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: Ready - lastProbeTime: &quot;null&quot; lastTransitionTime: &quot;2023-07-20T08:38:56Z&quot; reason: PodCompleted status: &quot;False&quot; type: ContainersReady ... containerStatuses: - containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb image: registry.suse.com/suse/sles/15.4/virt-launcher:0.54.0-150400.3.3.2 imageID: sha256:43bb08efdabb90913534b70ec7868a2126fc128887fb5c3c1b505ee6644453a2 lastState: {} name: compute ready: false restartCount: 0 started: false state: terminated: containerID: containerd://0d7a0f64f91438cb78f026853e6bebf502df1bdeb64878d351fa5756edc98deb exitCode: 0 finishedAt: &quot;2023-07-20T08:38:55Z&quot; reason: Completed startedAt: &quot;2023-07-18T13:50:17Z&quot; A critical difference is that the Stop and Start actions appear in the stateChangeRequests property of vm. status: conditions: ... printableStatus: Stopped stateChangeRequests: - action: Stop uid: 8d2cf524-7e73-4713-86f7-89e7399f25db - action: Start Root Cause​ The root cause of this issue is under investigation. It is notable that the source code checks the status of vm and assumes that the object is starting. No Start and Restart operations are added to the object. func (vf *vmformatter) canStart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } .. } func (vf *vmformatter) canRestart(vm *kubevirtv1.VirtualMachine, vmi *kubevirtv1.VirtualMachineInstance) bool { if vf.isVMStarting(vm) { return false } ... } func (vf *vmformatter) isVMStarting(vm *kubevirtv1.VirtualMachine) bool { for _, req := range vm.Status.StateChangeRequests { if req.Action == kubevirtv1.StartRequest { return true } } return false } Workaround​ To address the issue, you can force delete the pod using the command kubectl delete pod virt-launcher-ocffm031v000-rrkss -n namespace --force. After the pod is successfully deleted, the Start button becomes visible again on the Harvester UI. Related Issue​ https://github.com/harvester/harvester/issues/4659 VM Stuck in Starting State with Error Messsage not a device node​ Impacted versions: v1.3.0 Issue Description​ Some VMs may fail to start and then become unresponsive after the cluster or some nodes are restarted. On the Dashboard screen of the Harvester UI, the status of the affected VMs is stuck at Starting. Issue Analysis​ The status of the pod related to the affected VM is CreateContainerError. $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vm1-w9bqs 0/2 CreateContainerError 0 9m39s The phrase failed to generate spec: not a device node can be found in the following: $kubectl get pods -oyaml apiVersion: v1 items: apiVersion: v1 kind: Pod metadata: ... containerStatuses: - image: registry.suse.com/suse/sles/15.5/virt-launcher:1.1.0-150500.8.6.1 imageID: &quot;&quot; lastState: {} name: compute ready: false restartCount: 0 started: false state: waiting: message: 'failed to generate container &quot;50f0ec402f6e266870eafb06611850a5a03b2a0a86fdd6e562959719ccc003b5&quot; spec: failed to generate spec: not a device node' reason: CreateContainerError kubelet.log file: file path: /var/lib/rancher/rke2/agent/logs/kubelet.log E0205 20:44:31.683371 2837 pod_workers.go:1294] &quot;Error syncing pod, skipping&quot; err=&quot;failed to \\&quot;StartContainer\\&quot; for \\&quot;compute\\&quot; with CreateContainerError: \\&quot;failed t o generate container \\\\\\&quot;255d42ec2e01d45b4e2480d538ecc21865cf461dc7056bc159a80ee68c411349\\\\\\&quot; spec: failed to generate spec: not a device node\\&quot;&quot; pod=&quot;default/virt-laun cher-caddytest-9tjzj&quot; podUID=d512bf3e-f215-4128-960a-0658f7e63c7c containerd.log file: file path: /var/lib/rancher/rke2/agent/containerd/containerd.log time=&quot;2024-02-21T11:24:00.140298800Z&quot; level=error msg=&quot;CreateContainer within sandbox \\&quot;850958f388e63f14a683380b3c52e57db35f21c059c0d93666f4fdaafe337e56\\&quot; for &amp;ContainerMetadata{Name:compute,Attempt:0,} failed&quot; error=&quot;failed to generate container \\&quot;5ddad240be2731d5ea5210565729cca20e20694e364e72ba14b58127e231bc79\\&quot; spec: failed to generate spec: not a device node&quot; After adding debug information to containerd, it identifies the error message not a device node is upon the file pvc-3c1b28fb-*. time=&quot;2024-02-22T15:15:08.557487376Z&quot; level=error msg=&quot;CreateContainer within sandbox \\&quot;d23af3219cb27228623cf8168ec27e64e836ed44f2b2f9cf784f0529a7f92e1e\\&quot; for &amp;ContainerMetadata{Name:compute,Attempt:0,} failed&quot; error=&quot;failed to generate container \\&quot;e4ed94fb5e9145e8716bcb87aae448300799f345197d52a617918d634d9ca3e1\\&quot; spec: failed to generate spec: get device path: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-3c1b28fb-683e-4bf5-9869-c9107a0f1732/20291c6b-62c3-4456-be8a-fbeac118ec19 containerPath: /dev/disk-0 error: not a device node&quot; This is a CSI related file, but it is an empty file instead of the expected device file. Then the containerd denied the CreateContainer request. $ ls /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-3c1b28fb-683e-4bf5-9869-c9107a0f1732/ -alth total 8.0K drwxr-x--- 2 root root 4.0K Feb 22 15:10 . -rw-r--r-- 1 root root 0 Feb 22 14:28 aa851da3-cee1-45be-a585-26ae766c16ca -rw-r--r-- 1 root root 0 Feb 22 14:07 20291c6b-62c3-4456-be8a-fbeac118ec19 drwxr-x--- 4 root root 4.0K Feb 22 14:06 .. -rw-r--r-- 1 root root 0 Feb 21 15:48 4333c9fd-c2c8-4da2-9b5a-1a310f80d9fd -rw-r--r-- 1 root root 0 Feb 21 09:18 becc0687-b6f5-433e-bfb7-756b00deb61b $file /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-3c1b28fb-683e-4bf5-9869-c9107a0f1732/20291c6b-62c3-4456-be8a-fbeac118ec19 : empty The output listed above directly contrasts with the following example, which shows the expected device file of a running VM. $ ls /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-732f8496-103b-4a08-83af-8325e1c314b7/ -alth total 8.0K drwxr-x--- 2 root root 4.0K Feb 21 10:53 . drwxr-x--- 4 root root 4.0K Feb 21 10:53 .. brw-rw---- 1 root root 8, 16 Feb 21 10:53 4883af80-c202-4529-a2c6-4e7f15fe5a9b Root Cause​ After the cluster or specific nodes are rebooted, the kubelet calls NodePublishVolume for the new pod without first calling NodeStageVolume. Moreover, the Longhorn CSI plugin bind mounts the regular file at the staging target path (previously used by the deleted pod) to the target path, and the operation is considered successful. Workaround​ Cluster level operation: Find the backing pods of the affected VMs and the related Longhorn volumes. $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vm1-nxfm4 0/2 CreateContainerError 0 7m11s $ kubectl get pvc -A NAMESPACE NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE default vm1-disk-0-9gc6h Bound pvc-f1798969-5b72-4d76-9f0e-64854af7b59c 1Gi RWX longhorn-image-fxsqr 7d22h Stop the affected VMs from Harvester UI. The VM may stuck in Stopping, continue the next step. Delete the backing pods forcely. $ kubectl delete pod virt-launcher-vm1-nxfm4 --force Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod &quot;virt-launcher-vm1-nxfm4&quot; force deleted The VM is off now. Node level operation, node by node: Cordon a node. Unmout all the affected Longhorn volumes in this node. You need to ssh to this node and execute the sudo -i umount path command. $ umount /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/* umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/4b2ab666-27bd-4e3c-a218-fb3d48a72e69: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/6aaf2bbe-f688-4dcd-855a-f9e2afa18862: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/91488f09-ff22-45f4-afc0-ca97f67555e7: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/bb4d0a15-737d-41c0-946c-85f4a56f072f: not mounted. umount: /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/dev/d2a54e32-4edc-4ad8-a748-f7ef7a2cacab: not mounted. Uncordon this node. Start the affected VMs from harvester UI. Wait some time, the VM will run successfully. The newly generated csi file is an expected device file. $ ls /var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-f1798969-5b72-4d76-9f0e-64854af7b59c/ -alth ... brw-rw---- 1 root root 8, 64 Mar 6 11:47 7beb531d-a781-4775-ba5e-8773773d77f1 Related Issue​ https://github.com/harvester/harvester/issues/5109 https://github.com/longhorn/longhorn/issues/8009","keywords":"","version":"v1.4 (dev)"},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"/v1.4/upgrade/troubleshooting","content":"Troubleshooting Overview​ Here are some tips to troubleshoot a failed upgrade: Check version-specific upgrade notes. You can click the version in the support matrix table to see if there are any known issues.Dive into the upgrade design proposal. The following section briefly describes phases within an upgrade and possible diagnostic methods. Diagnose the upgrade flow​ A Harvester upgrade process contains several phases. Phase 1: Provision upgrade repository VM.​ The Harvester controller downloads a Harvester release ISO file and uses it to provision a VM. During this phase you can see the upgrade status windows show: The time to complete the phase depends on the user's network speed and cluster resource utilization. We see failures in this phase due to network speed. If this happens, the user can start over the upgrade again. We can also check the repository VM (named with the format upgrade-repo-hvst-xxxx) status and its corresponding pod: $ kubectl get vm -n harvester-system NAME AGE STATUS READY upgrade-repo-hvst-upgrade-9gmg2 101s Starting False $ kubectl get pods -n harvester-system | grep upgrade-repo-hvst virt-launcher-upgrade-repo-hvst-upgrade-9gmg2-4mnmq 1/1 Running 0 4m44s Phase 2: Preload container images​ The Harvester controller creates jobs on each Harvester node to download images from the repository VM and preload them. These are the container images required for the next release. During this stage you can see the upgrade status windows shows: It will take a while for all nodes to preload images. If the upgrade fails at this phase, the user can check job logs in the cattle-system namespace: $ kubectl get jobs -n cattle-system | grep prepare apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 0/1 47s 47s apply-hvst-upgrade-9gmg2-prepare-on-node4-with-2bbea1599a-041e4 1/1 2m3s 2m50s $ kubectl logs jobs/apply-hvst-upgrade-9gmg2-prepare-on-node1-with-2bbea1599a-f0e86 -n cattle-system ... It's also safe to start over the upgrade if an upgrade fails at this phase. Phase 3: Upgrade system services​ In this phase, Harvester controller upgrades component Helm charts with a job. The user can check the apply-manifest job with the following command: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s $ kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system ... Phase 4: Upgrade nodes​ The Harvester controller creates jobs on each node (one by one) to upgrade nodes' OSes and RKE2 runtime. For multi-node clusters, there are two kinds of jobs to update a node: pre-drain job: live-migrate or shutdown VMs on a node. When the job completes, the embedded Rancher service upgrades RKE2 runtime on a node.post-drain job: upgrade OS and reboot. For single-node clusters, there is only one single-node-upgrade type job for each node (named with the format hvst-upgrade-xxx-single-node-upgrade-&lt;hostname&gt;). The user can check node jobs by: $ kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=node NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-post-drain-node1 1/1 118s 6m34s hvst-upgrade-9gmg2-post-drain-node2 0/1 9s 9s hvst-upgrade-9gmg2-pre-drain-node1 1/1 3s 8m14s hvst-upgrade-9gmg2-pre-drain-node2 1/1 7s 85s $ kubectl logs -n harvester-system jobs/hvst-upgrade-9gmg2-post-drain-node2 ... caution Please do not start over an upgrade if the upgrade fails at this phase. Phase 5: Clean-up​ The Harvester controller deletes the upgrade repository VM and all files that are no longer needed. Common operations​ Start over an upgrade​ Log in to a control plane node. List Upgrade CRs in the cluster: # become root $ sudo -i # list the on-going upgrade $ kubectl get upgrade.harvesterhci.io -n harvester-system -l harvesterhci.io/latestUpgrade=true NAME AGE hvst-upgrade-9gmg2 10m Delete the Upgrade CR $ kubectl delete upgrade.harvesterhci.io/hvst-upgrade-9gmg2 -n harvester-system Click the upgrade button in the Harvester dashboard to start an upgrade again. Download upgrade logs​ We have designed and implemented a mechanism to automatically collect all the upgrade-related logs and display the upgrade procedure. By default, this is enabled. You can also choose to opt out of such behavior. You can click the Download Log button to download the log archive during an upgrade. Log entries will be collected as files for each upgrade-related Pod, even for intermediate Pods. The support bundle provides a snapshot of the current state of the cluster, including logs and resource manifests, while the upgrade log preserves any logs generated during an upgrade. By combining these two, you can further investigate the issues during upgrades. After the upgrade ended, Harvester stops collecting the upgrade logs to avoid occupying the disk space. In addition, you can click the Dismiss it button to purge the upgrade logs. For more details, please refer to the upgrade log HEP. caution The storage volume for storing upgrade-related logs is 1GB by default. If an upgrade went into issues, the logs may consume all the available space of the volume. To work around such kind of incidents, try the following steps: Detach the log-archive Volume by scaling down the fluentd StatefulSet and downloader Deployment. # Locate the StatefulSet and Deployment $ kubectl -n harvester-system get statefulsets -l harvesterhci.io/upgradeLogComponent=aggregator NAME READY AGE hvst-upgrade-xxxxx-upgradelog-infra-fluentd 1/1 43s $ kubectl -n harvester-system get deployments -l harvesterhci.io/upgradeLogComponent=downloader NAME READY UP-TO-DATE AVAILABLE AGE hvst-upgrade-xxxxx-upgradelog-downloader 1/1 1 1 38s # Scale down the resources to terminate any Pods using the volume $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=0 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=0 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled Expand the volume size via Longhorn dashboard. For more details, please refer to the volume expansion guide. # Here's how to find out the actual name of the target volume $ kubectl -n harvester-system get pvc -l harvesterhci.io/upgradeLogComponent=log-archive -o jsonpath='{.items[].spec.volumeName}' pvc-63355afb-ce61-46c4-8781-377cf962278a Recover the fluentd StatefulSet and downloader Deployment. $ kubectl -n harvester-system scale statefulset hvst-upgrade-xxxxx-upgradelog-infra-fluentd --replicas=1 statefulset.apps/hvst-upgrade-xxxxx-upgradelog-infra-fluentd scaled $ kubectl -n harvester-system scale deployment hvst-upgrade-xxxxx-upgradelog-downloader --replicas=1 deployment.apps/hvst-upgrade-xxxxx-upgradelog-downloader scaled ","keywords":"","version":"v1.4 (dev)"},{"title":"Upgrade from v1.1.1/v1.1.2 to v1.1.3","type":0,"sectionRef":"#","url":"/v1.4/upgrade/v1-1-1-to-v1-1-3","content":"Upgrade from v1.1.1/v1.1.2 to v1.1.3 General information​ An Upgrade button appears on the Dashboard screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see Start an upgrade. For air-gapped environments, see Prepare an air-gapped upgrade. Known Issues​ 1. The upgrade process is stuck when pre-draining a node. (Case 1)​ Starting from v1.1.0, Harvester waits for all volumes to become healthy before upgrading a node (for clusters with three or more nodes). When this issue occurs, you can check the health of the affected volumes on the embedded Longhorn UI. You can also check the pre-drain job logs. For more troubleshooting information, see Phase 4: Upgrade nodes. 2. The upgrade process is stuck when pre-draining a node. (Case 2)​ An upgrade is stuck, as shown in the screenshot below: Harvester is unable to proceed with the upgrade and the status of two or more nodes is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. The upgrade process is stuck on the first node.​ Harvester attempts to upgrade the first node but is unable to proceed. The upgrade eventually fails because the job is not completed by the expected end time. Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. The status of a Fleet bundle after the upgrade indicates that deployment errors occurred.​ After an upgrade is completed, the status of a bundle managed by Fleet may be ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]. The errors that occurred while deploying the bundle may block the next Harvester upgrade or managedChart update if not addressed. To check the status of bundles, run the following command: kubectl get bundles -A The following output indicates that the issue exists in your cluster. NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. The upgrade process stops after the upgrade repository is created.​ Harvester is unable to retrieve the harvester-release.yaml file and proceed with the upgrade. The following error message is displayed: Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers)context deadline exceeded (Client.Timeout exceeded while awaiting headers)` message: This issue was fixed in v1.1.2. For v1.1.0 and v1.1.1 users, however, the workaround is to restart the upgrade process. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. The upgrade is stuck in the &quot;Pre-drained&quot; state.​ This issue could be caused by a misconfigured pod disruption budget (PDB). You can perform the following steps to confirm the cause and use the current workaround. In this example, the affected node is harvester-node-1. Check the name of the instance-manager-e or instance-manager-r pod on the node. $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output shows that the instance-manager-r-d4ed2788 pod is on the node. Check the Rancher logs and verify that the instance-manager-e or instance-manager-r pod cannot be drained. $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check if a PDB is associated with the node. $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager for the associated PDB. $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output does not show the affected node, the issue exists in your cluster. In this example, the output shows harvester-node-2 instead of harvester-node-1. Check the health of all volumes. kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should show that all volumes are marked healthy. If not, consider uncordoning nodes to improve volume health. Remove the misconfigured PDB. kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.4 (dev)"},{"title":"Upgrade from v1.1.0/v1.1.1 to v1.1.2","type":0,"sectionRef":"#","url":"/v1.4/upgrade/v1-1-to-v1-1-2","content":"Upgrade from v1.1.0/v1.1.1 to v1.1.2 danger Please do not upgrade a running cluster to v1.1.2 if your machine has an Intel E810 NIC card. We saw some reports that the NIC card has a problem when added to a bonding device. Please check this issue for more info: https://github.com/harvester/harvester/issues/3860. General information​ Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known Issues​ 1. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 2. An upgrade is stuck when pre-draining a node (case 2)​ An upgrade is stuck, as shown in the screenshot below: And you can also observe that multiple nodes' status is SchedulingDisabled. $ kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready control-plane,etcd,master 20d v1.24.7+rke2r1 node2 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 node3 Ready,SchedulingDisabled control-plane,etcd,master 20d v1.24.7+rke2r1 Related issue: [BUG] Multiple nodes pre-drains in an upgrade Workaround: https://github.com/harvester/harvester/issues/3216#issuecomment-1328607004 3. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 4. After an upgrade, a fleet bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress]​ There is a chance fleet-managed bundle's status is ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] after an upgrade. To check if this happened, run the following command: kubectl get bundles -A If you see the following output, it's possible that your cluster has hit the issue: NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS fleet-local fleet-agent-local 0/1 ErrApplied(1) [Cluster fleet-local/local: another operation (install/upgrade/rollback) is in progress] fleet-local local-managed-system-agent 1/1 fleet-local mcc-harvester 1/1 fleet-local mcc-harvester-crd 1/1 fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 1/1 fleet-local mcc-rancher-logging-crd 1/1 fleet-local mcc-rancher-monitoring 1/1 fleet-local mcc-rancher-monitoring-crd 1/1 Related issue: [BUG] Harvester single node upgrade will get another operation (install/upgrade/rollback) is in progress error Workaround: https://github.com/harvester/harvester/issues/3616#issuecomment-1489892688 5. An upgrade stops because it can't retrieve the harvester-release.yaml file​ An upgrade is stopped with the Get &quot;http://upgrade-repo-hvst-upgrade-mldzx.harvester-system/harvester-iso/harvester-release.yaml&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) message: We have fixed this issue in v1.1.2. But for v1.1.0 and v1.1.1 users, the workaround is to start over an upgrade. Please refer to Start over an upgrade. Related issue: https://github.com/harvester/harvester/issues/3729 Workaround: Start over an upgrade 6. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: This could be caused by a misconfigured PDB. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4","keywords":"","version":"v1.4 (dev)"},{"title":"Upgrade from v1.1.2/v1.2.0 to v1.2.1","type":0,"sectionRef":"#","url":"/v1.4/upgrade/v1-2-0-to-v1-2-1","content":"Upgrade from v1.1.2/v1.2.0 to v1.2.1 Important changes to this version​ Harvester v1.2.1 fixes upgrade known issues found in v1.2.0, we suggest upgrading v1.1.2 and v1.2.0 clusters to v1.2.1. The known issues found in v1.2.0 are: An Upgrade is stuck in the Post-draining stateAn upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent For clusters already upgraded to v1.2.0, please refer to the release note for new changes. General information​ tip Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this URL for the script. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ Please check v1.2.0 known issues.","keywords":"","version":"v1.4 (dev)"},{"title":"Access to the Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/vm/access-to-the-vm","content":"Access to the Virtual Machine Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI​ VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, e.g., the Ubuntu-Minimal-Cloud image, the VM can only be accessed with the serial console. SSH Access​ Harvester provides two ways to inject SSH public keys into virtual machines. Generally, these methods fall into two categories. Static key injection, which places keys in the cloud-init script when the virtual machine is first powered on; dynamic injection, which allows keys or basic auth to be updated dynamically at runtime. Static SSH Key Injection via cloud-init​ You can provide ssh keys to your virtual machines during the creation time on the Basics tab. Additionally, you can place the public ssh keys into your cloud-init script to allow it to take place. Example of SSH key cloud-init configuration:​ #cloud-config ssh_authorized_keys: - &gt;- ssh-rsa #replace with your public key Dynamic SSH Key Injection via Qemu guest agent​ Available as of v1.0.1 Harvester supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the qemuGuestAgent propagation method. note This method requires the qemu guest agent to be installed within the guest VM. When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. You can inject your access credentials via the Harvester dashboard as below: Select the VM and click ⋮ button.Click the Edit Config button and go to the Access Credentials tab.Click to add either basic auth credentials or ssh keys, (e.g., add opensuse as the user and select your ssh keys if your guest OS is OpenSUSE).Make sure your qemu guest agent is already installed and the VM should be restarted after the credentials are added. note You need to enter the VM to edit password or remove SSH-Key after deleting the credentials from the UI. Access with the SSH Client​ Once the VM is up and running, you can enter the IP address of the VM in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@&lt;ip-address-or-hostname&gt; ","keywords":"Harvester harvester Rancher rancher Access to the VM","version":"v1.4 (dev)"},{"title":"Upload Images","type":0,"sectionRef":"#","url":"/v1.4/upload-image","content":"Upload Images Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL​ To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File​ Currently, qcow2, raw, and ISO images are supported. note Please do not refresh the page until the file upload is finished. HTTP 413 Error in Rancher Multi-Cluster Management​ You can upload images from the Multi-Cluster Management screen on the Rancher UI. When the status of an image is Uploading but the progress indicator displays 0% for an extended period, check the HTTP response status code. 413 indicates that the size of the request body exceeds the limit. The maximum request body size should be specific to the cluster that is hosting Rancher (for example, RKE2 clusters have a default limit of 1 MB but no such limit exists in K3s clusters). The current workaround is to upload images from the Harvester UI. If you choose to upload images from the Rancher UI, you may need to configure related settings on the ingress server (for example, proxy-body-size in NGINX). If Rancher is deployed on an RKE2 cluster, perform the following steps: Edit the Rancher ingress. $ kubectl -n cattle-system edit ingress rancher Specify a value for nginx.ingress.kubernetes.io/proxy-body-size. Example: Delete the stuck image, and then restart the upload process. Create Images via Volumes​ On the Volumes page, click Export Image. Enter the image name and select a StorageClass to create an image. Image StorageClass​ When creating an image, you can select a StorageClass and use its pre-defined parameters like replicas, node selectors and disk selectors . note The image will not use the StorageClass selected here directly. It's just a StorageClass template. Instead, it will create a special StorageClass under the hood with a prefix name of longhorn-. This is automatically done by the Harvester backend, but it will inherit the parameters from the StorageClass you have selected. Image Labels​ You can add labels to the image, which will help identify the OS type more accurately. Also, you can add any custom labels for filtering if needed. If your image name or URL contains any valid information, the UI will automatically recognize the OS type and image category for you. If not, you can also manually specify those corresponding labels on the UI.","keywords":"Harvester harvester Rancher rancher Import Images","version":"v1.4 (dev)"},{"title":"Clone VM","type":0,"sectionRef":"#","url":"/v1.4/vm/clone-vm","content":"Clone VM Available as of v1.1.0 VM can be cloned with/without data. This function doesn't need to take a VM snapshot or set up a backup target first. Clone VM with volume data​ On the Virtual Machines page, click Clone of the VM actions.Set a new VM name and click Create to create a new VM. Clone VM without volume data​ Cloning a VM without volume data creates a new VM with the same configuration as the source VM. On the Virtual Machines page, click Clone of the VM actions.Unclick the clone volume data checkbox.Set a new VM name and click Create to create a new VM.","keywords":"Harvester harvester Rancher rancher Clone VM","version":"v1.4 (dev)"},{"title":"VM Backup, Snapshot & Restore","type":0,"sectionRef":"#","url":"/v1.4/vm/backup-restore","content":"VM Backup, Snapshot &amp; Restore VM Backup &amp; Restore​ Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. note A backup target must be set up. For more information, see Configure Backup Target. If the backup target has not been set, you’ll be prompted with a message to do so. Configure Backup Target​ A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings &gt; backup-target. Parameter\tType\tDescriptionType\tstring\tChoose S3 or NFS Endpoint\tstring\tA hostname or an IP address. It can be left empty for AWS S3. BucketName\tstring\tName of the bucket BucketRegion\tstring\tRegion of the bucket AccessKeyID\tstring\tA user-id that uniquely identifies your account SecretAccessKey\tstring\tThe password to your account Certificate\tstring\tPaste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle\tbool\tUse VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup​ Once the backup target is set, go to the Virtual Machines page.Click Take Backup of the VM actions to create a new VM backup.Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Backup &amp; Snapshot &gt; VM Backups page to view all VM backups. The State will be set to Ready once the backup is complete. Users can either restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup​ To restore a new VM from a backup, follow these steps: Go to the VM Backups page.Click the Restore Backup button at the top right.Specify the new VM name and click Create.A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a backup​ You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the VM Backups page.Click the Restore Backup button at the top right.Click Replace Existing.You can view the restore process from the Virtual Machines page. Restore a new VM on another Harvester cluster​ Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata &amp; content backup feature. prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster​ Check the existing image name (normally starts with image-) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat &lt;&lt;EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: &quot;&quot; pvcNamespace: &quot;&quot; sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster​ Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster.Go to the VM Backups page.Select the synced VM backup metadata and choose to restore a new VM with a specified VM name.A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page. VM Snapshot &amp; Restore​ Available as of v1.1.0 VM snapshots are created from the Virtual Machines page. The VM snapshot volumes will be stored in the cluster, and they can be used to either restore a new VM or replace an existing VM. Create a VM snapshot​ Go to the Virtual Machines page.Click Take VM Snapshot of the VM actions to create a new VM snapshot.Set a custom snapshot name and click Create to create a new VM snapshot. Result: The snapshot is created. You can also go to the Backup &amp; Snapshot &gt; VM Snapshots page to view all VM snapshots. The State will be set to Ready once the snapshot is complete. Users can either restore a new VM or replace an existing VM using this snapshot. Restore a new VM using a snapshot​ To restore a new VM from a snapshot, follow these steps: Go to the VM Snapshots page.Click the Restore Snapshot button at the top right.Specify the new VM name and click Create.A new VM will be restored using the snapshot volumes and metadata, and you can access it from the Virtual Machines page. Replace an existing VM using a snapshot​ You can replace an existing VM using the snapshot. note You can only choose to retain the previous volumes. Go to the VM Snapshots page.Click the Restore Snapshot button at the top right.Click Replace Existing.You can view the restore process from the Virtual Machines page.","keywords":"Harvester harvester Rancher rancher VM Backup  Snapshot & Restore","version":"v1.4 (dev)"},{"title":"Upgrade from v1.1.2 to v1.2.0 (not recommended)","type":0,"sectionRef":"#","url":"/v1.4/upgrade/v1-1-2-to-v1-2-0","content":"Upgrade from v1.1.2 to v1.2.0 (not recommended) caution Due to the known issues found v1.2.0: An Upgrade is stuck in the Post-draining stateAn upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent We don't recommend upgrading to v1.2.0. Please upgrade your v1.1.x cluster to v1.2.1. General information​ tip Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this URL for the script. Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to start an upgrade. For the air-gap env upgrade, please refer to prepare an air-gapped upgrade. Known issues​ 1. An upgrade can't start and reports &quot;validator.harvesterhci.io&quot; denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready​ If a cluster is configured with a storage network, an upgrade can't start with the following message. Related issue: [Doc] upgrade stuck while upgrading system service with alertmanager and prometheus Workaround: https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192 2. An upgrade is stuck in Creating Upgrade Repository​ During an upgrade, Creating Upgrade Repository is stuck in the Pending state: Please perform the following steps to check if the cluster runs into the issue: Check the upgrade repository pod: If the virt-launcher-upgrade-repo-hvst-&lt;upgrade-name&gt; pod stays in ContainerCreating, your cluster might have run into this issue. In this case, proceed with step 2. Check the upgrade repository volume in the Longhorn GUI. Go to Longhorn GUI. Navigate to the Volume page. Check the upgrade repository VM volume. It should be attached to a pod called virt-launcher-upgrade-repo-hvst-&lt;upgrade-name&gt;. If one of the volume's replicas stays in Stopped (gray color), the cluster is running into the issue. Related issue: [BUG] upgrade stuck on create upgrade VM Workaround: Delete the Stopped replica from Longhorn GUI. Or,Start over the upgrade. 3. An upgrade is stuck when pre-draining a node​ Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count &gt;= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the &quot;pre-draining&quot; state. Visit &quot;Access Embedded Longhorn&quot; to see how to access the embedded Longhorn GUI. You can also check the pre-drain job logs. Please refer to Phase 4: Upgrade nodes in the troubleshooting guide. 4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline​ An upgrade fails, as shown in the screenshot below: Related issue: [BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline Workaround: https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690 5. An upgrade is stuck in the Pre-drained state​ You might see an upgrade is stuck in the &quot;pre-drained&quot; state: In this stage, Kubernetes is supposed to drain the workload on the node, but some reasons might cause the process to stall. 5.1 The node contains a Longhorn instance-manager-r pod that serves single-replica volume(s)​ Longhorn doesn't allow draining a node if the node contains the last surviving replica of a volume. To check if a node is running into this situation, follow these steps: List single-replica volumes with the command: kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + &quot;/&quot; + .metadata.name' For example: $ kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + &quot;/&quot; + .metadata.name' longhorn-system/pvc-d1f19bab-200e-483b-b348-c87cfbba85ab Check if the replica resides on the stuck node: List the NodeID of the volume's replica with the command: kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == &quot;&lt;volume&gt;&quot;) | .spec.nodeID' For example: $ kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == &quot;pvc-d1f19bab-200e-483b-b348-c87cfbba85ab&quot;) | .spec.nodeID' node1 If the result shows that the replica resides on the node where the upgrade is stuck (in this example, node1), your cluster is hitting this issue. There are a couple of ways to address this situation. Choose the most appropriate method for your VM: Shut down the VM that uses the single-replica volume to detach the volume, allowing the upgrade to continue.Adjust the volumes's replicas to more than one. Go to Longhorn GUI.Go to the Volume page.Locate the problematic volume and click the icon on the right side, then select Update Replicas Count:Increase the Number of Replicas and select OK. 5.2 Misconfigured Longhorn instance-manager-r Pod Disruption Budgets (PDB)​ A misconfigured PDB could cause this issue. To check if that's the case, perform the following steps: Assume the stuck node is harvester-node-1. Check the instance-manager-e or instance-manager-r pod names on the stuck node: $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager instance-manager-r-d4ed2788 1/1 Running 0 3d8h The output above shows that the instance-manager-r-d4ed2788 pod is on the node. Check Rancher logs and verify that the instance-manager-e or instance-manager-r pod can't be drained: $ kubectl logs deployment/rancher -n cattle-system ... 2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def 2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788 2023-03-28T17:10:55.080933607Z error when evicting pods/&quot;instance-manager-r-d4ed2788&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Run the command to check if there is a PDB associated with the stuck node: $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels.&quot;longhorn.io/node&quot;==&quot;harvester-node-1&quot;) | .metadata.name' instance-manager-r-466e3c7f Check the owner of the instance manager to this PDB: $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID' harvester-node-2 If the output doesn't match the stuck node (in this example output, harvester-node-2 doesn't match the stuck node harvester-node-1), then we can conclude this issue happens. Before applying the workaround, check if all volumes are healthy: kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == &quot;attached&quot;)| .status.robustness' The output should all be healthy. If this is not the case, you might want to uncordon nodes to make the volume healthy again. Remove the misconfigured PDB: kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system Related issue: [BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0-&gt;v1.1.2-rc4 5.3 The instance-manager-e pod could not be drained​ During an upgrade, you might encounter an issue where you can't drain the instance-manager-e pod. When this situation occurs, you will see error messages in the Rancher logs like the ones shown below: $ kubectl logs deployment/rancher -n cattle-system | grep &quot;evicting pod&quot; evicting pod longhorn-system/instance-manager-r-a06a43f3437ab4f643eea7053b915a80 evicting pod longhorn-system/instance-manager-e-452e87d2 error when evicting pods/&quot;instance-manager-r-a06a43f3437ab4f643eea7053b915a80&quot; -n &quot;Longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. error when evicting pods/&quot;instance-manager-e-452e87d2&quot; -n &quot;longhorn-system&quot; (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. Check the instance-manager-e to see if any engine instances remain. $ kubectl get instancemanager instance-manager-e-452e87d2 -n longhorn-system -o yaml | yq -e &quot;.status.instances&quot; pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57: spec: name: pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57 status: endpoint: &quot;&quot; errorMsg: &quot;&quot; listen: &quot;&quot; portEnd: 10001 portStart: 10001 resourceVersion: 0 state: running type: &quot;&quot; In this example, the instance-manager-e-452e87d2 still has an engine instance, so you can't drain the pod. You need to check the engine numbers to see if any engine number is redundant. Each PVC should only have one engine. # kubectl get engines -n longhorn-system -l longhornvolume=pvc-7b120d60-1577-4716-be5a-62348271025a NAME STATE NODE INSTANCEMANAGER IMAGE AGE pvc-76120d60-1577-4716-be5a-62348271025a-e-08220662 running harvester-qv4hd instance-manager-e-625d715e2f2e7065d64339f9b31407c2 longhornio/longhorn-engine:v1.4.3 2d12h pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 running harvester-lhlkv instance-manager-e-452e87d2 longhornio/longhorn-engine:v1.4.3 4d10h The example above shows that two engines exist for the same PVC, which is a known issue in Longhorn #6642. To resolve this, delete the redundant engine to allow the upgrade to continue. To determine which engine is the correct one, use the following command: $ kubectl get volumes pvc-7b120d60-1577-4716-be5a-62348271025a -n longhorn-system NAME STATE ROBUSTNESS SCHEDULED SIZE NODE AGE pvc-7b120d60-1577-4716-be5a-62348271025a attached healthy 42949672960 harvester-q4vhd 4d10h In this example, the volume pvc-7b120d60-1577-4716-be5a-62348271025a is active on the node harvester-q4vhd, indicating that the engine not running on this node is redundant. To make the engine inactive and trigger its automatic deletion by Longhorn, run the following command: $ kubectl patch engine pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 -n longhorn-system --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/active&quot;, &quot;value&quot;: false}]' engine.longhorn.io/pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 patched After a few seconds, you can verify the engine's status: $ kubectl get engine -n longhorn-system|grep pvc-7b120d60-1577-4716-be5a-62348271025a pvc-7b120d60-1577-4716-be5a-62348271025a-e-08220b62 running harvester-q4vhd instance-manager-e-625d715e2f2e7065d64339f9631407c2 longhornio/longhorn-engine:v1.4.3 2d13h The instance-manager-e pod should now drain successfully, allowing the upgrade to proceed. Related issue: [BUG] Upgrade (v1.1.2 -&gt; v1.2.0-rc6) stuck in pre-drained 6. An upgrade is stuck in the Upgrading System Service state​ If you notice the upgrade is stuck in the Upgrading System Service state for a long period of time, you might need to investigate if the upgrade is stuck in the apply-manifests phase. POD prometheus-rancher-monitoring-prometheus-0 is to be deleted​ Check the log of the apply-manifests pod to see if the following messages repeat. $ kubectl -n harvester-system logs hvst-upgrade-md6wr-apply-manifests-wqslg --tail=10 Tue Sep 5 10:20:39 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:45 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:50 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:20:55 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Tue Sep 5 10:21:00 UTC 2023 there are still 1 pods in cattle-monitoring-system to be deleted Check if the prometheus-rancher-monitoring-prometheus-0 pod is stuck with the status Terminating. $ kubectl -n cattle-monitoring-system get pods NAME READY STATUS RESTARTS AGE prometheus-rancher-monitoring-prometheus-0 0/3 Terminating 0 19d Find the UID of the terminating pod with the following command: $ kubectl -n cattle-monitoring-system get pod prometheus-rancher-monitoring-prometheus-0 -o jsonpath='{.metadata.uid}' 33f43165-6faa-4648-927d-69097901471c Get access to any node of the cluster via the console or SSH. Search for the related log messages in /var/lib/rancher/rke2/agent/logs/kubelet.log using the pod's UID. E0905 10:26:18.769199 17399 reconciler.go:208] &quot;operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : UnmountVolume.NewUnmounter failed for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory&quot; err=&quot;UnmountVolume.NewUnmounter failed for volume \\&quot;pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot; (UniqueName: \\&quot;kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\&quot;) pod \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot; (UID: \\&quot;33f43165-6faa-4648-927d-69097901471c\\&quot;) : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory&quot; If kubelet continues to complain about the volume failing to unmount, apply the following workaround to allow the upgrade to proceed. Forcibly remove the pod stuck with the status Terminating with the following command: kubectl delete pod prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system --force Related issue [BUG] The rancher-monitoring Pod stuck at terminating status when upgrading from v1.1.2 to v1.2.0-rc6 Multiple PODs in cattle-monitoring-system namespace are to be deleted​ Check the log of the apply-manifests pod to see if the following messages repeat. there are still 10 pods in cattle-monitoring-system to be deleted Fri Dec 8 19:06:56 UTC 2023 there are still 10 pods in cattle-monitoring-system to be deleted Fri Dec 8 19:07:01 UTC 2023 When it continues to show 10 (or other number) pods, it encounters below issue. The monitoring feature is deployed from the rancher-monitoring ManagedChart, in Harvester v1.2.0,v1.2.1, this ManagedChart is converted to Harvester Addon feature when upgrading. The ManagedChart rancher-monitoring is deleted, normally, all the generated resources including deployment, daemonset etc. will be deleted automatically. But in this case, those resources are not deleted. The above log reflects the result. Following instructions will guide to delete them manually. Locate the affected resources in the cattle-monitoring-system namespace. Root level resources in cattle-monitoring-system Customized CRD: Prometheus Object: rancher-monitoring-prometheus Sub-object: statefulset.apps/prometheus-rancher-monitoring-prometheus Customized CRD: Alertmanager object: rancher-monitoring-alertmanager Sub-object: statefulset.apps/alertmanager-rancher-monitoring-alertmanager Deployment: rancher-monitoring-grafana rancher-monitoring-kube-state-metrics rancher-monitoring-operator rancher-monitoring-prometheus-adapter Daemonset: rancher-monitoring-prometheus-node-exporter Delete the affected resources. Use below commands to delete them, meanwhile check the log of the `apply-manifests` until it does not report `there are still x pods in cattle-monitoring-system to be deleted`. kubectl delete prometheus rancher-monitoring-prometheus -n cattle-monitoring-system kubectl delete alertmanager rancher-monitoring-alertmanager -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-grafana -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-kube-state-metrics -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-operator -n cattle-monitoring-system kubectl delete deployment rancher-monitoring-prometheus-adapter -n cattle-monitoring-system kubectl delete daemonset rancher-monitoring-prometheus-node-exporter -n cattle-monitoring-system note You may need to run some of the commands more than once to completely delete the resources. Related issue [BUG] upgrade hung on apply-manifests 7. Upgrade stuck in the Upgrading System Service state​ If an upgrade is stuck in an Upgrading System Service state for an extended period, some system services' certificates may have expired. To investigate and resolve this issue, follow these steps: Find the apply-manifest job's name with the command: kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest Example output: NAME COMPLETIONS DURATION AGE hvst-upgrade-9gmg2-apply-manifests 0/1 46s 46s Check the job's log with the command: kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system If the following messages appear in the log, continue to the next step: Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)... Check CAPI cluster's state with the command: kubectl get clusters.provisioning.cattle.io local -n fleet-local -o yaml If you see a condition similar to the one below, it's likely that the cluster has encountered the issue: - lastUpdateTime: &quot;2023-01-17T16:26:48Z&quot; message: 'configuring bootstrap node(s) custom-24cb32ce8387: waiting for probes: kube-controller-manager, kube-scheduler' reason: Waiting status: Unknown type: Updated Find the machine's hostname with the following command, and follow the workaround to see if service certificates expire on a node: kubectl get machines.cluster.x-k8s.io -n fleet-local &lt;machine_name&gt; -o yaml | yq .status.nodeRef.name Replace &lt;machine_name&gt; with the machine's name from the output in the previous step. note If multiple nodes joined the cluster around the same time, you should perform the workaround on all those nodes. Related issue: [DOC/ENHANCEMENT] need to add cert-rotate feature, otherwise upgrade may stuck on Waiting for CAPI cluster fleet-local/local to be provisioned Workaround: https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311 8. The registry.suse.com/harvester-beta/vmdp:latest image is not available in air-gapped environment​ Harvester does not package the registry.suse.com/harvester-beta/vmdp:latest image in the ISO file as of v1.1.0. For Windows VMs before v1.1.0, they used this image as a container disk. However, kubelet may remove old images to free up bytes. Windows VMs can't access an air-gapped environment when this image is removed. You can fix this issue by changing the image to registry.suse.com/suse/vmdp/vmdp:2.5.4.2 and restarting the Windows VMs. Related issue: [BUG] VMDP Image wrong after upgrade to Harvester 1.2.0 9. An Upgrade is stuck in the Post-draining state​ note This known issue is fixed in v1.2.1. The node might be stuck in the OS upgrade process if you encounter the Post-draining state, as shown below. Harvester uses elemental upgrade to help us upgrade the OS. Check the elemental upgrade logs to see if there are any errors. You can check the elemental upgrade logs with the following commands: # View the post-drain job, which should be named `hvst-upgrade-xxx-post-drain-xxx` $ kubectl get pod --selector=harvesterhci.io/upgradeJobType=post-drain -n harvester-system # Check the logs with the following command $ kubectl logs -n harvester-system pods/hvst-upgrade-xxx-post-drain-xxx Suppose you see the following error in the logs. An incomplete state.yaml causes this issue. Flag --directory has been deprecated, 'directory' is deprecated please use 'system' instead INFO[2023-09-13T12:02:42Z] Starting elemental version 0.3.1 INFO[2023-09-13T12:02:42Z] reading configuration form '/tmp/tmp.N6rn4F6mKM' ERRO[2023-09-13T12:02:42Z] Invalid upgrade command setup undefined state partition elemental upgrade failed with return code: 33 + ret=33 + '[' 33 '!=' 0 ']' + echo 'elemental upgrade failed with return code: 33' + cat /host/usr/local/upgrade_tmp/elemental-upgrade-20230913120242.log In this case, Harvester upgrades the elemental-cli to the latest version. It will try to find the state partition from the state.yaml. If the state.yaml is incomplete, there is a chance it will fail to find the state partition. The incomplete state.yaml will look like the following. # Autogenerated file by elemental client, do not edit date: &quot;2023-09-13T08:31:42Z&quot; state: # we are missing `label` here. active: source: dir:///tmp/tmp.01deNrXNEC label: COS_ACTIVE fs: ext2 passive: null Remove this incomplete state.yaml file to work around this issue. (The post-draining will retry every 10 minutes). Remount the state partition to RW. $ mount -o remount,rw /run/initramfs/cos-state Remove the state.yaml. $ rm -f /run/initramfs/cos-state/state.yaml Remount the state partition to RO. $ mount -o remount,ro /run/initramfs/cos-state After performing the steps above, you should pass post-draining with the next retry. Related issues: [BUG] Upgrade stuck with first node in Post-draining stateA potential bug in NewElementalPartitionsFromList which caused upgrade error code 33 Workaround: https://github.com/harvester/harvester/issues/4526#issuecomment-1732853216 10. An upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent​ note This known issue is fixed in v1.2.1. If an upgrade is stuck in an Upgrading System Service state for an extended period, follow these steps to investigate this issue: Find the pods related to the upgrade: kubectl get pods -A | grep upgrade Example output: # kubectl get pods -A | grep upgrade cattle-system system-upgrade-controller-5685d568ff-tkvxb 1/1 Running 0 85m harvester-system hvst-upgrade-vq4hl-apply-manifests-65vv8 1/1 Running 0 87m // waiting for managedchart to be ready .. The pod hvst-upgrade-vq4hl-apply-manifests-65vv8 has the following loop log: Current version: 102.0.0+up40.1.2, Current state: WaitApplied, Current generation: 23 Sleep for 5 seconds to retry Check the status for all bundles. Note thata couple of bundles are OutOfSync: # kubectl get bundle -A NAMESPACE NAME BUNDLEDEPLOYMENTS-READY STATUS ... fleet-local mcc-local-managed-system-upgrade-controller 1/1 fleet-local mcc-rancher-logging 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-logging-crd 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-monitoring 0/1 OutOfSync(1) [Cluster fleet-local/local] fleet-local mcc-rancher-monitoring-crd 0/1 WaitApplied(1) [Cluster fleet-local/local] The pod fleet-agent-* has following error log: fleet-agent pod log: time=&quot;2023-09-19T12:18:10Z&quot; level=error msg=&quot;Failed to register agent: looking up secret cattle-fleet-local-system/fleet-agent-bootstrap: Post \\&quot;https://192.168.122.199/apis/fleet.cattle.io/ v1alpha1/namespaces/fleet-local/clusterregistrations\\&quot;: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.122.199 because it doesn't contain any IP SANs&quot; Check the ssl-certificates settings in Harvester: From the command line: # kubectl get settings.harvesterhci.io ssl-certificates NAME VALUE ssl-certificates {&quot;publicCertificate&quot;:&quot;-----BEGIN CERTIFICATE-----\\nMIIFNDCCAxygAwIBAgIUS7DoHthR/IR30+H/P0pv6HlfOZUwDQYJKoZIhvcNAQEL\\nBQAwFjEUMBIGA1UEAwwLZXhhbXBsZS5j....&quot;} From the Harvester Web UI: Check the server-url setting, it is the value of VIP: # kubectl get settings.management.cattle.io -n cattle-system server-url NAME VALUE server-url https://192.168.122.199 The root cause: User sets the self-signed ssl-certificates with FQDN in the Harvester settings, but the server-url points to the VIP, the fleet-agent pod fails to register. For example: create self-signed certificate for (*).example.com openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \\ -keyout example.key -out example.crt -subj &quot;/CN=example.com&quot; \\ -addext &quot;subjectAltName=DNS:example.com,DNS:*.example.com&quot; The general outputs are: example.crt, example.key The workaround: Update server-url with the value of https://harv31.example.com # kubectl edit settings.management.cattle.io -n cattle-system server-url setting.management.cattle.io/server-url edited ... # kubectl get settings.management.cattle.io -n cattle-system server-url NAME VALUE server-url https://harv31.example.com After the workaround is applied, the fleet-agent pod is replaced by Rancher automatically and registers successfully, the upgrade continues. Related issue: [BUG] Upgrade to Harvester 1.2.0 fails in fleet-agent due to customer provided SSL certificate without IP SAN Workaround: https://github.com/harvester/harvester/issues/4519#issuecomment-1727132383","keywords":"","version":"v1.4 (dev)"},{"title":"Create a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/vm/index","content":"Create a Virtual Machine How to Create a VM​ You can create one or more virtual machines from the Virtual Machines page. note Please refer to this page for creating Windows virtual machines. Choose the option to create either one or multiple VM instances.Select the namespace of your VMs, only the harvester-public namespace is visible to all users.The VM Name is a required field.(Optional) VM template is optional, you can choose iso-image, raw-image or windows-iso-image template to speed up your VM instance creation.Configure the virtual machine's CPU and memory (see overcommit settings if you want to over-provision).Select SSH keys or upload new keys.Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM.To configure networks, go to the Networks tab. The Management Network is added by default, you can remove it if the VLAN network is configured.You can also add additional networks to the VMs using VLAN networks. You may configure the VLAN networks on Advanced &gt; Networks first. (Optional) Set node affinity rules on the Node Scheduling tab.(Optional) Set workload affinity rules on the VM Scheduling tab.Advanced options such as run strategy, os type and cloud-init data are optional. You may configure these in the Advanced Options section when applicable. Volumes​ You can add one or more additional volumes via the Volumes tab, by default the first disk will be the root disk, you can change the boot order by dragging and dropping volumes, or using the arrow buttons. A disk can be made accessible via the following types: type\tdescriptiondisk\tThis type will expose the volume as an ordinary disk to the VM. cd-rom\tThis type will expose the volume as a cd-rom drive to the VM. It is read-only by default. A volume's StorageClass can be specified when adding a new empty volume; for other volumes (such as VM images), the StorageClass is defined during image creation. Adding a container disk​ A container disk is an ephemeral storage volume that can be assigned to any number of VMs and provides the ability to store and distribute VM disks in the container image registry. A container disk is: An ideal tool if you want to replicate a large number of VM workloads or inject machine drivers that do not require persistent data. Ephemeral volumes are designed for VMs that need more storage but don't care whether that data is stored persistently across VM restarts or only expect some read-only input data to be present in files, like configuration data or secret keys.Not a good solution for any workload that requires persistent root disks across VM restarts. A container disk is added when creating a VM by providing a Docker image. When creating a VM, follow these steps: Go to the Volumes tab.Select Add Container.Enter a Name for the container disk.Choose a disk Type.Add a Docker Image. A disk image, with the format qcow2 or raw, must be placed into the /disk directory.Raw and qcow2 formats are supported, but qcow2 is recommended in order to reduce the container image's size. If you use an unsupported image format, the VM will get stuck in a Running state.A container disk also allows you to store disk images in the /disk directory. An example of creating such a container image can be found here. Choose a Bus type. Networks​ You can choose to add both the management network or VLAN network to your VM instances via the Networks tab, the management network is optional if you have the VLAN network configured. Network interfaces are configured through the Type field. They describe the properties of the virtual interfaces seen inside the guest OS: type\tdescriptionbridge\tConnect using a Linux bridge masquerade\tConnect using iptables rules to NAT the traffic Management Network​ A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, VMs are accessible through the management network within the cluster nodes. Secondary Network​ It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks. In bridge VLAN, virtual machines are connected to the host network through a linux bridge. The network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Node Scheduling​ Node Scheduling allows you to constrain which nodes your VMs can be scheduled on based on node labels. See the Kubernetes Node Affinity Documentation for more details. VM Scheduling​ VM Scheduling allows you to constrain which nodes your VMs can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels. For instance, you can combine Required with Affinity to instruct the scheduler to place VMs from two services in the same zone, enhancing communication efficiency. Likewise, the use of Preferred with Anti-Affinity can help distribute VMs of a particular service across multiple zones for increased availability. See the Kubernetes Pod Affinity and Anti-Affinity Documentation for more details. Advanced Options​ Run Strategy​ Available as of v1.0.2 Prior to v1.0.2, Harvester used the Running (a boolean) field to determine if the VM instance should be running. However, a simple boolean value is not always sufficient to fully describe the user's desired behavior. For example, in some cases the user wants to be able to shut down the instance from inside the virtual machine. If the running field is used, the VM will be restarted immediately. In order to meet the scenario requirements of more users, the RunStrategy field is introduced. This is mutually exclusive with Running because their conditions overlap somewhat. There are currently four RunStrategies defined: Always: The VM instance will always exist. If VM instance crashes, a new one will be spawned. This is the same behavior as Running: true. RerunOnFailure (default): If the previous instance failed in an error state, a VM instance will be respawned. If the guest is successfully stopped (e.g. shut down from inside the guest), it will not be recreated. Manual: The presence or absence of a VM instance is controlled only by the start/stop/restart VirtualMachine actions. Stop: There will be no VM instance. If the guest is already running, it will be stopped. This is the same behavior as Running: false. Cloud Configuration​ Harvester supports the ability to assign a startup script to a virtual machine instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Cloud-init​ Cloud-init is a widely adopted project and the industry standard multi-distribution method for cross-platform cloud instance initialization. It is supported across all major cloud image provider like SUSE, Redhat, Ubuntu and etc., cloud-init has established itself as the defacto method of providing startup scripts to VMs. Harvester supports injecting your custom cloud-init startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom user-data and network-data scripts at boot. Example of password configuration for the default user: #cloud-config password: password chpasswd: { expire: False } ssh_pwauth: True Example of network-data configuration using DHCP: network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp - type: physical name: eth1 subnets: - type: dhcp You can also use the Advanced &gt; Cloud Config Templates feature to create a pre-defined cloud-init configuration template for the VM. Installing the QEMU guest agent​ The QEMU guest agent is a daemon that runs on the virtual machine instance and passes information to the host about the VM, users, file systems, and secondary networks. Install guest agent checkbox is enabled by default when a new VM is created. note If your OS is openSUSE and the version is less than 15.3, please replace qemu-guest-agent.service with qemu-ga.service. TPM Device​ Available as of v1.2.0 Trusted Platform Module (TPM) is a cryptoprocessor that secures hardware using cryptographic keys. According to Windows 11 Requirements, the TPM 2.0 device is a hard requirement of Windows 11. In the Harvester UI, you can add an emulated TPM 2.0 device to a VM by checking the Enable TPM box in the Advanced Options tab. note Currently, only non-persistent vTPMs are supported, and their state is erased after each VM shutdown. Therefore, Bitlocker should not be enabled. One-time Boot For ISO Installation​ When creating a VM to boot from cd-rom, you can use the bootOrder option so that the OS can boot from cd-rom during image installation, and boot from the disk when the installation is complete without unmounting the cd-rom. The following example describes how to install an ISO image using openSUSE Leap 15.4: Click Images in the left sidebar and download the openSUSE Leap 15.4 ISO image.Click Virtual Machines in the left sidebar, then create a VM. You need to fill up those VM basic configurations.Click the Volumes tab, In the Image field, select the image downloaded in step 1 and ensure Type is cd-romClick Add Volume and select an existing StorageClass.Drag Volume to the top of Image Volume as follows. In this way, the bootOrder of Volume will become 1. Click Create.Open the VM web-vnc you just created and follow the instructions given by the installer.After the installation is complete, reboot the VM as instructed by the operating system (you can remove the installation media after booting the system).After the VM reboots, it will automatically boot from the disk volume and start the operating system.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Create a VM","version":"v1.4 (dev)"},{"title":"Hot-Plug Volumes","type":0,"sectionRef":"#","url":"/v1.4/vm/hotplug-volume","content":"Hot-Plug Volumes Harvester supports adding hot-plug volumes to a running VM. info Currently, KubeVirt only supports disk bus scsi for hot-plug volumes. For more information, see this issue. Adding Hot-Plug Volumes to a Running VM​ The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select ⋮ &gt; Add Volume. Enter the Name and select the Volume. Click Apply.","keywords":"Harvester Hot-plug Volume","version":"v1.4 (dev)"},{"title":"Edit a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/vm/edit-vm","content":"Edit a Virtual Machine How to Edit a VM​ After creating a virtual machine, you can edit your virtual machine by clicking the ⋮ button and selecting the Edit Config button. note In addition to editing the description, a restart of the virtual machine is required for configuration changes to take effect. Basics​ On the basics tab, you can config your requested CPU and memory, a VM restart is required for this configuration to take effect. SSH Keys are injected into the cloud-init script when the virtual machine is first powered on. In order for the modified ssh key to take effect after the virtual machine is startup, the cloud-init script needs to be reinstalled from your guest OS. Networks​ You can add additional VLAN networks to your VM instances after booting, the management network is optional if you have the VLAN network configured. Additional NICs are not enabled by default unless you configure them manually in the guest OS, e.g. using wicked for your OpenSUSE Server or netplan for your Ubuntu Server. For more details about the network implementation, please refer to the Networking page. Volumes​ You can add additional volumes to the VM after booting. You can also expand the size of the volume after shutting down the VM, click the VM and go to the Volumes tab, edit the size of the expanded volume. After restarting the VM and waiting for the resize to complete, your disk will automatically finish expanding. Access Credentials​ Access Credentials allow you to inject basic auth or ssh keys dynamically at run time when your guest OS has QEMU guest agent installed. For more details please check the page here: Dynamic SSH Key Injection via Qemu guest agent.","keywords":"Harvester harvester Rancher rancher Virtual Machine virtual machine Edit a VM","version":"v1.4 (dev)"},{"title":"Create a Windows Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/vm/create-windows-vm","content":"Create a Windows Virtual Machine Create one or more virtual machines from the Virtual Machines page. note For creating Linux virtual machines, please refer to this page. How to Create a Windows VM​ Header Section​ Create a single VM instance or multiple VM instances.Set the VM name.(Optional) Provide a description for the VM.(Optional) Select the VM template windows-iso-image-base-template. This template will add a volume with the virtio drivers for Windows. Basics Tab​ Configure the number of CPU cores assigned to the VM.Configure the amount of Memory assigned to the VM. note As mentioned above, it is recommended that you use the Windows VM template. The Volumes section will describe the options which the Windows VM template created automatically. caution The bootOrder values need to be set with the installation image first. If you change it, your VM might not boot into the installation disk. Volumes Tab​ The first volume is an Image Volume with the following values: Name: The value cdrom-disk is set by default. You can keep it or change it.Type: Select cd-rom.Image: Select the Windows image to be installed. See Upload Images for the full description on how to create new images.Size: The value 20 is set by default. You can change it if your image has a bigger size.Bus: The value SATA is set by default. It's recommended you don't change it. The second volume is a Volume with the following values: Name: The value rootdisk is set by default. You can keep it or change it.Type: Select disk.StorageClass: You can use the default StorageClass harvester-longhorn or specify a custom one.Size: The value 32 is set by default. See the disk space requirements for Windows Server and Windows 11 before changing this value.Bus: The value VirtIO is set by default. You can keep it or change it to the other available options, SATA or SCSI. The third volume is a Container with the following values: Name: The value virtio-container-disk is set by default. You can keep it or change it.Type: Select cd-rom.Docker Image: The value registry.suse.com/suse/vmdp/vmdp:2.5.4.2 is set by default. We recommend not changing this value.Bus: The value SATA is set by default. We recommend not changing this value. You can add additional disks using the buttons Add Volume, Add Existing Volume, Add VM Image, or Add Container. Networks Tab​ The Management Network is added by default with the following values: Name: The value default is set by default. You can keep it or change it.Model: The value e1000 is set by default. You can keep it or change it to the other available options from the dropdown.Network: The value management Network is set by default. You can't change this option if no other network has been created. See Harvester Network for the full description on how to create new networks.Type: The value masquerade is set by default. You can keep it or change it to the other available option, bridge. You can add additional networks by clicking Add Network. caution Changing the Node Scheduling settings can impact Harvester features, such as disabling Live migration. Node Scheduling Tab​ Node Scheduling is set to Run VM on any available node by default. You can keep it or change it to the other available options from the dropdown. Advanced Options Tab​ OS Type: The value Windows is set by default. It's recommended you don't change it.Machine Type: The value None is set by default. It's recommended you don't change it. See the KubeVirt Machine Type documentation before you change this value.(Optional) Hostname: Set the VM hostname.(Optional) Cloud Config: Both User Data and Network Data values are set with default values. Currently, these configurations are not applied to Windows-based VMs.(Optional) Enable TPM, Booting in EFI mode, Secure Boot: Both the TPM 2.0 device and UEFI firmware with Secure Boot are hard requirements for Windows 11. note Currently, only non-persistent vTPMs are supported, and their state is erased after each VM shutdown. Therefore, Bitlocker should not be enabled. Footer Section​ Once all the settings are in place, click on Create to create the VM. note If you need to add advanced settings, you can edit the VM configuration directly by clicking on Edit as YAML. And if you want to cancel all changes made, click Cancel. Installation of Windows​ Select the VM you just created, and click Start to boot up the VM. Boot into the installer, and follow the instructions given by the installer. (Optional) If you are using virtio based volumes, you will need to load the specific driver to allow the installer to detect them. If you're using VM template windows-iso-image-base-template, the instruction is as follows: Click on Load driver, and then click Browse on the dialog box, and find a CD-ROM drive with a VMDP-WIN prefix. Next, find the driver directory according to the Windows version you're installing; for example, Windows Server 2012r2 should expand win8.1-2012r2 and choose the pvvx directory inside.Click OK to allow the installer to scan this directory for drivers, choose SUSE Block Driver for Windows, and click Next to load the driver.Wait for the installer to load up the driver. If you choose the correct driver version the virtio volumes will be detected once the driver is loaded. (Optional) If you are using other virtio based hardware like network adapter, you will need to install those drivers manually after completing the installation. To install drivers, open the VMDP driver disk, and use the installer based on your platform. The support matrix of VMDP driver pack for Windows are as follows (assume the VMDP CD-ROM drive path is E): Version\tSupported\tDriver pathWindows 7\tNo\tN/A Windows Server 2008\tNo\tN/A Windows Server 2008r2\tNo\tN/A Windows 8 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows Server 2012 x86(x64)\tYes\tE:\\win8-2012\\x86(x64)\\pvvx Windows 8.1 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows Server 2012r2 x86(x64)\tYes\tE:\\win8.1-2012r2\\x86(x64)\\pvvx Windows 10 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2016 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows Server 2019 x86(x64)\tYes\tE:\\win10-server\\x86(x64)\\pvvx Windows 11 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx Windows Server 2022 x86(x64)\tYes\tE:\\win10-2004\\x86(x64)\\pvvx note If you didn't use the windows-iso-image-base-template template, and you still need virtio devices, please make sure to add your custom Windows virtio driver to allow it to detect the hardware correctly. note For full instructions on how to install the VMDP guest driver and tools see the documentation at https://documentation.suse.com/sle-vmdp/2.5/html/vmdp/index.html Known Issues​ Windows ISO unable to boot when using EFI mode​ When using EFI mode with Windows, you may find the system booted with other devices like HDD or UEFI shell like the one below: That's because Windows will prompt a Press any key to boot from CD or DVD... to let the user decide whether to boot from the installer ISO or not, and it needs human intervention to allow the system to boot from CD or DVD. Alternately if the system has already booted into the UEFI shell, you can type in reset to force the system to reboot again. Once the prompt appears you can press any key to let system boot from Windows ISO. VM crashes when reserved memory not enough​ There is a known issue with Windows VM when it is allocated more than 8GiB without enough reserve memory configured. The VM crashes without warning. This can be fixed by allocating at least 256MiB of reserved memory to the template on the Advanced Options tab. If 256MiB doesn't work, try 512MiB. BSoD (Blue Screen of Death) at first boot time of Windows​ There is a known issue with Windows VM using Windows Server 2016 and above, a BSoD with error code KMODE_EXCEPTION_NOT_HANDLED may appears at the first boot time of Windows. We are still looking into it and will fix this issue in the future release. As a workaround, you can create or modify the file /etc/modprobe.d/kvm.conf within the installation of Harvester by updating /oem/99_custom.yaml like below: name: Harvester Configuration stages: initramfs: - commands: # ... files: - path: /etc/modprobe.d/kvm.conf permissions: 384 owner: 0 group: 0 content: | options kvm ignore_msrs=1 encoding: &quot;&quot; ownerstring: &quot;&quot; # ... note This is still an experimental solution. For more information, please refer to this issue and please let us know if you have encountered any issues after applying this workaround.","keywords":"Harvester harvester Rancher rancher Windows windows Virtual Machine virtual machine Create a Windows VM","version":"v1.4 (dev)"},{"title":"Live Migration","type":0,"sectionRef":"#","url":"/v1.4/vm/live-migration","content":"Live Migration Live migration means moving a virtual machine to a different host without downtime. note Live migration is not allowed when the virtual machine is using a management network of bridge interface type.Live migration is not allowed when the virtual machine has any volume of the CD-ROM type. Such volumes should be ejected before live migration.Live migration is not allowed when the virtual machine has any volume of the Container Disk type. Such volumes should be removed before live migration.Live migration is not allowed when the virtual machine has any PCIDevice passthrough enabled. Such devices need to be removed before live migration. Starting a Migration​ Go to the Virtual Machines page.Find the virtual machine that you want to migrate and select ⋮ &gt; Migrate.Choose the node to which you want to migrate the virtual machine. Click Apply. When you have node scheduling rules configured for a VM, you must ensure that the target nodes you are migrating to meet the VM's runtime requirements. The list of nodes you get to search and select from will be generated based on: VM scheduling rules.Possibly node rules from the network configuration. Aborting a Migration​ Go to the Virtual Machines page.Find the virtual machine in migrating status that you want to abort. Select ⋮ &gt; Abort Migration. Migration Timeouts​ Completion Timeout​ The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout​ Live migration will also be aborted when copying memory doesn't make any progress in 150s.","keywords":"Harvester harvester Rancher rancher Live Migration","version":"v1.4 (dev)"},{"title":"Resource Overcommit","type":0,"sectionRef":"#","url":"/v1.4/vm/resource-overcommit","content":"Resource Overcommit Harvester supports global configuration of resource overload percentages on CPU, memory, and storage. By setting overcommit-config, this will allow scheduling of additional virtual machines even when physical resources are fully utilized. Harvester allows you to overcommit CPU and RAM on compute nodes. This allows you to increase the number of instances running on your cloud at the cost of reducing the performance of the instances. The Compute service uses the following ratios by default: CPU allocation ratio: 1600%RAM allocation ratio: 150%Storage allocation ratio: 200% note Classic memory overcommitment or memory ballooning is not yet supported by this feature. In other words, memory used by a virtual machine instance cannot be returned once allocated. Configure the global setting overcommit-config​ Users can modify the global overcommit-config by following the steps below, and it will be applied to each newly created virtual machine after the change. Go to the Advanced &gt; Settings page. Find the overcommit-config setting. Configure the desired CPU, Memory, and Storage ratio. Configure overcommit for a single virtual machine​ In situations where you require specific configurations for individual virtual machines without affecting the global settings, you can easily achieve this by modifying the spec.template.spec.domain.resources.limits.&lt;memory|cpu&gt; value on the corresponding virtual machine spec directly. Reserve more memory for the system overhead​ By default, the Harvester reserves a certain amount of system management overhead memory from the memory allocated for the virtual machine. In most cases, this will not cause any problems. However, some operating systems, such as Windows 2022, will request more memory than is reserved. To address the issue, Harvester provides an annotation harvesterhci.io/reservedMemory on VirtualMachine custom resource to let you specify the amount of memory to reserve. For instance, add harvesterhci.io/reservedMemory: 200Mi if you decide to reserve 200 MiB for the system overhead of the VM. apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: + harvesterhci.io/reservedMemory: 200Mi kubevirt.io/latest-observed-api-version: v1 kubevirt.io/storage-observed-api-version: v1alpha3 network.harvesterhci.io/ips: '[]' ... ... Why my virtual machines are scheduled unevenly?​ The scheduling of virtual machines depends on the underlying behavior of the kube-scheduler. We have a dedicated article explaining the details. If you would like to learn more, check out: Harvester Knowledge Base: VM Scheduling.","keywords":"Harvester Overcommit Overprovision ballooning","version":"v1.4 (dev)"},{"title":"Clone a Volume","type":0,"sectionRef":"#","url":"/v1.4/volume/clone-volume","content":"Clone a Volume How to Clone a Volume​ After creating a volume, you can clone the volume by following the steps below: Click the ⋮ button and select the Clone option. Select clone volume data. Configure the Name of the new volume and click Create. (Optional) A cloned volume can be added to a VM using Add Existing Volume.","keywords":"Volume","version":"v1.4 (dev)"},{"title":"Edit a Volume","type":0,"sectionRef":"#","url":"/v1.4/volume/edit-volume","content":"Edit a Volume After creating a volume, you can edit your volume by clicking the ⋮ button and selecting the Edit Config option. Expand a Volume​ You can expand a volume by increasing the value of the Size parameter directly. To prevent the expansion from interference by unexpected data R/W, Harvester supports offline expansion only. You must shut down the VM or detach the volume first if it is attached to a VM, and the detached volume will automatically attach to a random node with maintenance mode to expand automatically. Cancel a Failed Volume Expansion​ If you specify a size larger than Longhorn's capacity during the expansion, the status of the volume expansion will be stuck in Resizing. You can cancel the failed volume expansion by clicking the ⋮ button and selecting the Cancel Expand option. Change the StorageClass of an Existing Volume​ The StorageClass of an existing volume cannot be changed. However, you can change the StorageClass while restoring a new volume from the snapshot by following the steps below: Take a volume snapshot.Select StorageClass when restoring the volume using snapshot.","keywords":"Volume","version":"v1.4 (dev)"},{"title":"Export a Volume to Image","type":0,"sectionRef":"#","url":"/v1.4/volume/export-volume","content":"Export a Volume to Image You can select and export an existing volume to an image by following the steps below: Click the ⋮ button and select the Export Image option. Select the Namespace of the new image. Configure the Name of the new image. Select an existing StorageClass. (Optional) You can download the exported image from the Images page by clicking the ⋮ button and selecting the Download option.","keywords":"Volume","version":"v1.4 (dev)"},{"title":"Create a Volume","type":0,"sectionRef":"#","url":"/v1.4/volume/index","content":"Create a Volume Create an Empty Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose New in Source.Select an existing StorageClass.Configure the Size of the volume. Create an Image Volume​ Header Section​ Set the Volume Name.(Optional) Provide a Description for the Volume. Basics Tab​ Choose VM Image in Source.Select an existing Image.Configure the Size of the volume.","keywords":"Volume","version":"v1.4 (dev)"},{"title":"Volume Snapshots","type":0,"sectionRef":"#","url":"/v1.4/volume/volume-snapshots","content":"Volume Snapshots A volume snapshot represents a snapshot of a volume on a storage system. After creating a volume, you can create a volume snapshot and restore a volume to the snapshot's state. With volume snapshots, you can easily copy or restore a volume's configuration. Create Volume Snapshots​ You can create a volume snapshot from an existing volume by following these steps: Go to the Volumes page. Choose the volume that you want to take a snapshot of and select ⋮ &gt; Take Snapshot. Enter a Name for the snapshot. Select Create to finish creating a new volume snapshot. Check the status of this operation and view all volume snapshots by going to the Volumes page and selecting the Snapshots tab. When the Ready To Use becomes √, the volume snapshot is ready to use. note A recurring snapshot is currently not supported and is tracked via harvester/harvester#572. Restore a new volume from a volume snapshot​ You can restore a new volume from an existing volume snapshot by following these steps: Go to the Backup &amp; Snapshot &gt; Volume Snapshots page or select a Volume from the Volumes page and go to the Snapshots tab. Select ⋮ &gt; Restore. Specify the Name of the new volume. If the source volume is not an image volume, you can select a different StorageClass. You can not change the StorageClass if the source volume is an image volume. Select Create to finish restoring a new volume.","keywords":"Volume Snapshot Volume Snapshots","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine","content":"Replace a Virtual Machine PUT /apis/kubevirt.io/v1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachines/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachine object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastProbeTime string lastTransitionTime string message string reason string status stringrequired type stringrequired ] created boolean memoryDumpRequest object claimName stringrequired endTimestamp string fileName string message string phase stringrequired startTimestamp string printableStatus string ready boolean restoreInProgress string snapshotInProgress string startFailure object consecutiveFailCount int32 lastFailedVMIUID string retryAfterTimestamp string stateChangeRequests object[] Array [ action stringrequired data object property name* string uid string ] volumeRequests object[] Array [ addVolumeOptions object disk objectrequired blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string dryRun string[] name stringrequired volumeSource objectrequired dataVolume object hotpluggable boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean removeVolumeOptions object dryRun string[] name stringrequired ] volumeSnapshotStatuses object[] Array [ enabled booleanrequired Default value: false name stringrequired reason string ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Backup","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine-backup","content":"Replace a Virtual Machine Backup PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinebackups/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineBackup object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired source objectrequired apiGroup string kind stringrequired name stringrequired type string status object backupTarget object bucketName string bucketRegion string endpoint string conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] creationTime string csiDriverVolumeSnapshotClassNames object property name* string error object message string time string readyToUse boolean secretBackups object[] Array [ data object property name* byte name string ] source object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] sourceUID string volumeBackups object[] Array [ creationTime string csiDriverName stringrequired error object message string time string longhornBackupName string name string persistentVolumeClaim objectrequired metadata object name stringrequired namespace string spec object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string readyToUse boolean volumeName stringrequired ] Loading...","keywords":"","version":"v1.4 (dev)"},{"title":"Replace a Virtual Machine Template Version","type":0,"sectionRef":"#","url":"/v1.4/api/replace-namespaced-virtual-machine-template-version","content":"Replace a Virtual Machine Template Version PUT /apis/harvesterhci.io/v1beta1/namespaces/{namespace:[a-z0-9][a-z0-9\\-]*}/virtualmachinetemplateversions/{name:[a-z0-9][a-z0-9\\-]*} Update a VirtualMachineTemplateVersion object. Request​ Path Parameters name stringrequired Name of the resource namespace stringrequired Object name and auth scope, such as for teams and projects application/jsonapplication/yaml Body required apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Responses​ 200201401 OK application/jsonapplication/yaml SchemaExample (from schema) Schema apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired description string imageId string keyPairIds string[] templateId stringrequired vm object metadata object name stringrequired namespace string spec object dataVolumeTemplates object[] Array [ apiVersion stringrequired kind stringrequired metadata object name stringrequired namespace string spec objectrequired checkpoints object[] Array [ current stringrequired previous stringrequired ] contentType string finalCheckpoint boolean preallocation boolean priorityClassName string pvc object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired dataSourceRef object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string source object blank object http object certConfigMap string extraHeaders string[] secretExtraHeaders string[] secretRef string url stringrequired imageio object certConfigMap string diskId stringrequired secretRef string url stringrequired pvc object name stringrequired namespace stringrequired registry object certConfigMap string imageStream string pullMethod string secretRef string url string s3 object certConfigMap string secretRef string url stringrequired upload object vddk object backingFile string initImageURL string secretRef string thumbprint string url string uuid string sourceRef object kind stringrequired name stringrequired namespace string storage object accessModes string[] dataSource object apiGroup string kind stringrequired name stringrequired resources object limits object property name* string Default value: [object Object] requests object property name* string Default value: [object Object] selector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string storageClassName string volumeMode string volumeName string status object ] flavor object kind string name stringrequired preference object kind string name stringrequired runStrategy string running boolean template objectrequired metadata object name stringrequired namespace string spec object accessCredentials object[] Array [ sshPublicKey object propagationMethod objectrequired configDrive object qemuGuestAgent object users string[]required source objectrequired secret object secretName stringrequired userPassword object propagationMethod objectrequired qemuGuestAgent object source objectrequired secret object secretName stringrequired ] affinity object nodeAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ preference objectrequired matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object nodeSelectorTerms object[]required Array [ matchExpressions object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] matchFields object[] Array [ key stringrequired operator stringrequired Possible values: [DoesNotExist, Exists, Gt, In, Lt, NotIn] values string[] ] ] podAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] podAntiAffinity object preferredDuringSchedulingIgnoredDuringExecution object[] Array [ podAffinityTerm objectrequired labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired weight int32required ] requiredDuringSchedulingIgnoredDuringExecution object[] Array [ labelSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaceSelector object matchExpressions object[] Array [ key stringrequired operator stringrequired values string[] ] matchLabels object property name* string namespaces string[] topologyKey stringrequired ] dnsConfig object nameservers string[] options object[] Array [ name string value string ] searches string[] dnsPolicy string Possible values: [ClusterFirst, ClusterFirstWithHostNet, Default, None] domain objectrequired chassis object asset string manufacturer string serial string sku string version string clock object timer object hpet object present boolean tickPolicy string hyperv object present boolean kvm object present boolean pit object present boolean tickPolicy string rtc object present boolean tickPolicy string track string timezone string utc object offsetSeconds int32 cpu object cores int64 dedicatedCpuPlacement boolean features object[] Array [ name stringrequired policy string ] isolateEmulatorThread boolean model string numa object guestMappingPassthrough object realtime object mask string sockets int64 threads int64 devices objectrequired autoattachGraphicsDevice boolean autoattachMemBalloon boolean autoattachPodInterface boolean autoattachSerialConsole boolean blockMultiQueue boolean clientPassthrough object disableHotplug boolean disks object[] Array [ blockSize object custom object logical int32required physical int32required matchVolume object enabled boolean bootOrder int32 cache string cdrom object bus string readonly boolean tray string dedicatedIOThread boolean disk object bus string pciAddress string readonly boolean io string lun object bus string readonly boolean name stringrequired serial string shareable boolean tag string ] filesystems object[] Array [ name stringrequired virtiofs objectrequired ] gpus object[] Array [ deviceName stringrequired name stringrequired tag string virtualGPUOptions object display object enabled boolean ramFB object enabled boolean ] hostDevices object[] Array [ deviceName stringrequired name stringrequired tag string ] inputs object[] Array [ bus string name stringrequired type stringrequired ] interfaces object[] Array [ bootOrder int32 bridge object dhcpOptions object bootFileName string ntpServers string[] privateOptions object[] Array [ option int32required value stringrequired ] tftpServerName string macAddress string macvtap object masquerade object model string name stringrequired pciAddress string ports object[] Array [ name string port int32required protocol string ] slirp object sriov object tag string ] networkInterfaceMultiqueue boolean rng object sound object model string name stringrequired tpm object useVirtioTransitional boolean watchdog object i6300esb object action string name stringrequired features object acpi object enabled boolean apic object enabled boolean endOfInterrupt boolean hyperv object evmcs object enabled boolean frequencies object enabled boolean ipi object enabled boolean reenlightenment object enabled boolean relaxed object enabled boolean reset object enabled boolean runtime object enabled boolean spinlocks object enabled boolean spinlocks int64 synic object enabled boolean synictimer object direct object enabled boolean enabled boolean tlbflush object enabled boolean vapic object enabled boolean vendorid object enabled boolean vendorid string vpindex object enabled boolean kvm object hidden boolean pvspinlock object enabled boolean smm object enabled boolean firmware object bootloader object bios object useSerial boolean efi object secureBoot boolean kernelBoot object container object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string initrdPath string kernelPath string kernelArgs string serial string uuid string ioThreadsPolicy string launchSecurity object sev object machine object type string memory object guest string hugepages object pageSize string resources object limits object property name* string Default value: [object Object] overcommitGuestOverhead boolean requests object property name* string Default value: [object Object] evictionStrategy string hostname string livenessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 networks object[] Array [ multus object default boolean networkName stringrequired name stringrequired pod object vmIPv6NetworkCIDR string vmNetworkCIDR string ] nodeSelector object property name* string priorityClassName string readinessProbe object exec object command string[] failureThreshold int32 guestAgentPing object httpGet object host string httpHeaders object[] Array [ name stringrequired value stringrequired ] path string port int-or-stringrequired Default value: [object Object] scheme string Possible values: [HTTP, HTTPS] initialDelaySeconds int32 periodSeconds int32 successThreshold int32 tcpSocket object host string port int-or-stringrequired Default value: [object Object] timeoutSeconds int32 schedulerName string startStrategy string subdomain string terminationGracePeriodSeconds int64 tolerations object[] Array [ effect string Possible values: [NoExecute, NoSchedule, PreferNoSchedule] key string operator string Possible values: [Equal, Exists] tolerationSeconds int64 value string ] volumes object[] Array [ cloudInitConfigDrive object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string cloudInitNoCloud object networkData string networkDataBase64 string networkDataSecretRef object name string secretRef object name string userData string userDataBase64 string configMap object name string optional boolean volumeLabel string containerDisk object image stringrequired imagePullPolicy string Possible values: [Always, IfNotPresent, Never] imagePullSecret string path string dataVolume object hotpluggable boolean name stringrequired downwardAPI object fields object[] Array [ fieldRef object apiVersion string fieldPath stringrequired mode int32 path stringrequired resourceFieldRef object containerName string divisor string Default value: [object Object] resource stringrequired ] volumeLabel string downwardMetrics object emptyDisk object capacity stringrequired Default value: [object Object] ephemeral object persistentVolumeClaim object claimName stringrequired readOnly boolean hostDisk object capacity string Default value: [object Object] path stringrequired shared boolean type stringrequired memoryDump object claimName stringrequired hotpluggable boolean readOnly boolean name stringrequired persistentVolumeClaim object claimName stringrequired hotpluggable boolean readOnly boolean secret object optional boolean secretName string volumeLabel string serviceAccount object serviceAccountName string sysprep object configMap object name string secret object name string ] status object conditions object[] Array [ lastTransitionTime string lastUpdateTime string message string reason string status stringrequired type stringrequired ] version int32 Loading...","keywords":"","version":"v1.4 (dev)"}],"options":{"language":["en"],"indexBaseUrl":true,"maxHits":10,"highlightResult":true,"id":"default"}}